---
# Source: charon-relay/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-charon-relay-0
  namespace: default
  labels:
    name: my-charon-relay-0
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      name: my-charon-relay-0
---
# Source: charon-relay/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-charon-relay-1
  namespace: default
  labels:
    name: my-charon-relay-1
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      name: my-charon-relay-1
---
# Source: charon-relay/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-charon-relay-2
  namespace: default
  labels:
    name: my-charon-relay-2
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      name: my-charon-relay-2
---
# Source: charon-relay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-charon-relay-0
  namespace: default
  labels:
    name: my-charon-relay-0
---
# Source: charon-relay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-charon-relay-1
  namespace: default
  labels:
    name: my-charon-relay-1
---
# Source: charon-relay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-charon-relay-2
  namespace: default
  labels:
    name: my-charon-relay-2
---
# Source: charon-relay/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-charon-relay-0
  labels:
    name: my-charon-relay-0
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-charon-relay-1
  labels:
    name: my-charon-relay-1
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-charon-relay-2
  labels:
    name: my-charon-relay-2
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-charon-relay-0
  labels:
    name: my-charon-relay-0
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-charon-relay-0
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-0
    namespace: default
---
# Source: charon-relay/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-charon-relay-1
  labels:
    name: my-charon-relay-1
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-charon-relay-1
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-1
    namespace: default
---
# Source: charon-relay/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-charon-relay-2
  labels:
    name: my-charon-relay-2
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-charon-relay-2
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-2
    namespace: default
---
# Source: charon-relay/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-charon-relay-0
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-charon-relay-1
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-charon-relay-2
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
---
# Source: charon-relay/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-charon-relay-0
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-charon-relay-0
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-0
    namespace: default
---
# Source: charon-relay/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-charon-relay-1
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-charon-relay-1
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-1
    namespace: default
---
# Source: charon-relay/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-charon-relay-2
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-charon-relay-2
subjects:
  - kind: ServiceAccount
    name: my-charon-relay-2
    namespace: default
---
# Source: charon-relay/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-charon-relay-0
  namespace: default
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  sessionAffinity: None
  ports:
    - name: relay-http
      protocol: TCP
      port: 3640
      targetPort: 3640
    - name: p2p-tcp
      protocol: TCP
      port: 3610
      targetPort: 3610
    - name: monitoring
      protocol: TCP
      port: 3620
      targetPort: 3620
  selector:
    name: my-charon-relay-0
---
# Source: charon-relay/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-charon-relay-1
  namespace: default
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  sessionAffinity: None
  ports:
    - name: relay-http
      protocol: TCP
      port: 3640
      targetPort: 3640
    - name: p2p-tcp
      protocol: TCP
      port: 3610
      targetPort: 3610
    - name: monitoring
      protocol: TCP
      port: 3620
      targetPort: 3620
  selector:
    name: my-charon-relay-1
---
# Source: charon-relay/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-charon-relay-2
  namespace: default
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  sessionAffinity: None
  ports:
    - name: relay-http
      protocol: TCP
      port: 3640
      targetPort: 3640
    - name: p2p-tcp
      protocol: TCP
      port: 3610
      targetPort: 3610
    - name: monitoring
      protocol: TCP
      port: 3620
      targetPort: 3620
  selector:
    name: my-charon-relay-2
---
# Source: charon-relay/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-charon-relay-0
  namespace: default
  labels:
    name: my-charon-relay-0
spec:
  replicas: 1
  serviceName: my-charon-relay-0
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: my-charon-relay-0
  template:
    metadata:
      labels:
        name: my-charon-relay-0
    spec:
      serviceAccountName: my-charon-relay-0
      initContainers:
        - name: loadbalancer-ip-fetcher
          image: bitnami/kubectl:latest
          command:
            - "/bin/sh"
            - "-c"
            - |
              echo "Fetching load balancer ip ...";
              while true; do
                LB_IP=$(kubectl get svc my-charon-relay-0 -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}');
                if [ ! -z "$LB_IP" ]; then
                  echo "Load balancer ip: $LB_IP";
                  echo "$LB_IP" > /shared/load_balancer_ip;
                  break;
                else
                  echo "Load balancer ip not available yet, retrying in 5 seconds...";
                  sleep 5;
                fi;
              done;
          volumeMounts:
            - name: shared
              mountPath: /shared
      containers:
        - command:
            - sh
            - -ac
            - >
              export LOAD_BALANCER_IP=$(cat /shared/load_balancer_ip);
              exec /usr/local/bin/charon relay --p2p-external-ip="${LOAD_BALANCER_IP}"
              --auto-p2pkey=true
              --http-address=0.0.0.0:3640
              --log-format=json
              --log-level=debug
              --loki-service="${NODE_NAME}"
              --monitoring-address=0.0.0.0:3620
              --p2p-max-connections=16384
              --p2p-max-reservations=512
              --p2p-relay-loglevel=debug
              --p2p-tcp-address=0.0.0.0:3610
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_CLUSTER_DOMAIN
            value: 
          image: obolnetwork/charon:v1.0.0
          name: charon-relay
          securityContext:
            runAsUser: 0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /livez
              port: 3640
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 3
            httpGet:
              path: /readyz
              port: 3640
          workingDir: /charon
          volumeMounts:
            - name: data
              mountPath: /charon
            - name: shared
              mountPath: "/shared"
      volumes:
        - name: shared
          emptyDir: {}
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: standard
---
# Source: charon-relay/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-charon-relay-1
  namespace: default
  labels:
    name: my-charon-relay-1
spec:
  replicas: 1
  serviceName: my-charon-relay-1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: my-charon-relay-1
  template:
    metadata:
      labels:
        name: my-charon-relay-1
    spec:
      serviceAccountName: my-charon-relay-1
      initContainers:
        - name: loadbalancer-ip-fetcher
          image: bitnami/kubectl:latest
          command:
            - "/bin/sh"
            - "-c"
            - |
              echo "Fetching load balancer ip ...";
              while true; do
                LB_IP=$(kubectl get svc my-charon-relay-1 -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}');
                if [ ! -z "$LB_IP" ]; then
                  echo "Load balancer ip: $LB_IP";
                  echo "$LB_IP" > /shared/load_balancer_ip;
                  break;
                else
                  echo "Load balancer ip not available yet, retrying in 5 seconds...";
                  sleep 5;
                fi;
              done;
          volumeMounts:
            - name: shared
              mountPath: /shared
      containers:
        - command:
            - sh
            - -ac
            - >
              export LOAD_BALANCER_IP=$(cat /shared/load_balancer_ip);
              exec /usr/local/bin/charon relay --p2p-external-ip="${LOAD_BALANCER_IP}"
              --auto-p2pkey=true
              --http-address=0.0.0.0:3640
              --log-format=json
              --log-level=debug
              --loki-service="${NODE_NAME}"
              --monitoring-address=0.0.0.0:3620
              --p2p-max-connections=16384
              --p2p-max-reservations=512
              --p2p-relay-loglevel=debug
              --p2p-tcp-address=0.0.0.0:3610
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_CLUSTER_DOMAIN
            value: 
          image: obolnetwork/charon:v1.0.0
          name: charon-relay
          securityContext:
            runAsUser: 0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /livez
              port: 3640
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 3
            httpGet:
              path: /readyz
              port: 3640
          workingDir: /charon
          volumeMounts:
            - name: data
              mountPath: /charon
            - name: shared
              mountPath: "/shared"
      volumes:
        - name: shared
          emptyDir: {}
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: standard
---
# Source: charon-relay/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-charon-relay-2
  namespace: default
  labels:
    name: my-charon-relay-2
spec:
  replicas: 1
  serviceName: my-charon-relay-2
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: my-charon-relay-2
  template:
    metadata:
      labels:
        name: my-charon-relay-2
    spec:
      serviceAccountName: my-charon-relay-2
      initContainers:
        - name: loadbalancer-ip-fetcher
          image: bitnami/kubectl:latest
          command:
            - "/bin/sh"
            - "-c"
            - |
              echo "Fetching load balancer ip ...";
              while true; do
                LB_IP=$(kubectl get svc my-charon-relay-2 -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}');
                if [ ! -z "$LB_IP" ]; then
                  echo "Load balancer ip: $LB_IP";
                  echo "$LB_IP" > /shared/load_balancer_ip;
                  break;
                else
                  echo "Load balancer ip not available yet, retrying in 5 seconds...";
                  sleep 5;
                fi;
              done;
          volumeMounts:
            - name: shared
              mountPath: /shared
      containers:
        - command:
            - sh
            - -ac
            - >
              export LOAD_BALANCER_IP=$(cat /shared/load_balancer_ip);
              exec /usr/local/bin/charon relay --p2p-external-ip="${LOAD_BALANCER_IP}"
              --auto-p2pkey=true
              --http-address=0.0.0.0:3640
              --log-format=json
              --log-level=debug
              --loki-service="${NODE_NAME}"
              --monitoring-address=0.0.0.0:3620
              --p2p-max-connections=16384
              --p2p-max-reservations=512
              --p2p-relay-loglevel=debug
              --p2p-tcp-address=0.0.0.0:3610
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_CLUSTER_DOMAIN
            value: 
          image: obolnetwork/charon:v1.0.0
          name: charon-relay
          securityContext:
            runAsUser: 0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 5
            httpGet:
              path: /livez
              port: 3640
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 3
            httpGet:
              path: /readyz
              port: 3640
          workingDir: /charon
          volumeMounts:
            - name: data
              mountPath: /charon
            - name: shared
              mountPath: "/shared"
      volumes:
        - name: shared
          emptyDir: {}
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: standard
---
# Source: charon-relay/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-charon-relay-test-connection"
  labels:
    helm.sh/chart: charon-relay-0.1.8
    app.kubernetes.io/name: charon-relay
    app.kubernetes.io/instance: my-charon-relay
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['my-charon-relay:']
  restartPolicy: Never
