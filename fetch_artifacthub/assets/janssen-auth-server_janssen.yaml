---
# Source: janssen/charts/auth-server/templates/auth-server-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-janssen-auth-server
  labels:
    APP_NAME: auth-server
    app: my-janssen-auth-server
    helm.sh/chart: auth-server-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm  
spec:
  maxUnavailable: 90%
  selector:
    matchLabels:
      app: my-janssen-auth-server
---
# Source: janssen/charts/casa/templates/casa-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-janssen-casa
  labels:
    APP_NAME: casa
    app: my-janssen-casa
    helm.sh/chart: casa-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm  
spec:
  maxUnavailable: 90%
  selector:
    matchLabels:
      app: my-janssen-casa
---
# Source: janssen/charts/config-api/templates/config-api-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-janssen-config-api
  labels:
    APP_NAME: config-api
    app: my-janssen-config-api
    helm.sh/chart: config-api-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm      
spec:
  maxUnavailable: 90%
  selector:
    matchLabels:
      app: my-janssen-config-api
---
# Source: janssen/charts/fido2/templates/fido2-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-janssen-fido2
  labels:
    APP_NAME: fido2
    app: my-janssen-fido2
    helm.sh/chart: fido2-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm  
spec:
  maxUnavailable: 90%
  selector:
    matchLabels:
      app: my-janssen-fido2
---
# Source: janssen/charts/scim/templates/scim-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-janssen-scim
  labels:
    APP_NAME: scim
    app: my-janssen-scim
    helm.sh/chart: scim-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm  
spec:
  maxUnavailable: 90%
  selector:
    matchLabels:
      app: my-janssen-scim
---
# Source: janssen/charts/config/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-janssen-configuration-file
  namespace: default
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
  configuration.json: |-
    {
      "_configmap": {
        "hostname": "demoexample.jans.io",
        "country_code": "US",
        "state": "TX",
        "city": "Austin",
        "admin_email": "support@jans.io",
        "orgName": "Janssen",
        "auth_sig_keys": "RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512",
        "auth_enc_keys": "RSA1_5 RSA-OAEP",
        "optional_scopes": "[\"sql\"]",
        "init_keys_exp": 48
      },
      "_secret": {
        "admin_password": "Test1234#",
        
        "redis_password": "P@assw0rd",
        
        "sql_password": "Test1234#",
        
        
        
        
        "encoded_salt": ""
      }
    }
---
# Source: janssen/charts/config/templates/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-janssen-config-cm
  namespace: default
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
data:
  # Jetty header size in bytes in the auth server
  CN_JETTY_REQUEST_HEADER_SIZE: "8192"
  # Port used by Prometheus JMX agent
  CN_PROMETHEUS_PORT: ""
  
  
  
  
  # [vault_envs] Envs related to Hashicorp vault
  
  CN_SQL_DB_SCHEMA: ""
  CN_SQL_DB_DIALECT: mysql
  CN_SQL_DB_HOST: my-release-mysql.default.svc.cluster.local
  CN_SQL_DB_PORT: "3306"
  CN_SQL_DB_NAME: jans
  CN_SQL_DB_USER: jans
  CN_SQL_DB_TIMEZONE: UTC
  CN_CONFIG_ADAPTER: kubernetes
  CN_SECRET_ADAPTER: kubernetes
  CN_CONFIG_KUBERNETES_NAMESPACE: "default"
  CN_SECRET_KUBERNETES_NAMESPACE: "default"
  CN_CONFIG_KUBERNETES_CONFIGMAP: cn
  CN_SECRET_KUBERNETES_SECRET: cn
  CN_CONTAINER_METADATA: "kubernetes"
  CN_MAX_RAM_PERCENTAGE: "75.0"
  CN_CACHE_TYPE: "NATIVE_PERSISTENCE"
  CN_DOCUMENT_STORE_TYPE: "DB"
  DOMAIN: "demoexample.jans.io"
  CN_AUTH_SERVER_BACKEND: "auth-server:8080"
  CN_AUTH_APP_LOGGERS: '{"audit_log_level":"INFO","audit_log_target":"FILE","auth_log_level":"INFO","auth_log_target":"STDOUT","enable_stdout_log_prefix":"true","http_log_level":"INFO","http_log_target":"FILE","ldap_stats_log_level":"INFO","ldap_stats_log_target":"FILE","persistence_duration_log_level":"INFO","persistence_duration_log_target":"FILE","persistence_log_level":"INFO","persistence_log_target":"FILE","script_log_level":"INFO","script_log_target":"FILE"}'
  CN_CONFIG_API_APP_LOGGERS: '{"config_api_log_level":"INFO","config_api_log_target":"STDOUT","enable_stdout_log_prefix":"true","ldap_stats_log_level":"INFO","ldap_stats_log_target":"FILE","persistence_duration_log_level":"INFO","persistence_duration_log_target":"FILE","persistence_log_level":"INFO","persistence_log_target":"FILE","script_log_level":"INFO","script_log_target":"FILE"}'
  CN_PERSISTENCE_TYPE: sql
  CN_KEY_ROTATION_FORCE: "false"
  CN_KEY_ROTATION_CHECK: "3600"
  CN_KEY_ROTATION_INTERVAL: "48"
  CN_SSL_CERT_FROM_SECRETS: "true"
  CN_CONTAINER_MAIN_NAME: my-janssen-auth-server
  # options: default/user/site/cache/statistic used only if CN_PERSISTENCE_TYPE is hybrid or hybrid
  # Auto enable installation of some services
  
  
  CN_SCIM_ENABLED: "true"
  CN_SCIM_PROTECTION_MODE: "OAUTH"
  CN_SCIM_APP_LOGGERS: '{"enable_stdout_log_prefix":"true","ldap_stats_log_level":"INFO","ldap_stats_log_target":"FILE","persistence_duration_log_level":"INFO","persistence_duration_log_target":"FILE","persistence_log_level":"INFO","persistence_log_target":"FILE","scim_log_level":"INFO","scim_log_target":"STDOUT","script_log_level":"INFO","script_log_target":"FILE"}'
  CN_FIDO2_APP_LOGGERS: '{"enable_stdout_log_prefix":"true","fido2_log_level":"INFO","fido2_log_target":"STDOUT","persistence_duration_log_level":"INFO","persistence_duration_log_target":"FILE","persistence_log_level":"INFO","persistence_log_target":"FILE","script_log_level":"INFO","script_log_target":"FILE"}'  # CASA
  CN_CASA_APP_LOGGERS: '{"casa_log_level":"INFO","casa_log_target":"STDOUT","enable_stdout_log_prefix":"true","timer_log_level":"INFO","timer_log_target":"FILE"}'
  CN_SQL_PASSWORD_FILE: /etc/jans/conf/sql_password
  CN_COUCHBASE_PASSWORD_FILE: /etc/jans/conf/couchbase_password
  CN_COUCHBASE_SUPERUSER_PASSWORD_FILE: /etc/jans/conf/couchbase_superuser_password
  CN_LDAP_PASSWORD_FILE: /etc/jans/conf/ldap_password
  CN_LDAP_TRUSTSTORE_PASSWORD_FILE: /etc/jans/conf/ldap_truststore_password
  CN_LDAP_CERT_FILE: /etc/certs/opendj.crt
  CN_LDAP_KEY_FILE: /etc/certs/opendj.key
  CN_LDAP_CACERT_FILE: /etc/certs/opendj.pem
  CN_LDAP_TRUSTSTORE_FILE: /etc/certs/opendj.pkcs12
  CN_CONFIG_API_PLUGINS: "fido2,scim,user-mgt"
  CN_LOCK_ENABLED: "false"
  CN_OPA_URL: "http://opa.opa.svc.cluster.cluster.local:8181/v1"
  CN_MESSAGE_TYPE: "DISABLED"
  CN_CONFIGURATOR_CONFIGURATION_FILE: "/etc/jans/conf/configuration.json"
  CN_CONFIGURATOR_DUMP_FILE: "/etc/jans/conf/configuration.out.json"
---
# Source: janssen/charts/config/templates/configmaps.yaml
apiVersion: v1
data:
  tls_generator.py: |-
    from kubernetes import config, client
    import logging
    import base64

    from jans.pycloudlib import get_manager

    log_format = '%(asctime)s - %(name)8s - %(levelname)5s - %(message)s'
    logging.basicConfig(format=log_format, level=logging.INFO)
    logger = logging.getLogger("tls-generator")

    # use the serviceAccount k8s gives to pods
    config.load_incluster_config()
    core_cli = client.CoreV1Api()

    def patch_or_create_namespaced_secret(name, literal, value_of_literal, namespace="default",
                                          secret_type="Opaque", second_literal=None, value_of_second_literal=None,
                                          data=None):
        """Patch secret and if not exist create
        :param name:
        :param literal:
        :param value_of_literal:
        :param namespace:
        :param secret_type:
        :param second_literal:
        :param value_of_second_literal:
        :param data:
        :return:
        """
        # Instantiate the Secret object
        body = client.V1Secret()
        metadata = client.V1ObjectMeta(name=name)
        body.data = data
        if not data:
            body.data = {literal: value_of_literal}
        body.metadata = metadata
        body.type = secret_type
        if second_literal:
            body.data = {literal: value_of_literal, second_literal: value_of_second_literal}
        try:
            core_cli.patch_namespaced_secret(name, namespace, body)
            logger.info('Secret  {} in namespace {} has been patched'.format(name, namespace))
            return
        except client.rest.ApiException as e:
            if e.status == 404 or not e.status:
                try:
                    core_cli.create_namespaced_secret(namespace=namespace, body=body)
                    logger.info('Created secret {} of type {} in namespace {}'.format(name, secret_type, namespace))
                    return True
                except client.rest.ApiException as e:
                    logger.exception(e)
                    return False
            logger.exception(e)
            return False

    # check if janssen secret exists
    def get_certs(secret_name, namespace):
        """

        :param namespace:
        :return:  ssl cert and key from janssen secrets
        """
        def b64encode(value):
            return base64.b64encode(value.encode()).decode()

        manager = get_manager()

        # returns empty string if not found
        ssl_cert = manager.secret.get("ssl_cert")
        if ssl_cert:
            ssl_cert = b64encode(ssl_cert)

        # returns empty string if not found
        ssl_key = manager.secret.get("ssl_key")
        if ssl_key:
            ssl_key = b64encode(ssl_key)
        return ssl_cert, ssl_key


    def main():
        namespace = "default"
        secret_name = "cn"
        cert, key = get_certs(secret_name, namespace)
        # global vars
        name = "tls-certificate"

        # if istio is enabled

        if cert and key:
            patch_or_create_namespaced_secret(name=name,
                                              namespace=namespace,
                                              literal="tls.crt",
                                              value_of_literal=cert,
                                              secret_type="kubernetes.io/tls",
                                              second_literal="tls.key",
                                              value_of_second_literal=key)
        else:
            logger.error(
                "No certificate or key was found in secrets."
                "This can happen when the ssl certificate for the domain is able to be pulled."
                "In that scenario the ssl_cert will be pulled from the domain provided"
            )

    if __name__ == "__main__":
        main()

kind: ConfigMap
metadata:
  name: my-janssen-config-tls-script
  namespace: default
  labels:
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
---
# Source: janssen/charts/config/templates/configmaps.yaml
apiVersion: v1
data:
  updatelbip.py: |-
    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-

    # Update the IP of the load balancer automatically

    """
     License terms and conditions for Janssen Cloud Native Edition:
     https://www.apache.org/licenses/LICENSE-2.0
    """

    import socket
    import os
    import logging
    import time

    logger = logging.getLogger("update-lb-ip")
    logger.setLevel(logging.INFO)
    ch = logging.StreamHandler()
    fmt = logging.Formatter('%(levelname)s - %(asctime)s - %(message)s')
    ch.setFormatter(fmt)
    logger.addHandler(ch)


    def backup(hosts):
        timenow = time.strftime("%c")
        timestamp = "Backup occurred %s \n" % timenow
        logger.info("Backing up hosts file to /etc/hosts.back ...")
        with open('/etc/hosts.back', 'a+') as f:
            f.write(timestamp)
            for line in hosts:
                f.write(line)


    def get_hosts(lb_addr, domain):
        ip_list = []
        hosts_list = []
        ais = socket.getaddrinfo(lb_addr, 0, 0, 0, 0)
        for result in ais:
            ip_list.append(result[-1][0])
        ip_list = list(set(ip_list))
        for ip in ip_list:
            add_host = ip + " " + domain
            hosts_list.append(add_host)

        return hosts_list


    def main():
        try:
            while True:
                lb_addr = os.environ.get("LB_ADDR", "")
                domain = os.environ.get("DOMAIN", "demoexample.jans.io")
                host_file = open('/etc/hosts', 'r').readlines()
                hosts = get_hosts(lb_addr, domain)
                stop = []
                for host in hosts:
                    for i in host_file:
                        if host.replace(" ", "") in i.replace(" ", ""):
                            stop.append("found")
                if len(stop) != len(hosts):
                    backup(host_file)
                    logger.info("Writing new hosts file")
                    with open('/etc/hosts', 'w') as f:
                        for line in host_file:
                            if domain not in line:
                                f.write(line)
                        for host in hosts:
                            f.write(host)
                            f.write("\n")
                        f.write("\n")
                time.sleep(300)
        except KeyboardInterrupt:
            logger.warning("Canceled by user; exiting ...")


    if __name__ == "__main__":
        main()

kind: ConfigMap
metadata:
  name: my-janssen-updatelbip
  namespace: default
  labels:
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
---
# Source: janssen/charts/config/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-janssen-default-cluster-admin-binding
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: User
    # change it to your actual account; the email can be fetched using
    # the following command: `gcloud info | grep Account`
    name: "ACCOUNT"
    apiGroup: rbac.authorization.k8s.io
---
# Source: janssen/charts/config/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: config-load
  name: my-janssen-default-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
---
# Source: janssen/charts/config/templates/roles.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-janssen-default-cn-role
  namespace: default
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: [""] # "" refers to the core API group
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
# Source: janssen/charts/config/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-janssen-default-rolebinding
  namespace: default
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
subjects:
- kind: User
  name: system:serviceaccount:default:default # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role # this must be Role or ClusterRole
  name: my-janssen-default-cn-role # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
---
# Source: janssen/charts/auth-server/templates/service.yml
apiVersion: v1
kind: Service
metadata:
  name: auth-server
  namespace: default
  labels:
    APP_NAME: auth-server
    app: my-janssen-auth-server
    helm.sh/chart: auth-server-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 8080
    name: http-auth
  selector:
    app: my-janssen-auth-server #auth-server
  sessionAffinity: None
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
---
# Source: janssen/charts/casa/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: casa
  namespace: default
  labels:
    APP_NAME: casa
    app: my-janssen-casa
    helm.sh/chart: casa-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8080
      name: http-casa
  selector:
    app: my-janssen-casa
    app.kubernetes.io/instance: my-janssen
  sessionAffinity: None
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
---
# Source: janssen/charts/config-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  # the name must match the application
  name: config-api
  namespace: default
  labels:
    APP_NAME: config-api
    app: my-janssen-config-api
    helm.sh/chart: config-api-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 9444
      name: tcp-config-api-ssl
    - port: 8074
      name: tcp-config-api-http
  selector:
    app: my-janssen-config-api
  sessionAffinity: None
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
---
# Source: janssen/charts/fido2/templates/service.yml
apiVersion: v1
kind: Service
metadata:
  name: fido2
  namespace: default
  labels:
    APP_NAME: fido2
    app: my-janssen-fido2
    helm.sh/chart: fido2-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 8080
    name: http-fido2
  selector:
    app: my-janssen-fido2 #fido2
  sessionAffinity: None
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
---
# Source: janssen/charts/scim/templates/service.yml
apiVersion: v1
kind: Service
metadata:
  name: scim
  namespace: default
  labels:
    APP_NAME: scim
    app: my-janssen-scim
    helm.sh/chart: scim-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 8080
    name: http-scim
  selector:
    app: my-janssen-scim #scim
  sessionAffinity: None
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
---
# Source: janssen/charts/auth-server/templates/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-janssen-auth-server
  namespace: default
  labels:
    APP_NAME: auth-server
    app: my-janssen-auth-server
    helm.sh/chart: auth-server-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-janssen-auth-server
  template:
    metadata:
      labels:
        APP_NAME: auth-server
        app: my-janssen-auth-server
    spec:
      dnsPolicy: ""
      serviceAccountName: default
      containers:
      - name: auth-server
        imagePullPolicy: IfNotPresent
        image: ghcr.io/janssenproject/jans/auth-server:1.1.5-1
        env:
          - name: CN_AUTH_JAVA_OPTIONS
            value: "-XX:MaxDirectMemorySize=1025m -Xmx1275m"                    
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
        ports:
        - name: http-auth
          containerPort: 8080
        
        envFrom:
        - configMapRef:
            name: my-janssen-config-cm
        
        
        lifecycle:
          {}
        volumeMounts:
          - mountPath: /etc/jans/conf/configuration.json
            name: my-janssen-configuration-file
            subPath: configuration.json
        livenessProbe:
          exec:
            command:
            - python3
            - /app/scripts/healthcheck.py
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - python3
            - /app/scripts/healthcheck.py
          initialDelaySeconds: 25
          periodSeconds: 25
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 2500m
            memory: 2500Mi
          requests:
            cpu: 2500m
            memory: 2500Mi
      hostAliases:
      - ip: 22.22.22.22
        hostnames:
        - demoexample.jans.io
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
---
# Source: janssen/charts/casa/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-janssen-casa
  namespace: default
  labels:
    APP_NAME: casa
    app: my-janssen-casa
    helm.sh/chart: casa-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-janssen-casa
      app.kubernetes.io/instance: my-janssen
  template:
    metadata:
      labels:
        APP_NAME: casa
        app: my-janssen-casa
        app.kubernetes.io/instance: my-janssen
    spec:
      dnsPolicy: ""
      securityContext:
        {}
      serviceAccountName: default
      containers:
        - name: casa
          securityContext:
            {}
          image: "ghcr.io/janssenproject/jans/casa:1.1.5-1"
          env:
            - name: CN_CASA_JAVA_OPTIONS
              value: "-XX:MaxDirectMemorySize=205m -Xmx255m"                        
          imagePullPolicy: IfNotPresent
          ports:
            - name: http-casa
              containerPort: 8080
              protocol: TCP
            
          envFrom:
            - configMapRef:
                name: my-janssen-config-cm
            
            
          lifecycle:
            {}
          volumeMounts:
            - mountPath: /etc/jans/conf/configuration.json
              name: my-janssen-configuration-file
              subPath: configuration.json
          livenessProbe:
            httpGet:
              path: /jans-casa/health-check
              port: http-casa
            initialDelaySeconds: 25
            periodSeconds: 25
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /jans-casa/health-check
              port: http-casa
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 500Mi
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
      hostAliases:
       - ip: 22.22.22.22
         hostnames:
         - demoexample.jans.io
---
# Source: janssen/charts/config-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-janssen-config-api
  namespace: default
  labels:
    APP_NAME: config-api
    app: my-janssen-config-api
    helm.sh/chart: config-api-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-janssen-config-api
  template:
    metadata:
      labels:
        app: my-janssen-config-api
        release: my-janssen
    spec:
      dnsPolicy: ""
      serviceAccountName: default
      containers:
        - name: config-api
          image: "ghcr.io/janssenproject/jans/config-api:1.1.5-1"
          env:
            - name: CN_CONFIG_API_JAVA_OPTIONS
              value: "-XX:MaxDirectMemorySize=492m -Xmx612m"                        
          securityContext:
            runAsUser: 1000
            runAsNonRoot: true
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9444
            - containerPort: 8074
            
          envFrom:
            - configMapRef:
                name: my-janssen-config-cm
          livenessProbe:
            httpGet:
              path: /jans-config-api/api/v1/health/live
              port: 8074
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: jans-config-api/api/v1/health/ready
              port: 8074
            initialDelaySeconds: 25
            periodSeconds: 25
            timeoutSeconds: 5
          lifecycle:
            {}
          volumeMounts:
            - mountPath: /etc/jans/conf/configuration.json
              name: my-janssen-configuration-file
              subPath: configuration.json
          resources:
            limits:
              cpu: 1000m
              memory: 1200Mi
            requests:
              cpu: 1000m
              memory: 1200Mi
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
      hostAliases:
      - ip: 22.22.22.22
        hostnames:
        - demoexample.jans.io
---
# Source: janssen/charts/fido2/templates/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-janssen-fido2
  namespace: default
  labels:
    APP_NAME: fido2
    app: my-janssen-fido2
    helm.sh/chart: fido2-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-janssen-fido2
  template:
    metadata:
      labels:
        APP_NAME: fido2
        app: my-janssen-fido2
    spec:
      dnsPolicy: ""
      serviceAccountName: default
      containers:
      - name: fido2
        imagePullPolicy: IfNotPresent
        image: ghcr.io/janssenproject/jans/fido2:1.1.5-1
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
        env:
          - name: CN_FIDO2_JAVA_OPTIONS
            value: "-XX:MaxDirectMemorySize=205m -Xmx255m"                    
        ports:
        - name: http-fido2
          containerPort: 8080
        
        envFrom:
        - configMapRef:
            name: my-janssen-config-cm
        
        
        lifecycle:
          {}
        volumeMounts:
          - mountPath: /etc/jans/conf/configuration.json
            name: my-janssen-configuration-file
            subPath: configuration.json
        livenessProbe:
          httpGet:
            path: /jans-fido2/sys/health-check
            port: http-fido2
          initialDelaySeconds: 25
          periodSeconds: 25
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /jans-fido2/sys/health-check
            port: http-fido2
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 500m
            memory: 500Mi
      hostAliases:
      - ip: 22.22.22.22
        hostnames:
        - demoexample.jans.io
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
---
# Source: janssen/charts/scim/templates/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-janssen-scim
  namespace: default
  labels:
    APP_NAME: scim
    app: my-janssen-scim
    helm.sh/chart: scim-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-janssen-scim
  template:
    metadata:
      labels:
        APP_NAME: scim
        app: my-janssen-scim
    spec:
      dnsPolicy: ""
      serviceAccountName: default
      containers:
      - name: scim
        imagePullPolicy: IfNotPresent
        image: ghcr.io/janssenproject/jans/scim:1.1.5-1
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
        env:
          - name: CN_SCIM_JAVA_OPTIONS
            value: "-XX:MaxDirectMemorySize=492m -Xmx612m"                    
        resources:
          limits:
            cpu: 1000m
            memory: 1200Mi
          requests:
            cpu: 1000m
            memory: 1200Mi
        ports:
        - name: http-scim
          containerPort: 8080
        
        envFrom:
        - configMapRef:
            name: my-janssen-config-cm
        
        
        lifecycle:
          {}
        volumeMounts:
          - mountPath: /etc/jans/conf/configuration.json
            name: my-janssen-configuration-file
            subPath: configuration.json
        livenessProbe:
          httpGet:
            path: /jans-scim/sys/health-check
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /jans-scim/sys/health-check
            port: 8080
          initialDelaySeconds: 25
          periodSeconds: 25
          timeoutSeconds: 5
      hostAliases:
      - ip: 22.22.22.22
        hostnames:
        - demoexample.jans.io
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
---
# Source: janssen/charts/auth-server/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-janssen-auth-server
  labels:
    APP_NAME: auth-server
    app: my-janssen-auth-server
    helm.sh/chart: auth-server-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-janssen-auth-server
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
---
# Source: janssen/charts/casa/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-janssen-casa
  labels:
    APP_NAME: casa
    app: my-janssen-casa
    helm.sh/chart: casa-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-janssen-casa
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
---
# Source: janssen/charts/config-api/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-janssen-config-api
  labels:
    APP_NAME: config-api
    app: my-janssen-config-api
    helm.sh/chart: config-api-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-janssen-config-api
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
---
# Source: janssen/charts/fido2/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-janssen-fido2
  labels:
    APP_NAME: fido2
    app: my-janssen-fido2
    helm.sh/chart: fido2-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-janssen-fido2
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
---
# Source: janssen/charts/scim/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-janssen-scim
  labels:
    APP_NAME: scim
    app: my-janssen-scim
    helm.sh/chart: scim-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-janssen-scim
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
---
# Source: janssen/charts/config/templates/load-init-config.yml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-janssen-config
  namespace: default
  labels:
    APP_NAME: configurator
    app: my-janssen-config-init-load
    helm.sh/chart: config-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      name: config-job
      labels:
        APP_NAME: configurator
        app: my-janssen-config-init-load
    spec:
      dnsPolicy: ""
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
        - name: my-janssen-config-tls-script
          configMap:
            name: my-janssen-config-tls-script
      serviceAccountName: default
      containers:
      - name: config
        image: "ghcr.io/janssenproject/jans/configurator:1.1.5-1"
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
        env:                        
        lifecycle:
          {}
        volumeMounts:
          - mountPath: /etc/jans/conf/configuration.json
            name: my-janssen-configuration-file
            subPath: configuration.json
          - mountPath: /scripts/tls_generator.py
            name: my-janssen-config-tls-script
            subPath: tls_generator.py
        envFrom:
        - configMapRef:
            name: my-janssen-config-cm
        
        
        resources:
          limits:
            cpu: 300m
            memory: 300Mi
          requests:
            cpu: 300m
            memory: 300Mi
        command:
          - tini
          - -g
          - --
          - /bin/sh
          - -c
          - |
              /app/scripts/entrypoint.sh load && /usr/bin/python3 /scripts/tls_generator.py
      restartPolicy: Never
---
# Source: janssen/charts/persistence/templates/jobs.yml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-janssen-persistence
  namespace: default
  labels:
    APP_NAME: persistence-loader
    app: my-janssen-persistence
    helm.sh/chart: persistence-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      name: persistence
      labels:
        APP_NAME: persistence-loader
        app: my-janssen-persistence
    spec:
      dnsPolicy: ""
      restartPolicy: Never
      serviceAccountName: default
      containers:
      - name: persistence
        image: "ghcr.io/janssenproject/jans/persistence-loader:1.1.5-1"
        securityContext:
          runAsUser: 1000
          runAsNonRoot: true
        env:                        
        envFrom:
        - configMapRef:
            name: my-janssen-config-cm
        
        
        lifecycle:
          {}
        volumeMounts:
          - mountPath: /etc/jans/conf/configuration.json
            name: my-janssen-configuration-file
            subPath: configuration.json
        resources:
          limits:
            cpu: 300m
            memory: 300Mi
          requests:
            cpu: 300m
            memory: 300Mi
      volumes:
        - name: my-janssen-configuration-file
          secret:
            secretName: my-janssen-configuration-file
---
# Source: janssen/charts/auth-server-key-rotation/templates/cronjobs.yaml
kind: CronJob
apiVersion: batch/v1
metadata:
  name: my-janssen-auth-server-key-rotation
  namespace: default
  labels:
    APP_NAME: auth-server-key-rotation
    release: my-janssen
    app: my-janssen-auth-server-key-rotation
    helm.sh/chart: auth-server-key-rotation-1.1.5
    app.kubernetes.io/instance: my-janssen
    app.kubernetes.io/version: "1.1.5"
    app.kubernetes.io/managed-by: Helm
spec:
  schedule: "@every 48h"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          dnsPolicy: ""
          serviceAccountName: default
          containers:
            - name: auth-server-key-rotation
              image: "ghcr.io/janssenproject/jans/certmanager:1.1.5-1"
              env:                                
              imagePullPolicy: IfNotPresent
              lifecycle:
                {}
              volumeMounts:
                - mountPath: /etc/jans/conf/configuration.json
                  name: my-janssen-configuration-file
                  subPath: configuration.json
              envFrom:
                - configMapRef:
                    name: my-janssen-config-cm
                
                
              resources:
                limits:
                  cpu: 300m
                  memory: 300Mi
                requests:
                  cpu: 300m
                  memory: 300Mi
              args: ["patch", "auth", "--opts", "interval:48", "--opts", "key-strategy:NEWER", "--opts", "privkey-push-delay:0", "--opts", "privkey-push-strategy:NEWER"]
          volumes:
            - name: my-janssen-configuration-file
              secret:
                secretName: my-janssen-configuration-file
          restartPolicy: Never
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-openid-config
  labels:
    app: my-janssen-nginx-ingress-openid-config
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/.well-known/openid-configuration
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /.well-known/openid-configuration
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-device-code
  labels:
    app: my-janssen-nginx-ingress-device-code
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/device_authorization.htm
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /device-code
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-firebase-messaging
  labels:
    app: my-janssen-nginx-ingress-firebase-messaging
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/firebase-messaging-sw.js
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /firebase-messaging-sw.js
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-uma2-config
  labels:
    app: my-janssen-nginx-ingress-uma2-config
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/restv1/uma2-configuration
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /.well-known/uma2-configuration
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-webfinger
  labels:
    app: my-janssen-nginx-ingress-webfinger
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/.well-known/webfinger
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /.well-known/webfinger
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-webdiscovery
  labels:
    app: my-janssen-nginx-ingress-webdiscovery
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/.well-known/simple-web-discovery
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /.well-known/simple-web-discovery
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-config-api
  labels:
    app: my-janssen-nginx-ingress-config-api
  annotations:
    nginx.org/ssl-services: "configapi"
    nginx.ingress.kubernetes.io/proxy-next-upstream: "error timeout invalid_header http_500 http_502 http_503 http_504"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /jans-config-api
            pathType: Prefix
            backend:
              service:
                name: config-api
                port:
                  number: 8074
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-u2f-config
  labels:
    app: my-janssen-nginx-ingress-u2f-config
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/rewrite-target: /jans-auth/restv1/fido-configuration
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /.well-known/fido-configuration
            pathType: Exact
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-janssen-nginx-ingress-auth-server
  labels:
    app: my-janssen-nginx-ingress-auth-server
  annotations:
    nginx.org/ssl-services: "auth-server"
    nginx.ingress.kubernetes.io/proxy-next-upstream: "error timeout invalid_header http_500 http_502 http_503 http_504"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - "demoexample.jans.io"
      secretName: tls-certificate
  rules:
    - host: "demoexample.jans.io"
      http:
        paths:
          - path: /jans-auth
            pathType: Prefix
            backend:
              service:
                name: auth-server
                port:
                  number: 8080
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
---
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
---
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
---
---
# Source: janssen/charts/nginx-ingress/templates/ingress.yaml
---
