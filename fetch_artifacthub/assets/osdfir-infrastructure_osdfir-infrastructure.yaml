---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/primary/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-osdfir-infrastructure-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
    app.kubernetes.io/component: primary
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-osdfir-infrastructure-timesketch-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: timesketch-redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-osdfir-infrastructure-turbinia-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: turbinia-redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-osdfir-infrastructure-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: osdfir-infrastructure/charts/timesketch/charts/opensearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "opensearch-cluster-master-pdb"
  labels:
    helm.sh/chart: opensearch-2.20.0
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "2.14.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: opensearch-cluster-master
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-osdfir-infrastructure-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
automountServiceAccountToken: false
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: osdfir-infrastructure/charts/timesketch/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: timesketch
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: osdfir-infrastructure/charts/turbinia/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: turbinia
  namespace: "default"
  labels:
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-osdfir-infrastructure-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/replicas/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-osdfir-infrastructure-redis-replica
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: osdfir-infrastructure/charts/yeti/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: yeti
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
type: Opaque
data:
  postgres-password: "UFNRdXA0R09kQQ=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-timesketch-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
type: Opaque
data:
  redis-password: "akFDUzJGVElUSQ=="
---
# Source: osdfir-infrastructure/charts/timesketch/templates/secret-oidc-allowed-users.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-timesketch-access-list
  namespace: "default"
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
type: Opaque
data:
  authenticated-emails-list: ""
---
# Source: osdfir-infrastructure/charts/timesketch/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-timesketch-secret
  namespace: "default"
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  timesketch-secret: "dm83Wmg3S0NJWllmRzE3Q0c1bWdBS291N1FCZ2s4MWg="
  timesketch-user: "dXF0dEpBdmZLdEFpelhzZQ=="
---
# Source: osdfir-infrastructure/charts/turbinia/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-turbinia-secret
  namespace: "default"
  labels:
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
stringData:
  turbinia-secret: |
    {
        "default": {
            "description": "This file is used by turbinia-client to determine the location of the API server and if authentication will be used. These options should match your Turbinia deployment.",
            "comments": "By default, the credentials and client secrets files are located in the user's home directory.",
            "API_SERVER_ADDRESS": "http://localhost",
            "API_SERVER_PORT": 8000,
            "API_AUTHENTICATION_ENABLED": false,
            "CLIENT_SECRETS_FILENAME": ".client_secrets.json",
            "CREDENTIALS_FILENAME": ".credentials_default.json"
        }
    }
---
# Source: osdfir-infrastructure/charts/yeti/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-osdfir-infrastructure-yeti-secret
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  yeti-user: "YzY3RVNNQzBCcjdVaWJHbklzMXJIWE4xc3k0dGM1amY="
  yeti-arangodb: "NTVWRXExaDBFdFg0YXZCeg=="
  yeti-api: "NzI5NTc4NzU0NjQwMjI3NTE4NDcxNTE0NTgzNjc1OTE2MjYwOTEzMDIzMTA5OTg5MDQyODY5NzgxNDA5NDY3OQ=="
---
# Source: osdfir-infrastructure/charts/timesketch/charts/opensearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opensearch-cluster-master-config
  labels:
    helm.sh/chart: opensearch-2.20.0
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "2.14.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: opensearch-cluster-master
data:
  opensearch.yml: |
    plugins:
      security:
        allow_unsafe_democertificates: false
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: osdfir-infrastructure/charts/timesketch/templates/init-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-timesketch-init-configmap
  namespace: "default"
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  init-timesketch.sh: |
    #!/bin/sh
    set -e

    # Create timesketch config directory
    mkdir -p /etc/timesketch
    cd /etc/timesketch

    if [ $(ls /tmp/timesketch/ | wc -l) -gt 0 ]; then
      echo "Using existing configuration files provided."
      cp /tmp/timesketch/* /etc/timesketch/
    else
      echo -n "* Fetching configuration files.." 
      GITHUB_BASE_URL="https://raw.githubusercontent.com/google/timesketch/master"
      # Fetch default Timesketch config files
      git clone https://github.com/google/timesketch.git
      cp -r timesketch/data/* /etc/timesketch/
      rm -rf timesketch
      echo "OK"
    fi

    # Set up the Redis connection
    sed -i 's#^CELERY_BROKER_URL =.*#CELERY_BROKER_URL = "redis://default:'$REDIS_PASSWORD'@my-osdfir-infrastructure-timesketch-redis-master:6379"#' timesketch.conf
    sed -i 's#^CELERY_RESULT_BACKEND =.*#CELERY_RESULT_BACKEND = "redis://default:'$REDIS_PASSWORD'@my-osdfir-infrastructure-timesketch-redis-master:6379"#' timesketch.conf

    # Set up the Postgresql connection
    sed -i 's#postgresql://<USERNAME>:<PASSWORD>@localhost/timesketch#postgresql://postgres:'$POSTGRES_PASSWORD'@my-osdfir-infrastructure-postgresql:5432/timesketch#' timesketch.conf

    # Set up the Opensearch connection
    sed -i 's#^OPENSEARCH_HOST =.*#OPENSEARCH_HOST = "opensearch-cluster-master"#' timesketch.conf

    # Set up secret
    sed -i 's#^SECRET_KEY =.*#SECRET_KEY = "'$TIMESKETCH_SECRET'"#' timesketch.conf

    # Set up upload folder
    sed -i 's#^UPLOAD_ENABLED = False#UPLOAD_ENABLED = True#' timesketch.conf
    sed -i 's#^UPLOAD_FOLDER =.*#UPLOAD_FOLDER =  "/mnt/osdfirvolume/upload"#' timesketch.conf

    # Update scenarios paths
    sed -i 's#^SCENARIOS_PATH =.*#SCENARIOS_PATH = "/etc/timesketch/scenarios.yaml"#' timesketch.conf
    sed -i 's#^FACETS_PATH =.*#FACETS_PATH = "/etc/timesketch/facets.yaml"#' timesketch.conf
    sed -i 's#^QUESTIONS_PATH =.*#QUESTIONS_PATH = "/etc/timesketch/questions.yaml"#' timesketch.conf

    # OIDC integration

    # Yeti integration
    sed -i 's#^YETI_API_KEY =.*#YETI_API_KEY = "'$YETI_API_KEY'"#' timesketch.conf
    sed -i 's#^YETI_API_ROOT =.*#YETI_API_ROOT =  "http://my-osdfir-infrastructure-yeti:9000/api/v2"#' timesketch.conf
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: osdfir-infrastructure/charts/turbinia/templates/init-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-turbinia-init-configmap
  namespace: "default"
  labels:
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  init-turbinia.sh: |
    #!/bin/sh
    set -e
    apk add jq

    # Fix Filestore permissions
    chmod go+w /mnt/turbiniavolume

    # Create turbinia config directory
    mkdir -p /etc/turbinia
    cd /etc/turbinia

    if [  $(ls /tmp/turbinia/ | wc -l) -gt 0 ]; then
      echo "Using exisiting configuration file provided."
      cp /tmp/turbinia/* /etc/turbinia/turbinia.conf
    else
      # Pull default config if one is not already provided
      echo -n "* Fetching the release Turbinia configuration file..." 
      RELEASE_TAG=$(wget -O- https://api.github.com/repos/google/turbinia/releases | jq -r '.[0] | .tag_name')
      wget "https://raw.githubusercontent.com/google/turbinia/$RELEASE_TAG/turbinia/config/turbinia_config_tmpl.py" -O turbinia.conf
    fi

    # Set up the Redis connection
    sed -i -e "s/^TASK_MANAGER = .*$/TASK_MANAGER = 'Celery'/g" turbinia.conf
    sed -i -e "s/^STATE_MANAGER = .*$/STATE_MANAGER = 'Redis'/g" turbinia.conf
    sed -i -e 's#^REDIS_HOST =.*#REDIS_HOST = "my-osdfir-infrastructure-turbinia-redis-master"#' turbinia.conf 
    sed -i -e 's#^CELERY_BROKER =.*#CELERY_BROKER = "redis://my-osdfir-infrastructure-turbinia-redis-master:6379"#' turbinia.conf
    sed -i -e 's#^CELERY_BACKEND =.*#CELERY_BACKEND = "redis://my-osdfir-infrastructure-turbinia-redis-master:6379"#' turbinia.conf
    sed -i -e "s/^DEBUG_TASKS = .*$/DEBUG_TASKS = True/g" turbinia.conf
    
    # Setup logging, mount, and output paths
    sed -i -e "s/^SHARED_FILESYSTEM = .*$/SHARED_FILESYSTEM = True/g" turbinia.conf
    sed -i -e "s/^LOG_DIR = .*$/LOG_DIR = '\/mnt\/turbiniavolume\/logs'/g" turbinia.conf
    sed -i -e "s/^MOUNT_DIR_PREFIX = .*$/MOUNT_DIR_PREFIX = '\/mnt\/turbinia'/g" turbinia.conf
    sed -i -e "s/^OUTPUT_DIR = .*$/OUTPUT_DIR = '\/mnt\/turbiniavolume\/output'/g" turbinia.conf
    sed -i -e "s/^API_EVIDENCE_UPLOAD_DIR = .*$/API_EVIDENCE_UPLOAD_DIR = '\/mnt\/turbiniavolume\/upload'/g" turbinia.conf
    
    # Disable Turbinia Jobs
    sed -i -e "s/^DISABLED_JOBS = .*$/DISABLED_JOBS = ['BinaryExtractorJob', 'BulkExtractorJob', 'HindsightJob', 'PhotorecJob', 'VolatilityJob']/g" turbinia.conf
    # Set up Prometheus metrics
    sed -i -e "s/^PROMETHEUS_ENABLED = .*$/PROMETHEUS_ENABLED = True/g" turbinia.conf
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")
    HEADLESS_SERVICE="my-osdfir-infrastructure-redis-headless.default.svc.cluster.local"

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: osdfir-infrastructure/charts/yeti/templates/nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-osdfir-infrastructure-yeti-nginx-configmap
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  default.conf: |
    server {

        root /www;

        location /api/v2 {
            proxy_pass http://my-osdfir-infrastructure-yeti-api:8000;
        }
        
        location ~(^/docs|^/openapi.json) {
            proxy_pass http://my-osdfir-infrastructure-yeti-api:8000;
        }

        location / {
            try_files $uri $uri/ /index.html;
        }
    }
---
# Source: osdfir-infrastructure/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations: 
    helm.sh/resource-policy: keep
  name: osdfirvolume-claim
  namespace: "default"
spec:
  
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "2Gi"
---
# Source: osdfir-infrastructure/charts/timesketch/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: opensearch-cluster-master
  labels:
    helm.sh/chart: opensearch-2.20.0
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "2.14.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: opensearch-cluster-master
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: osdfir-infrastructure/charts/timesketch/charts/opensearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: opensearch-cluster-master-headless
  labels:
    helm.sh/chart: opensearch-2.20.0
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "2.14.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: opensearch-cluster-master
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like opensearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
  - name: metrics
    port: 9600
---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: timesketch-redis
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/component: master
---
# Source: osdfir-infrastructure/charts/timesketch/templates/metrics/web-metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-timesketch-frontend-metrics
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/timesketch/templates/metrics/worker-metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-timesketch-worker-metrics
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/component: worker
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/timesketch/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-timesketch
  namespace: "default"
  labels:
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 5000
      protocol: TCP
      targetPort: 5000
  selector:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: turbinia-redis
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/component: master
---
# Source: osdfir-infrastructure/charts/turbinia/templates/metrics/server-metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-turbinia-server-metrics
  labels:
    monitoring: enabled
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  ports:
    - port: 9200
      targetPort: 9200
      protocol: TCP
      name: http-metrics
  selector:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/turbinia/templates/metrics/worker-metrics-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-turbinia-worker-metrics
  labels:
    monitoring: enabled
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  ports:
    - port: 9200
      targetPort: 9200
      protocol: TCP
      name: http-metrics
  selector:
    app.kubernetes.io/component: worker
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/turbinia/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-turbinia
  labels:
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app.kubernetes.io/component: api
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: redis
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: replica
---
# Source: osdfir-infrastructure/charts/yeti/templates/api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-yeti-api
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app.kubernetes.io/component: api
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/yeti/templates/arangodb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-yeti-arangodb
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 8529
      protocol: TCP
  selector:
    app.kubernetes.io/component: arangodb
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/yeti/templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-osdfir-infrastructure-yeti
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 9000
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
---
# Source: osdfir-infrastructure/charts/timesketch/templates/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-timesketch
  namespace: "default"
  labels:
    app.kubernetes.io/component: frontend
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/name: timesketch
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "X7MVX" 
        prometheus.io/port: "8080"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: timesketch
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: timesketch
      securityContext:
        {}
      initContainers:
        - name: init-timesketch
          image: alpine/git
          command: ['sh', '-c', '/init/init-timesketch.sh']
          env:
            - name: TIMESKETCH_SECRET
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-secret 
                  key: timesketch-secret
            - name: REDIS_PASSWORD
              valueFrom:
                # Referencing from charts/redis/templates/_helpers.tpl
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-redis
                  key: redis-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                # Referencing from charts/postgresql/templates/_helpers.tpl
                secretKeyRef:
                  name: my-osdfir-infrastructure-postgresql
                  key: postgres-password 
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret
                  key: "yeti-api"
          volumeMounts:
            - mountPath: /init
              name: init-timesketch
            - mountPath: /etc/timesketch
              name: timesketch-configs
            - mountPath: /tmp/timesketch
              name: uploaded-configs
            - name: authenticated-emails
              mountPath: /init/authenticated-emails
              readOnly: true
      containers:
        - name: frontend
          securityContext:
            {}
          image: "us-docker.pkg.dev/osdfir-registry/timesketch/timesketch:latest"
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c", "./docker-entrypoint.sh timesketch-web"]
          lifecycle:
            postStart:
              exec:
                command: ["/bin/sh", "-c", "tsctl create-user timesketch --password $TIMESKETCH_USER"]
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
                  apiVersion: v1
            - name: TIMESKETCH_USER
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-secret 
                  key: timesketch-user
          volumeMounts:
            - mountPath: "/mnt/osdfirvolume/upload"
              name: timesketchvolume
              subPath: uploads
            - mountPath: /var/log/timesketch
              name: timesketchvolume
              subPathExpr: logs/$(POD_NAME)
            - mountPath: /etc/timesketch
              name: timesketch-configs
          ports:
            - containerPort: 8080
            - containerPort: 5000
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: timesketchvolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: init-timesketch
          configMap:
            name: my-osdfir-infrastructure-timesketch-init-configmap
            defaultMode: 0744
        - name: timesketch-configs
          emptyDir: {}
        - name: uploaded-configs
          configMap:
            name: my-osdfir-infrastructure-timesketch-configmap
            optional: true
        - name: authenticated-emails
          secret:
            items:
            - key: authenticated-emails-list
              path: authenticated-emails-list
            secretName: my-osdfir-infrastructure-timesketch-access-list
---
# Source: osdfir-infrastructure/charts/timesketch/templates/worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-timesketch-worker
  namespace: "default"
  labels:
    app.kubernetes.io/component: worker
    helm.sh/chart: timesketch-1.0.5
    app.kubernetes.io/name: timesketch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/name: timesketch
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "gB2Rt" 
        prometheus.io/port: "8080"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/name: timesketch
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: timesketch
      securityContext:
        {}
      initContainers:
        - name: init-timesketch
          image: alpine/git
          command: ['sh', '-c', '/init/init-timesketch.sh']
          env:
            - name: TIMESKETCH_SECRET
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-secret 
                  key: timesketch-secret
            - name: REDIS_PASSWORD
              valueFrom:
                # Referencing from charts/redis/templates/_helpers.tpl
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-redis
                  key: redis-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                # Referencing from charts/postgresql/templates/_helpers.tpl
                secretKeyRef:
                  name: my-osdfir-infrastructure-postgresql
                  key: postgres-password 
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret
                  key: "yeti-api"
          volumeMounts:
            - mountPath: /init
              name: init-timesketch
            - mountPath: /etc/timesketch
              name: timesketch-configs
            - mountPath: /tmp/timesketch
              name: uploaded-configs
            - name: authenticated-emails
              mountPath: /init/authenticated-emails
              readOnly: true
      containers:
        - name: worker
          securityContext:
            {}
          image: "us-docker.pkg.dev/osdfir-registry/timesketch/timesketch:latest"
          imagePullPolicy: IfNotPresent
          command: ["sh", "-c", "./docker-entrypoint.sh timesketch-worker"]
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
                  apiVersion: v1
            - name: WORKER_LOG_LEVEL
              value: "DEBUG"
          volumeMounts:
            - mountPath: "/mnt/osdfirvolume/upload"
              name: timesketchvolume
              subPath: uploads
            - mountPath: /var/log/timesketch
              name: timesketchvolume
              subPathExpr: logs/$(POD_NAME)
            - mountPath: /etc/timesketch
              name: timesketch-configs
          ports:
            - containerPort: 8080
            - containerPort: 5000
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
      volumes:
        - name: timesketchvolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: init-timesketch
          configMap:
            name: my-osdfir-infrastructure-timesketch-init-configmap
            defaultMode: 0744
        - name: timesketch-configs
          emptyDir: {}
        - name: uploaded-configs
          configMap:
            name: my-osdfir-infrastructure-timesketch-configmap
            optional: true
        - name: authenticated-emails
          secret:
            items:
            - key: authenticated-emails-list
              path: authenticated-emails-list
            secretName: my-osdfir-infrastructure-timesketch-access-list
---
# Source: osdfir-infrastructure/charts/turbinia/templates/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-turbinia-api
  namespace: "default"
  labels:
    app.kubernetes.io/component: api
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: api
      app.kubernetes.io/name: turbinia
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "4n09j"
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: api
        app.kubernetes.io/name: turbinia
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: turbinia
      securityContext:
        {}
      initContainers:
        - name: init-turbinia
          image: alpine
          command: ['sh', '-c', '/init/init-turbinia.sh']
          env:
          volumeMounts:
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /init
              name: init-turbinia
            - mountPath: /etc/turbinia
              name: turbinia-configs
            - mountPath: /tmp/turbinia
              name: user-configs 
      containers:
        - name: api
          securityContext:
            {}
          image: "us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-api-server:latest"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command: ['sh', '-c', "ps aux | grep api_server | grep -v grep"]
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: TURBINIA_EXTRA_ARGS
              value: "-d"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - mountPath: /var/log
              name: logs
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /etc/turbinia
              name: turbinia-configs
          ports:
            - containerPort: 9200
            - containerPort: 8000
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: turbiniavolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: logs
          emptyDir: {}
        - name: init-turbinia
          configMap:
            name: my-osdfir-infrastructure-turbinia-init-configmap
            defaultMode: 0777
        - name: turbinia-configs
          emptyDir: {}
        - name: user-configs
          configMap:
            name: my-osdfir-infrastructure-turbinia-configmap
            optional: true
---
# Source: osdfir-infrastructure/charts/turbinia/templates/server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-turbinia-server
  namespace: "default"
  labels:
    app.kubernetes.io/component: server
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: server
      app.kubernetes.io/name: turbinia
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "qebYT"
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: server
        app.kubernetes.io/name: turbinia
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: turbinia
      securityContext:
        {}
      initContainers:
        - name: init-turbinia
          image: alpine
          command: ['sh', '-c', '/init/init-turbinia.sh']
          env:
          volumeMounts:
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /init
              name: init-turbinia
            - mountPath: /etc/turbinia
              name: turbinia-configs
            - mountPath: /tmp/turbinia
              name: user-configs 
      containers:
        - name: server
          securityContext:
            {}
          image: "us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-server:latest"
          imagePullPolicy: IfNotPresent
          env:
            - name: TURBINIA_EXTRA_ARGS
              value: "-d"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /etc/turbinia
              name: turbinia-configs
          ports:
            - containerPort: 9200
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: turbiniavolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: init-turbinia
          configMap:
            name: my-osdfir-infrastructure-turbinia-init-configmap
            defaultMode: 0744
        - name: turbinia-configs
          emptyDir: {}
        - name: user-configs
          configMap:
            name: my-osdfir-infrastructure-turbinia-configmap
            optional: true
---
# Source: osdfir-infrastructure/charts/turbinia/templates/worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-turbinia-worker
  namespace: "default"
  labels:
    app.kubernetes.io/component: worker
    helm.sh/chart: turbinia-1.1.1
    app.kubernetes.io/name: turbinia
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/name: turbinia
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "CjRCO"
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/name: turbinia
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: turbinia
      securityContext:
        {}
      initContainers:
        - name: init-turbinia
          image: alpine
          command: ['sh', '-c', '/init/init-turbinia.sh']
          env:
          volumeMounts:
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /init
              name: init-turbinia
            - mountPath: /etc/turbinia
              name: turbinia-configs
            - mountPath: /tmp/turbinia
              name: user-configs 
      # The grace period needs to be set to the largest task timeout as
      # set in the turbinia configuration file.
      terminationGracePeriodSeconds: 86400
      containers:
        - name: worker
          securityContext:
            privileged: true
          image: "us-docker.pkg.dev/osdfir-registry/turbinia/release/turbinia-worker:latest"
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                  - "/bin/sh"
                  - "-c"
                  - "touch /tmp/turbinia-to-scaledown.lock && sleep 5 && /usr/bin/python3 /home/turbinia/check-lockfile.py"
          env:
            - name: TURBINIA_EXTRA_ARGS
              value: "-d"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - mountPath: /mnt/turbiniavolume
              name: turbiniavolume
            - mountPath: /etc/turbinia
              name: turbinia-configs
          ports:
            - containerPort: 9200
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
      volumes:
        - name: turbiniavolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: init-turbinia
          configMap:
            name: my-osdfir-infrastructure-turbinia-init-configmap
            defaultMode: 0744
        - name: turbinia-configs
          emptyDir: {}
        - name: user-configs
          configMap:
            name: my-osdfir-infrastructure-turbinia-configmap
            optional: true
---
# Source: osdfir-infrastructure/charts/yeti/templates/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-yeti-api
  namespace: "default"
  labels:
    app.kubernetes.io/component: api
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: api
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "usIAs" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: api
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: api
          securityContext:
            {}
          image: "yetiplatform/yeti:latest"
          imagePullPolicy: Always
          command: ["sh", "-c", "/docker-entrypoint.sh webserver"]
          lifecycle:
            postStart:
              exec:
                command: ["sh", "-c", "poetry run python yetictl/cli.py create-user yeti $YETI_USER_PASSWORD --api_key $YETI_API_KEY --admin"]
          env:
            - name: YETI_REDIS_HOST
              value: my-osdfir-infrastructure-redis-master
            - name: YETI_REDIS_PORT
              value: "6379"
            - name: YETI_REDIS_DATABASE
              value: "0"
            - name: YETI_ARANGODB_HOST
              value: my-osdfir-infrastructure-yeti-arangodb
            - name: YETI_ARANGODB_PORT
              value: "8529"
            - name: YETI_ARANGODB_DATABASE
              value: yeti
            - name: YETI_ARANGODB_USERNAME
              value: root
            - name: YETI_AUTH_SECRET_KEY
              value: "KNbO3r0KFH2AC8Xvr8IQ3se8LQ1K5eXQ"
            - name: YETI_AUTH_ALGORITHM
              value: HS256
            - name: YETI_AUTH_ACCESS_TOKEN_EXPIRE_MINUTES
              value: "30"
            - name: YETI_AUTH_ENABLED
              value: "True"
            - name: YETI_SYSTEM_PLUGINS_PATH
              value: "./plugins"
            - name: YETI_USER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-user
            - name: YETI_ARANGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-arangodb
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-api
            - name: YETI_TIMESKETCH_ENDPOINT
              value: "http://my-osdfir-infrastructure-timesketch:5000"
            - name: YETI_TIMESKETCH_USERNAME
              value: timesketch
            - name: YETI_TIMESKETCH_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "my-osdfir-infrastructure-timesketch-secret"
                  key: timesketch-user
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
            - containerPort: 8000
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
---
# Source: osdfir-infrastructure/charts/yeti/templates/arangodb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-yeti-arangodb
  namespace: "default"
  labels:
    app.kubernetes.io/component: arangodb
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: arangodb
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "Xsqad" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: arangodb
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: frontend
          securityContext:
            {}
          image: "arangodb:latest"
          imagePullPolicy: Always
          env:
            - name: ARANGO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-arangodb
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
            - containerPort: 8529
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
---
# Source: osdfir-infrastructure/charts/yeti/templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-yeti
  namespace: "default"
  labels:
    app.kubernetes.io/component: frontend
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "QsjVu" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: frontend
          securityContext:
            {}
          image: "yetiplatform/yeti-frontend:latest"
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
            - mountPath: /etc/nginx/conf.d/default.conf
              subPath: default.conf
              name: nginx-config
              readOnly: true
          ports:
            - containerPort: 9200
            - containerPort: 80
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
        - name: nginx-config
          configMap:
            name: my-osdfir-infrastructure-yeti-nginx-configmap
---
# Source: osdfir-infrastructure/charts/yeti/templates/tasks-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-osdfir-infrastructure-yeti-tasks
  namespace: "default"
  labels:
    app.kubernetes.io/component: tasks
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: tasks
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-osdfir-infrastructure
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "IfggB" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: tasks
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-osdfir-infrastructure
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: tasks
          securityContext:
            {}
          image: "yetiplatform/yeti:latest"
          imagePullPolicy: Always
          command: ["sh", "-c", "/docker-entrypoint.sh tasks"]
          env:
            - name: YETI_REDIS_HOST
              value: my-osdfir-infrastructure-redis-master
            - name: YETI_REDIS_PORT
              value: "6379"
            - name: YETI_REDIS_DATABASE
              value: "0"
            - name: YETI_ARANGODB_HOST
              value: my-osdfir-infrastructure-yeti-arangodb
            - name: YETI_ARANGODB_PORT
              value: "8529"
            - name: YETI_ARANGODB_DATABASE
              value: yeti
            - name: YETI_ARANGODB_USERNAME
              value: root
            - name: YETI_AUTH_SECRET_KEY
              value: "ioPQslFT67hQiWrvcJojUjy2PRvriMZT"
            - name: YETI_AUTH_ALGORITHM
              value: HS256
            - name: YETI_AUTH_ACCESS_TOKEN_EXPIRE_MINUTES
              value: "30"
            - name: YETI_AUTH_ENABLED
              value: "True"
            - name: YETI_SYSTEM_PLUGINS_PATH
              value: "./plugins"
            - name: YETI_USER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-user
            - name: YETI_ARANGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-arangodb
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-yeti-secret 
                  key: yeti-api
            - name: YETI_TIMESKETCH_ENDPOINT
              value: "http://my-osdfir-infrastructure-timesketch:5000"
            - name: YETI_TIMESKETCH_USERNAME
              value: timesketch
            - name: YETI_TIMESKETCH_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: "my-osdfir-infrastructure-timesketch-secret"
                  key: timesketch-user
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: osdfirvolume-claim
            readOnly: false
---
# Source: osdfir-infrastructure/charts/timesketch/charts/opensearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: opensearch-cluster-master
  labels:
    helm.sh/chart: opensearch-2.20.0
    app.kubernetes.io/name: opensearch
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/version: "2.14.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: opensearch-cluster-master
  annotations:
    majorVersion: "2"
spec:
  serviceName: opensearch-cluster-master-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: opensearch
      app.kubernetes.io/instance: my-osdfir-infrastructure
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: opensearch-cluster-master
    spec:
      accessModes:
      - "ReadWriteOnce"
      resources:
        requests:
          storage: "2Gi"
  template:
    metadata:
      name: "opensearch-cluster-master"
      labels:
        helm.sh/chart: opensearch-2.20.0
        app.kubernetes.io/name: opensearch
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/version: "2.14.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: opensearch-cluster-master
      annotations:
        configchecksum: 5b00f59ccff671a0fe5272e9f35ea2f6808c7e9708afdf5ae4da9976c253239
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: false
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/instance
                  operator: In
                  values:
                  - my-osdfir-infrastructure
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - opensearch
      terminationGracePeriodSeconds: 120
      volumes:
      - name: config
        configMap:
          name: opensearch-cluster-master-config
      - emptyDir: {}
        name: config-emptydir
      enableServiceLinks: true
      initContainers:
      - name: fsgroup-volume
        image: "busybox:latest"
        imagePullPolicy: "IfNotPresent"
        command: ['sh', '-c']
        args:
          - 'chown -R 1000:1000 /usr/share/opensearch/data'
        securityContext:
          runAsUser: 0
        resources:
          {}
        volumeMounts:
          - name: "opensearch-cluster-master"
            mountPath: /usr/share/opensearch/data
      - name: sysctl
        image: "busybox:latest"
        imagePullPolicy: "IfNotPresent"
        command:
        - sh
        - -c
        - |
          set -xe
          DESIRED="262144"
          CURRENT=$(sysctl -n vm.max_map_count)
          if [ "$DESIRED" -gt "$CURRENT" ]; then
            sysctl -w vm.max_map_count=$DESIRED
          fi
        securityContext:
          runAsUser: 0
          privileged: true
        resources:
          {}
      - name: configfile
        image: "opensearchproject/opensearch:2.14.0"
        imagePullPolicy: "IfNotPresent"
        command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash
          cp -r /tmp/configfolder/*  /tmp/config/
        resources:
          {}
        volumeMounts:
          - mountPath: /tmp/config/
            name: config-emptydir
          - name: config
            mountPath: /tmp/configfolder/opensearch.yml
            subPath: opensearch.yml
      containers:
      - name: "opensearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000

        image: "opensearchproject/opensearch:2.14.0"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          failureThreshold: 3
          periodSeconds: 5
          tcpSocket:
            port: 9200
          timeoutSeconds: 3
        startupProbe:
          failureThreshold: 30
          initialDelaySeconds: 5
          periodSeconds: 10
          tcpSocket:
            port: 9200
          timeoutSeconds: 3
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        - name: metrics
          containerPort: 9600
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
        env:
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: cluster.initial_master_nodes
          value: "opensearch-cluster-master-0,"
        - name: discovery.seed_hosts
          value: "opensearch-cluster-master-headless"
        - name: cluster.name
          value: "opensearch-cluster"
        - name: network.host
          value: "0.0.0.0"
        - name: OPENSEARCH_JAVA_OPTS
          value: "-Xmx512M -Xms512M"
        - name: node.roles
          value: "master,ingest,data,remote_cluster_client,"
        - name: DISABLE_INSTALL_DEMO_CONFIG
          value: "true"
        - name: DISABLE_SECURITY_PLUGIN
          value: "true"
        volumeMounts:
        - name: "opensearch-cluster-master"
          mountPath: /usr/share/opensearch/data
        - name: config-emptydir
          mountPath: /usr/share/opensearch/config/opensearch.yml
          subPath: opensearch.yml
---
# Source: osdfir-infrastructure/charts/timesketch/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-osdfir-infrastructure-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.3.0
    helm.sh/chart: postgresql-15.3.2
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: my-osdfir-infrastructure-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: my-osdfir-infrastructure-postgresql
      labels:
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 16.3.0
        helm.sh/chart: postgresql-15.3.2
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: my-osdfir-infrastructure-postgresql
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-osdfir-infrastructure
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:16.3.0-debian-12-r6
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-postgresql
                  key: postgres-password
            - name: POSTGRES_DATABASE
              value: "timesketch"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "dbname=timesketch" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "dbname=timesketch" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/tmp
              subPath: app-tmp-dir
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: osdfir-infrastructure/charts/timesketch/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-osdfir-infrastructure-timesketch-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: timesketch-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: timesketch-redis
      app.kubernetes.io/component: master
  serviceName: my-osdfir-infrastructure-timesketch-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: timesketch-redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 560c33ff34d845009b51830c332aa05fa211444d1877d3526d3599be7543aaa5
        checksum/secret: ffc11e63f38b833206f1bd38e91c8d31bdc42d1653a4ed89229fe48c47d631e6
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-osdfir-infrastructure-timesketch-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-osdfir-infrastructure
                    app.kubernetes.io/name: timesketch-redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-osdfir-infrastructure-timesketch-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-osdfir-infrastructure-timesketch-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-osdfir-infrastructure-timesketch-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-osdfir-infrastructure-timesketch-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-osdfir-infrastructure
          app.kubernetes.io/name: timesketch-redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: osdfir-infrastructure/charts/turbinia/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-osdfir-infrastructure-turbinia-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: turbinia-redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: turbinia-redis
      app.kubernetes.io/component: master
  serviceName: my-osdfir-infrastructure-turbinia-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: turbinia-redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 43cdf68c28f3abe25ce017a82f74dbf2437d1900fd69df51a55a3edf6193d141
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-osdfir-infrastructure-turbinia-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-osdfir-infrastructure
                    app.kubernetes.io/name: turbinia-redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-osdfir-infrastructure-turbinia-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-osdfir-infrastructure-turbinia-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-osdfir-infrastructure-turbinia-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-osdfir-infrastructure
          app.kubernetes.io/name: turbinia-redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-osdfir-infrastructure-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: my-osdfir-infrastructure-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 9b37095f5da9708690e70ad1925097054539b21870dbf3d204b7c5426c05323f
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-osdfir-infrastructure-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-osdfir-infrastructure
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-osdfir-infrastructure-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-osdfir-infrastructure-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-osdfir-infrastructure-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-osdfir-infrastructure
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: osdfir-infrastructure/charts/yeti/charts/redis/templates/replicas/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-osdfir-infrastructure-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-osdfir-infrastructure
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: replica
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-osdfir-infrastructure
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: replica
  serviceName: my-osdfir-infrastructure-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-osdfir-infrastructure
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 9b37095f5da9708690e70ad1925097054539b21870dbf3d204b7c5426c05323f
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-osdfir-infrastructure-redis-replica
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-osdfir-infrastructure
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: my-osdfir-infrastructure-redis-master-0.my-osdfir-infrastructure-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-osdfir-infrastructure-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-osdfir-infrastructure-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-osdfir-infrastructure-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-osdfir-infrastructure
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
