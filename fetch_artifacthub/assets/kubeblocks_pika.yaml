---
# Source: pika/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pika-conf-template
  labels:
    helm.sh/chart: pika-3.5.1
    app.kubernetes.io/name: pika
    app.kubernetes.io/instance: my-pika
    app.kubernetes.io/version: "3.5.1"
    app.kubernetes.io/managed-by: Helm
data:
  pika.conf: |-
    ###########################
    # Pika configuration file #
    ###########################
    
    # Pika port, the default value is 9221.
    # [NOTICE] Port Magic offsets of port+1000 / port+2000 are used by Pika at present.
    # Port 10221 is used for Rsync, and port 11221 is used for Replication, while the listening port is 9221.
    port : 9221
    
    # Random value identifying the Pika server, its string length must be 40.
    # If not set, Pika will generate a random string with a length of 40 random characters.
    # run-id:
    
    # The number of threads for running Pika.
    # It's not recommended to set this value exceeds
    # the number of CPU cores on the deployment server.
    thread-num : 1
    
    # Size of the thread pool, The threads within this pool
    # are dedicated to handling user requests.
    thread-pool-size : 12
    
    # The number of sync-thread for data replication from master, those are the threads work on slave nodes
    # and are used to execute commands sent from master node when replicating.
    sync-thread-num : 6
    
    # Directory to store log files of Pika, which contains multiple types of logs,
    # Including: INFO, WARNING, ERROR log, as well as binglog(write2fine) file which
    # is used for replication.
    log-path : ./log/
    
    # Directory to store the data of Pika.
    db-path : ./db/
    
    # The size of a single RocksDB memtable at the Pika's bottom layer(Pika use RocksDB to store persist data).
    # [Tip] Big write-buffer-size can improve writing performance,
    # but this will generate heavier IO load when flushing from buffer to disk,
    # you should configure it based on you usage scenario.
    # Supported Units [K|M|G], write-buffer-size default unit is in [bytes].
    write-buffer-size : 256M
    
    # The size of one block in arena memory allocation.
    # If <= 0, a proper value is automatically calculated.
    # (usually 1/8 of writer-buffer-size, rounded up to a multiple of 4KB)
    # Supported Units [K|M|G], arena-block-size default unit is in [bytes].
    arena-block-size :
    
    # Timeout of Pika's connection, counting down starts When there are no requests
    # on a connection (it enters sleep state), when the countdown reaches 0, the connection
    # will be closed by Pika.
    # [Tip] The issue of running out of Pika's connections may be avoided if this value
    # is configured properly.
    # The Unit of timeout is in [seconds] and its default value is 60(s).
    timeout : 60
    
    # The [password of administrator], which is empty by default.
    # [NOTICE] If this admin password is the same as user password (including both being empty),
    # the value of userpass will be ignored and all users are considered as administrators,
    # in this scenario, users are not subject to the restrictions imposed by the userblacklist.
    # PS: "user password" refers to value of the parameter below: userpass.
    requirepass :
    
    # Password for replication verify, used for authentication when a slave
    # connects to a master to request replication.
    # [NOTICE] The value of this parameter must match the "requirepass" setting on the master.
    masterauth :
    
    # The [password of user], which is empty by default.
    # [NOTICE] If this user password is the same as admin password (including both being empty),
    # the value of this parameter will be ignored and all users are considered as administrators,
    # in this scenario, users are not subject to the restrictions imposed by the userblacklist.
    # PS: "admin password" refers to value of the parameter above: requirepass.
    userpass :
    
    # The blacklist of commands for users that logged in by userpass,
    # the commands that added to this list will not be available for users except for administrator.
    # [Advice] It's recommended to add high-risk commands to this list.
    # [Format] Commands should be separated by ",". For example: FLUSHALL, SHUTDOWN, KEYS, CONFIG
    # By default, this list is empty.
    userblacklist :
    
    # Running Mode of Pika, The current version only supports running in "classic mode".
    # If set to 'classic', Pika will create multiple DBs whose number is the value of configure item "databases".
    instance-mode : classic
    
    # The number of databases when Pika runs in classic mode.
    # The default database id is DB 0. You can select a different one on
    # a per-connection by using SELECT. The db id range is [0, 'databases' value -1].
    # The value range of this parameter is [1, 8].
    databases : 1
    
    # The number of followers of a master. Only [0, 1, 2, 3, 4] is valid at present.
    # By default, this num is set to 0, which means this feature is [not enabled]
    # and the Pika runs in standalone mode.
    replication-num : 0
    
    # consensus level defines the num of confirms(ACKs) the leader node needs to receive from
    # follower nodes before returning the result to the client that sent the request.
    # The [value range] of this parameter is: [0, ...replicaiton-num].
    # The default value of consensus-level is 0, which means this feature is not enabled.
    consensus-level : 0
    
    # The Prefix of dump file's name.
    # All the files that generated by command "bgsave" will be name with this prefix.
    dump-prefix :
    
    # daemonize  [yes | no].
    #daemonize : yes
    
    # The directory to stored dump files that generated by command "bgsave".
    dump-path : ./dump/
    
    # TTL of dump files that generated by command "bgsave".
    # Any dump files which exceed this TTL will be deleted.
    # Unit of dump-expire is in [days] and the default value is 0(day),
    # which means dump files never expire.
    dump-expire : 0
    
    # Pid file Path of Pika.
    pidfile : ./pika.pid
    
    # The Maximum number of Pika's Connection.
    maxclients : 20000
    
    # The size of sst file in RocksDB(Pika is based on RocksDB).
    # sst files are hierarchical, the smaller the sst file size, the higher the performance and the lower the merge cost,
    # the price is that the number of sst files could be huge. On the contrary, the bigger the sst file size, the lower
    # the performance and the higher the merge cost, while the number of files is fewer.
    # Supported Units [K|M|G], target-file-size-base default unit is in [bytes] and the default value is 20M.
    target-file-size-base : 20M
    
    # Expire-time of binlog(write2file) files that stored within log-path.
    # Any binlog(write2file) files that exceed this expire time will be cleaned up.
    # The unit of expire-logs-days is in [days] and the default value is 7(days).
    # The [Minimum value] of this parameter is 1(day).
    expire-logs-days : 7
    
    # The maximum number of binlog(write2file) files.
    # Once the total number of binlog files exceed this value,
    # automatic cleaning will start to ensure the maximum number
    # of binlog files is equal to expire-logs-nums.
    # The [Minimum value] of this parameter is 10.
    expire-logs-nums : 10
    
    # The number of guaranteed connections for root user.
    # This parameter guarantees that there are 2(By default) connections available
    # for root user to log in Pika from 127.0.0.1, even if the maximum connection limit is reached.
    # PS: The maximum connection refers to the parameter above: maxclients.
    # The default value of root-connection-num is 2.
    root-connection-num : 2
    
    # Slowlog-write-errorlog
    slowlog-write-errorlog : no
    
    # The time threshold for slow log recording.
    # Any command whose execution time exceeds this threshold will be recorded in pika-ERROR.log,
    # which is stored in log-path.
    # The unit of slowlog-log-slower-than is in [microseconds(μs)] and the default value is 10000 μs / 10 ms.
    slowlog-log-slower-than : 10000
    
    # Slowlog-max-len
    slowlog-max-len : 128
    
    # Pika db sync path
    db-sync-path : ./dbsync/
    
    # The maximum Transmission speed during full synchronization.
    # The exhaustion of network can be prevented by setting this parameter properly.
    # The value range of this parameter is [1,1024] with unit in [MB/s].
    # [NOTICE] If this parameter is set to an invalid value(smaller than 0 or bigger than 1024),
    # it will be automatically reset to 1024.
    # The default value of db-sync-speed is -1 (1024MB/s).
    db-sync-speed : -1
    
    # The priority of slave node when electing new master node.
    # The slave node with [lower] value of slave-priority will have [higher priority] to be elected as the new master node.
    # This parameter is only used in conjunction with sentinel and serves no other purpose.
    # The default value of slave-priority is 100.
    slave-priority : 100
    
    # Specify network interface that work with Pika.
    #network-interface : eth1
    
    # The IP and port of the master node are specified by this parameter for
    # replication between master and slaves.
    # [Format] is "ip:port" , for example: "192.168.1.2:6666" indicates that
    # the slave instances that configured with this value will automatically send
    # SLAVEOF command to port 6666 of 192.168.1.2 after startup.
    # This parameter should be configured on slave nodes.
    #slaveof : master-ip:master-port
    
    
    # Daily/Weekly Automatic full compaction task is configured by compact-cron.
    #
    #  [Format-daily]: start time(hour)-end time(hour)/disk-free-space-ratio,
    #  example: with value of "02-04/60", Pika will perform full compaction task between 2:00-4:00 AM everyday if
    #  the disk-free-size / disk-size > 60%.
    #
    #  [Format-weekly]: week/start time(hour)-end time(hour)/disk-free-space-ratio,
    #  example: with value of "3/02-04/60", Pika will perform full compaction task between 2:00-4:00 AM every Wednesday if
    #  the disk-free-size / disk-size > 60%.
    #
    #  [Tip] Automatic full compaction is suitable for scenarios with multiple data structures
    #  and lots of items are expired or deleted, or key names are frequently reused.
    #
    #  [NOTICE]: If compact-interval is set, compact-cron will be masked and disabled.
    #
    #compact-cron : 3/02-04/60
    
    
    # Automatic full synchronization task between a time interval is configured by compact-interval.
    # [Format]: time interval(hour)/disk-free-space-ratio, example: "6/60", Pika will perform full compaction every 6 hours,
    # if the disk-free-size / disk-size > 60%.
    # [NOTICE]: compact-interval is prior than compact-cron.
    #compact-interval :
    
    # This window-size determines the amount of data that can be transmitted in a single synchronization process.
    # [Tip] In the scenario of high network latency. Increasing this size can improve synchronization efficiency.
    # Its default value is 9000. the [maximum] value is 90000.
    sync-window-size : 9000
    
    # Maximum buffer size of a client connection.
    # Only three values are valid here: [67108864(64MB) | 268435456(256MB) | 536870912(512MB)].
    # [NOTICE] Master and slaves must have exactly the same value for the max-conn-rbuf-size.
    # Supported Units [K|M|G]. Its default unit is in [bytes] and its default value is 268435456(256MB).
    max-conn-rbuf-size : 268435456
    
    
    #######################################################################E#######
    #! Critical Settings !#
    #######################################################################E#######
    
    # write_binlog  [yes | no]
    write-binlog : yes
    
    # The size of binlog file, which can not be modified once Pika instance started.
    # [NOTICE] Master and slaves must have exactly the same value for the binlog-file-size.
    # The [value range] of binlog-file-size is [1K, 2G].
    # Supported Units [K|M|G], binlog-file-size default unit is in [bytes] and the default value is 100M.
    binlog-file-size : 104857600
    
    # Automatically triggers a small compaction according to statistics
    # Use the cache to store up to 'max-cache-statistic-keys' keys
    # If 'max-cache-statistic-keys' set to '0', that means turn off the statistics function
    # and this automatic small compaction feature is disabled.
    max-cache-statistic-keys : 0
    
    # When 'delete' or 'overwrite' a specific multi-data structure key 'small-compaction-threshold' times,
    # a small compact is triggered automatically if the small compaction feature is enabled.
    # small-compaction-threshold default value is 5000 and the value range is [1, 100000].
    small-compaction-threshold : 5000
    
    # The maximum total size of all live memtables of the RocksDB instance that owned by Pika.
    # Flushing from memtable to disk will be triggered if the actual memory usage of RocksDB
    # exceeds max-write-buffer-size when next write operation is issued.
    # [RocksDB-Basic-Tuning](https://github.com/facebook/rocksdb/wiki/Setup-Options-and-Basic-Tuning)
    # Supported Units [K|M|G], max-write-buffer-size default unit is in [bytes].
    max-write-buffer-size : 10737418240
    
    # The maximum number of write buffers(memtables) that are built up in memory for one ColumnFamily in DB.
    # The default and the minimum number is 2. It means that Pika(RocksDB) will write to a write buffer
    # when it flushes the data of another write buffer to storage.
    # If max-write-buffer-num > 3, writing will be slowed down.
    max-write-buffer-num : 2
    
    # The maximum size of the response package to client to prevent memory
    # exhaustion caused by commands like 'keys *' and 'Scan' which can generate huge response.
    # Supported Units [K|M|G]. The default unit is in [bytes].
    max-client-response-size : 1073741824
    
    # The compression algorithm. You can not change it when Pika started.
    # Supported types: [snappy, zlib, lz4, zstd]. If you do not wanna compress the SST file, please set its value as none.
    # [NOTICE] The Pika official binary release just linking the snappy library statically, which means that
    # you should compile the Pika from the source code and then link it with other compression algorithm library statically by yourself.
    compression : snappy
    
    # if the vector size is smaller than the level number, the undefined lower level uses the
    # last option in the configurable array, for example, for 3 level
    # LSM tree the following settings are the same:
    # configurable array: [none:snappy]
    # LSM settings: [none:snappy:snappy]
    # When this configurable is enabled, compression is ignored,
    # default l0 l1 noCompression, l2 and more use `compression` option
    # https://github.com/facebook/rocksdb/wiki/Compression
    #compression_per_level : [none:none:snappy:lz4:lz4]
    
    # The number of background flushing threads.
    # max-background-flushes default value is 1 and the value range is [1, 4].
    max-background-flushes : 1
    
    # The number of background compacting threads.
    # max-background-compactions default value is 2 and the value range is [1, 8].
    max-background-compactions : 2
    
    # The number of background threads.
    # max-background-jobs default value is 3 and the value range is [2, 12].
    max-background-jobs : 3
    
    # maximum value of RocksDB cached open file descriptors
    max-cache-files : 5000
    
    # The ratio between the total size of RocksDB level-(L+1) files and the total size of RocksDB level-L files for all L.
    # Its default value is 10(x). You can also change it to 5(x).
    max-bytes-for-level-multiplier : 10
    
    # slotmigrate is mainly used to migrate slots, usually we will set it to no.
    # When you migrate slots, you need to set it to yes, and reload slotskeys before.
    # slotmigrate  [yes | no]
    slotmigrate : no
    
    # BlockBasedTable block_size, default 4k
    # block-size: 4096
    
    # block LRU cache, default 8M, 0 to disable
    # Supported Units [K|M|G], default unit [bytes]
    # block-cache: 8388608
    
    # num-shard-bits default -1, the number of bits from cache keys to be use as shard id.
    # The cache will be sharded into 2^num_shard_bits shards.
    # https://github.com/EighteenZi/rocksdb_wiki/blob/master/Block-Cache.md#lru-cache
    # num-shard-bits: -1
    
    # whether the block cache is shared among the RocksDB instances, default is per CF
    # share-block-cache: no
    
    # The slot number of pika when used with codis.
    default-slot-num : 1024
    
    # whether or not index and filter blocks is stored in block cache
    # cache-index-and-filter-blocks: no
    
    # pin_l0_filter_and_index_blocks_in_cache [yes | no]
    # When `cache-index-and-filter-blocks` is enabled, `pin_l0_filter_and_index_blocks_in_cache` is suggested to be enabled
    # pin_l0_filter_and_index_blocks_in_cache : no
    
    # when set to yes, bloomfilter of the last level will not be built
    # optimize-filters-for-hits: no
    # https://github.com/facebook/rocksdb/wiki/Leveled-Compaction#levels-target-size
    # level-compaction-dynamic-level-bytes: no
    
    ################################## RocksDB Rate Limiter #######################
    # rocksdb rate limiter
    # https://rocksdb.org/blog/2017/12/18/17-auto-tuned-rate-limiter.html
    # https://github.com/EighteenZi/rocksdb_wiki/blob/master/Rate-Limiter.md
    #######################################################################E#######
    
    # rate limiter bandwidth, default 200MB
    #rate-limiter-bandwidth : 209715200
    
    #rate-limiter-refill-period-us : 100000
    #
    #rate-limiter-fairness: 10
    
    # rate limiter auto tune https://rocksdb.org/blog/2017/12/18/17-auto-tuned-rate-limiter.html. the default value is false.
    #rate-limiter-auto-tuned : true
    
    ################################## RocksDB Blob Configure #####################
    # rocksdb blob configure
    # https://rocksdb.org/blog/2021/05/26/integrated-blob-db.html
    # wiki https://github.com/facebook/rocksdb/wiki/BlobDB
    #######################################################################E#######
    
    # enable rocksdb blob, default no
    # enable-blob-files : yes
    
    # values at or above this threshold will be written to blob files during flush or compaction.
    # Supported Units [K|M|G], default unit is in [bytes].
    # min-blob-size : 4K
    
    # the size limit for blob files
    # Supported Units [K|M|G], default unit is in [bytes].
    # blob-file-size : 256M
    
    # the compression type to use for blob files. All blobs in the same file are compressed using the same algorithm.
    # Supported types: [snappy, zlib, lz4, zstd]. If you do not wanna compress the SST file, please set its value as none.
    # [NOTICE] The Pika official binary release just link the snappy library statically, which means that
    # you should compile the Pika from the source code and then link it with other compression algorithm library statically by yourself.
    # blob-compression-type : lz4
    
    # set this to open to make BlobDB actively relocate valid blobs from the oldest blob files as they are encountered during compaction.
    # The value option is [yes | no]
    # enable-blob-garbage-collection : no
    
    # the cutoff that the GC logic uses to determine which blob files should be considered “old“.
    # This parameter can be tuned to adjust the trade-off between write amplification and space amplification.
    # blob-garbage-collection-age-cutoff : 0.25
    
    # if the ratio of garbage in the oldest blob files exceeds this threshold,
    # targeted compactions are scheduled in order to force garbage collecting the blob files in question
    # blob_garbage_collection_force_threshold : 1.0
    
    # the Cache object to use for blobs, default not open
    # blob-cache : 0
    
    # blob-num-shard-bits default -1, the number of bits from cache keys to be use as shard id.
    # The cache will be sharded into 2^blob-num-shard-bits shards.
    # blob-num-shard-bits : -1
    
  dashboard.toml: |-
    
    ##################################################
    #                                                #
    #                  Codis-Dashboard               #
    #                                                #
    ##################################################
    
    # Set Coordinator, only accept "zookeeper" & "etcd" & "filesystem".
    # for zookeeper/etcd, coorinator_auth accept "user:password" 
    # Quick Start
    #coordinator_name = "filesystem"
    #coordinator_addr = "/tmp/codis"
    coordinator_name = "etcd"
    coordinator_addr = "pika-cluster-etcd-0.pika-cluster-etcd-headless:2379,pika-cluster-etcd-1.pika-cluster-etcd-headless:2379,pika-cluster-etcd-1.pika-cluster-etcd-headless:2379"
    #coordinator_auth = ""
    
    # Set Codis Product Name/Auth.
    product_name = "codis-demo"
    product_auth = ""
    
    # Set bind address for admin(rpc), tcp only.
    admin_addr = "0.0.0.0:18080"
    
    # Set slot num
    max_slot_num = 1024
    
    # Set arguments for data migration (only accept 'sync' & 'semi-async').
    migration_method = "semi-async"
    migration_parallel_slots = 100
    migration_async_maxbulks = 200
    migration_async_maxbytes = "32mb"
    migration_async_numkeys = 500
    migration_timeout = "30s"
    
    # Set configs for redis sentinel.
    sentinel_check_server_state_interval = "5s"
    sentinel_check_master_failover_interval = "1s"
    sentinel_master_dead_check_times = 5
    sentinel_client_timeout = "10s"
    sentinel_quorum = 2
    sentinel_parallel_syncs = 1
    sentinel_down_after = "30s"
    sentinel_failover_timeout = "5m"
    sentinel_notification_script = ""
    sentinel_client_reconfig_script = ""
    
  proxy.toml: |-
    
    ##################################################
    #                                                #
    #                  Codis-Proxy                   #
    #                                                #
    ##################################################
    
    # Set Codis Product Name/Auth.
    product_name = "codis-demo"
    product_auth = ""
    
    # Set auth for client session
    #   1. product_auth is used for auth validation among codis-dashboard,
    #      codis-proxy and codis-server.
    #   2. session_auth is different from product_auth, it requires clients
    #      to issue AUTH <PASSWORD> before processing any other commands.
    session_auth = ""
    
    # Set bind address for admin(rpc), tcp only.
    admin_addr = "0.0.0.0:11080"
    
    # Set bind address for proxy, proto_type can be "tcp", "tcp4", "tcp6", "unix" or "unixpacket".
    proto_type = "tcp4"
    proxy_addr = "0.0.0.0:19000"
    
    # Set jodis address & session timeout
    #   1. jodis_name is short for jodis_coordinator_name, only accept "zookeeper" & "etcd".
    #   2. jodis_addr is short for jodis_coordinator_addr
    #   3. jodis_auth is short for jodis_coordinator_auth, for zookeeper/etcd, "user:password" is accepted.
    #   4. proxy will be registered as node:
    #        if jodis_compatible = true (not suggested):
    #          /zk/codis/db_{PRODUCT_NAME}/proxy-{HASHID} (compatible with Codis2.0)
    #        or else
    #          /jodis/{PRODUCT_NAME}/proxy-{HASHID}
    jodis_name = ""
    jodis_addr = ""
    jodis_auth = ""
    jodis_timeout = "20s"
    jodis_compatible = false
    
    # Set datacenter of proxy.
    proxy_datacenter = ""
    
    # Set max number of alive sessions.
    proxy_max_clients = 1000
    
    # Set max offheap memory size. (0 to disable)
    proxy_max_offheap_size = "1024mb"
    
    # Set heap placeholder to reduce GC frequency.
    proxy_heap_placeholder = "256mb"
    
    # Proxy will ping backend redis (and clear 'MASTERDOWN' state) in a predefined interval. (0 to disable)
    backend_ping_period = "5s"
    
    # Set backend recv buffer size & timeout.
    backend_recv_bufsize = "128kb"
    backend_recv_timeout = "30s"
    
    # Set backend send buffer & timeout.
    backend_send_bufsize = "128kb"
    backend_send_timeout = "30s"
    
    # Set backend pipeline buffer size.
    backend_max_pipeline = 20480
    
    # Set backend never read replica groups, default is false
    backend_primary_only = false
    
    # Set backend parallel connections per server
    backend_primary_parallel = 1
    backend_replica_parallel = 1
    
    # Set slot num
    max_slot_num = 1024
    
    # Set backend tcp keepalive period. (0 to disable)
    backend_keepalive_period = "75s"
    
    # Set number of databases of backend.
    backend_number_databases = 1
    
    # If there is no request from client for a long time, the connection will be closed. (0 to disable)
    # Set session recv buffer size & timeout.
    session_recv_bufsize = "128kb"
    session_recv_timeout = "30m"
    
    # Set session send buffer size & timeout.
    session_send_bufsize = "64kb"
    session_send_timeout = "30s"
    
    # Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked.
    # Set session pipeline buffer size.
    session_max_pipeline = 10000
    
    # Set session tcp keepalive period. (0 to disable)
    session_keepalive_period = "75s"
    
    # Set session to be sensitive to failures. Default is false, instead of closing socket, proxy will send an error response to client.
    session_break_on_failure = false
    
    # Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period.
    metrics_report_server = ""
    metrics_report_period = "1s"
    
    # Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb.
    metrics_report_influxdb_server = ""
    metrics_report_influxdb_period = "1s"
    metrics_report_influxdb_username = ""
    metrics_report_influxdb_password = ""
    metrics_report_influxdb_database = ""
    
    # Set statsd server (such as localhost:8125), proxy will report metrics to statsd.
    metrics_report_statsd_server = ""
    metrics_report_statsd_period = "1s"
    metrics_report_statsd_prefix = ""
---
# Source: pika/templates/script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pika-script-template
  labels:
    helm.sh/chart: pika-3.5.1
    app.kubernetes.io/name: pika
    app.kubernetes.io/instance: my-pika
    app.kubernetes.io/version: "3.5.1"
    app.kubernetes.io/managed-by: Helm
data:
  admin.sh: |-
    #! /bin/bash
    set -x
    
    # set instance role
    set_instance_role() {
      POD_ID=${HOSTNAME##*-}
      echo "POD_ID: "${POD_ID}
    }
    
    # set group id
    set_group_id() {
      GROUP_ID=${KB_CLUSTER_COMP_NAME##*-}
      echo "GROUP_ID: "${GROUP_ID}
    }
    
    # set codis dashboard
    set_codis_dashboard() {
      CODIS_DASHBOARD="${KB_CLUSTER_NAME}-codis-dashboard"
      echo "CODIS_DASHBOARD: "${CODIS_DASHBOARD}
      CODIS_ADMIN="/codis/bin/codis-admin --dashboard=${CODIS_DASHBOARD}:18080"
      echo "CODIS_ADMIN: "${CODIS_ADMIN}
    }
    
    wait_server_running() {
      until nc -z 127.0.0.1 9221; do
        echo waiting for pika
        sleep 2
      done
    }
    
    wait_dashboard_running() {
      until nc -z ${CODIS_DASHBOARD} 18080; do
        echo waiting for codis dashboard
        sleep 2
      done
    }
    
    wait_master_registered() {
      until $CODIS_ADMIN --list-group | jq -r '.[] | select(.id == '${GROUP_ID}') | .servers[] | select(.role == "master") | .server'; do
        echo waiting for master registered
        sleep 2
      done
    }
    
    reload_until_success() {
      until $CODIS_ADMIN --reload 1>/dev/null 2>&1; do
        echo waiting for reload success
        sleep 2
      done
    }
    
    register_server() {
      reload_until_success
      if [ ${POD_ID} -gt 0 ]; then wait_master_registered; fi
      $CODIS_ADMIN --create-group --gid=${GROUP_ID} 1>/dev/null 2>&1
      $CODIS_ADMIN --group-add --gid=${GROUP_ID} --addr=${KB_POD_FQDN}:9221
      $CODIS_ADMIN --sync-action --create --addr=${KB_POD_FQDN}:9221 1>/dev/null 2>&1
    }
    
    remove_server() {
      $CODIS_ADMIN --reload
      if [ $? != 0 ]; then exit 1; fi
      gid=${GROUP_ID}
      sleep 5
      $CODIS_ADMIN --group-del --gid=${GROUP_ID} --addr=${KB_POD_FQDN}:9221
    }
    
    reblance() {
      $CODIS_ADMIN --rebalance --confirm
    }
    
    set_group_id
    set_instance_role
    set_codis_dashboard
    
    if [ $# -eq 1 ]; then
      case $1 in
      --help)
        echo "Usage: $0 [options]"
        echo "Options:"
        echo "  --help                show help information"
        echo "  --register-server     register server to dashboard"
        echo "  --remove-server       remove server from dashboard"
        exit 0
        ;;
      --register-server)
        wait_dashboard_running
        wait_server_running
        register_server
        exit 0
        ;;
      --remove-server)
        remove_server
        exit 0
        ;;
      *)
        echo "Error: invalid option '$1'"
        exit 1
        ;;
      esac
    fi
    
  pika-group-post-start.sh: |-
    #! /bin/bash
    set -x

    echo "add your post start script logic here"
    envParams=$(env)
    echo "add envs can be used in script:"
    echo $envParams

    sleep 60
---
# Source: pika/templates/clusterdefinition.yaml
apiVersion: apps.kubeblocks.io/v1alpha1
kind: ClusterDefinition
metadata:
  name: pika
  labels:
    helm.sh/chart: pika-3.5.1
    app.kubernetes.io/name: pika
    app.kubernetes.io/instance: my-pika
    app.kubernetes.io/version: "3.5.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: pika
  connectionCredential:
    username: default
    password: "$(RANDOM_PASSWD)"
    endpoint: "$(SVC_FQDN):$(SVC_PORT_pika)"
    host: "$(SVC_FQDN)"
    port: "$(SVC_PORT_pika)"
  componentDefs:
    - name: pika-group
      workloadType: Stateful
      characterType: pika
      service:
        ports:
          - name: pika
            port: 9221
            targetPort: pika
      configSpecs:
        - name: pika-config
          templateRef: pika-conf-template
          namespace: default
          volumeName: config
      scriptSpecs:
        - name: pika-script
          templateRef: pika-script-template
          namespace: default
          volumeName: script
          defaultMode: 0555
      postStartSpec:
        cmdExecutorConfig:
          image: docker.io/pikadb/pika:v3.5.1
          command:
            - /script/pika-group-post-start.sh
        scriptSpecSelectors:
          - name: pika-script
      podSpec:
        containers:
          - name: pika
            ports:
              - name: pika
                containerPort: 9221
            volumeMounts:
              - name: config
                mountPath: /etc/pika
            command:
              - "/pika/bin/pika"
            args:
              - "-c"
              - "/etc/pika/pika.conf"
          - name: codis-admin
            volumeMounts:
              - name: script
                mountPath: /script
            command:
              - "/bin/bash"
            args:
              - "-c"
              - "/script/admin.sh --register-server;tail -f /dev/null"
    - name: etcd
      workloadType: Stateful
      characterType: etcd
      service:
        ports:
          - name: client
            port: 2379
            targetPort: client
          - name: peer
            port: 2380
            targetPort: peer
      volumeTypes:
        - name: data
          type: data
      configSpecs:
      podSpec:
        initContainers:
          - name: volume-permissions
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
              - /bin/sh
              - -ec
              - |
                chown -R 1001:1001 /bitnami/etcd
            securityContext:
              runAsUser: 0
            volumeMounts:
              - name: data
                mountPath: /bitnami/etcd
        containers:
          - name: etcd
            imagePullPolicy: "IfNotPresent"
            securityContext:
              runAsNonRoot: false
              runAsUser: 1001
              allowPrivilegeEscalation: false
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /bitnami/etcd
                name: data
            ports:
              - name: client
                containerPort: 2379
              - name: peer
                containerPort: 2380
            env:
              - name: BITNAMI_DEBUG
                value: "true"
              - name: MY_POD_IP
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: status.podIP
              - name: MY_POD_NAME
                valueFrom:
                  fieldRef:
                    apiVersion: v1
                    fieldPath: metadata.name
              - name: MY_STS_NAME
                value: $(KB_CLUSTER_COMP_NAME)
              - name: ETCDCTL_API
                value: "3"
              - name: ETCD_ON_K8S
                value: "yes"
              - name: ETCD_START_FROM_SNAPSHOT
                value: "no"
              - name: ETCD_DISASTER_RECOVERY
                value: "no"
              - name: ETCD_NAME
                value: $(MY_POD_NAME)
              - name: ETCD_DATA_DIR
                value: /bitnami/etcd/data
              - name: ETCD_LOG_LEVEL
                value: info
              - name: ALLOW_NONE_AUTHENTICATION
                value: "yes"
              - name: ETCD_INITIAL_CLUSTER_TOKEN
                value: "$(KB_CLUSTER_NAME)"
              - name: ETCD_INITIAL_CLUSTER_STATE
                value: "new"
              - name: ETCD_INITIAL_CLUSTER
                value: "$(KB_CLUSTER_NAME)-etcd-0=http://$(KB_CLUSTER_NAME)-etcd-0.$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2380,$(KB_CLUSTER_NAME)-etcd-1=http://$(KB_CLUSTER_NAME)-etcd-1.$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2380,$(KB_CLUSTER_NAME)-etcd-2=http://$(KB_CLUSTER_NAME)-etcd-2.$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2380"
              - name: ETCD_CLUSTER_DOMAIN
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local"
              - name: ETCD_AUTO_COMPACTION_MODE
                value: "periodic"
              - name: ETCD_AUTO_COMPACTION_RETENTION
                value: "1h"
              - name: ETCD_ADVERTISE_CLIENT_URLS
                value: "http://$(KB_POD_FQDN):2379,http://$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2379,http://$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2379"
              - name: ETCD_LISTEN_CLIENT_URLS
                value: http://0.0.0.0:2379
              - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
                value: http://$(KB_POD_FQDN).cluster.local:2380
              - name: ETCD_LISTEN_PEER_URLS
                value: http://0.0.0.0:2380
              - name: ETCD_QUOTA_BACKEND_BYTES
                value: "4294967296"
              - name: ETCD_HEARTBEAT_INTERVAL
                value: "500"
              - name: ETCD_ELECTION_TIMEOUT
                value: "2500"
              - name: ETCD_ENABLE_V2
                value: "true"
    - name: codis-proxy
      workloadType: Stateful
      characterType: pika
      service:
        ports:
          - name: proxy
            targetPort: proxy
            port: 11080
          - name: admin
            targetPort: admin
            port: 19000
      configSpecs:
        - name: codis-proxy-config
          templateRef: pika-conf-template
          namespace: default
          volumeName: config
      podSpec:
        initContainers:
          - name: wait-etcd
            env:
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local"
              - name: DASHBOARD_ADDR
                value: "$(KB_CLUSTER_NAME)-codis-dashboard"
            image: busybox:1.28
            command:
              - 'sh'
              - '-c'
              - "until nc -z ${ETCD_ADDR} 2379; do echo waiting for etcd; sleep 2; done;"
              - "until nc -z ${DASHBOARD_ADDR} 18080; do echo waiting for etcd; sleep 2; done;"
        containers:
          - name: codis-proxy
            imagePullPolicy: IfNotPresent
            ports:
              - containerPort: 11080
                name: proxy
              - containerPort: 19000
                name: admin
            volumeMounts:
              - name: config
                mountPath: /etc/codis
            env:
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2379"
              - name: DASHBOARD_ADDR
                value: "$(KB_CLUSTER_NAME)-codis-dashboard:18080"
              - name: PRODUCT_NAME
                value: "$(KB_CLUSTER_NAME)"
            command:
              - "/codis/bin/codis-proxy"
            args:
              - "-c"
              - "/etc/codis/proxy.toml"
              - "--host-admin"
              - "$(POD_IP):11080"
              - "--host-proxy"
              - "$(POD_IP):19000"
              - "--etcd"
              - "$(ETCD_ADDR)"
              - "--product_name"
              - "$(PRODUCT_NAME)"
              - "--pidfile"
              - "log/proxy.pid"
              - "--log-level=DEBUG"
            lifecycle:
              preStop:
                exec:
                  command:
                    - "/bin/sh"
                    - "-c"
                    - "/codis/bin/codis-admin --dashboard=${DASHBOARD_ADDR} --remove-proxy --addr=${POD_IP}:11080 1>/dev/null 2>&1"
    - name: codis-fe
      workloadType: Stateless
      characterType: pika
      service:
        ports:
          - name: fe
            targetPort: fe
            port: 8080
      podSpec:
        initContainers:
          - name: wait-etcd
            env:
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local"
              - name: DASHBOARD_ADDR
                value: "$(KB_CLUSTER_NAME)-codis-dashboard"
            image: busybox:1.28
            command:
              - 'sh'
              - '-c'
              - "until nc -z ${ETCD_ADDR} 2379; do echo waiting for etcd; sleep 2; done;"
              - "until nc -z ${DASHBOARD_ADDR} 18080; do echo waiting for etcd; sleep 2; done;"
        containers:
          - name: codis-fe
            imagePullPolicy: IfNotPresent
            ports:
              - containerPort: 8080
                name: fe
            env:
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2379"
            command:
              - "/codis/bin/codis-fe"
            args:
              - "--etcd"
              - "$(ETCD_ADDR)"
              - "--listen=0.0.0.0:8080"
              - "--assets=/codis/bin/assets"
              - "--log-level=DEBUG"
    - name: codis-dashboard
      workloadType: Stateful
      characterType: pika
      service:
        ports:
          - name: dashboard
            targetPort: dashboard
            port: 18080
      configSpecs:
        - name: codis-dashboard-config
          templateRef: pika-conf-template
          namespace: default
          volumeName: config
      podSpec:
        initContainers:
          - name: wait-etcd
            env:
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local"
            image: busybox:1.28
            command:
              - 'sh'
              - '-c'
              - "until nc -z ${ETCD_ADDR} 2379; do echo waiting for etcd; sleep 2; done;"
        containers:
          - name: codis-dashboard
            imagePullPolicy: IfNotPresent
            ports:
              - containerPort: 18080
                name: dashboard
            volumeMounts:
              - name: config
                mountPath: /etc/codis
            env:
              - name: POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: ETCD_ADDR
                value: "$(KB_CLUSTER_NAME)-etcd-headless.$(KB_NAMESPACE).svc.cluster.local:2379"
              - name: PRODUCT_NAME
                value: "$(KB_CLUSTER_NAME)"
            command:
              - "/codis/bin/codis-dashboard"
            args:
              - "-c"
              - "/etc/codis/dashboard.toml"
              - "--host-admin"
              - "$(POD_IP):18080"
              - "--etcd"
              - "$(ETCD_ADDR)"
              - "--product_name"
              - "$(PRODUCT_NAME)"
              - "--pidfile"
              - "log/dashboard.pid"
              - "--remove-lock"
              - "--log-level=DEBUG"
            lifecycle:
              postStart:
                exec:
                  command: [ "/bin/bash", "-c", "/codis/bin/codis-admin --dashboard-list  --etcd=${ETCD_ADDR}" ]
              preStop:
                exec:
                  command: [ "/bin/sh", "-c", "PID=$(cat log/dashboard.pid) && kill $PID && while ps -p 1 > /dev/null; do sleep 1; done" ]
---
# Source: pika/templates/clusterversion.yaml
apiVersion: apps.kubeblocks.io/v1alpha1
kind: ClusterVersion
metadata:
  name: pika-3.5.1
  labels:
    helm.sh/chart: pika-3.5.1
    app.kubernetes.io/name: pika
    app.kubernetes.io/instance: my-pika
    app.kubernetes.io/version: "3.5.1"
    app.kubernetes.io/managed-by: Helm
spec:
  clusterDefinitionRef: pika
  componentVersions:
    - componentDefRef: pika-group
      versionsContext:
        containers:
          - name: pika
            image: docker.io/pikadb/pika:v3.5.1
            imagePullPolicy: IfNotPresent
          - name: codis-admin
            image: docker.io/pikadb/codis:v3.5.1
            imagePullPolicy: IfNotPresent
    - componentDefRef: etcd
      versionsContext:
        containers:
          - name: etcd
            image: docker.io/bitnami/etcd:3.5.9
            imagePullPolicy: IfNotPresent
    - componentDefRef: codis-proxy
      versionsContext:
        containers:
          - name: codis-proxy
            image: docker.io/pikadb/codis:v3.5.1
            imagePullPolicy: IfNotPresent
    - componentDefRef: codis-fe
      versionsContext:
        containers:
          - name: codis-fe
            image: docker.io/pikadb/codis:v3.5.1
            imagePullPolicy: IfNotPresent
    - componentDefRef: codis-dashboard
      versionsContext:
        containers:
          - name: codis-dashboard
            image: docker.io/pikadb/codis:v3.5.1
            imagePullPolicy: IfNotPresent
