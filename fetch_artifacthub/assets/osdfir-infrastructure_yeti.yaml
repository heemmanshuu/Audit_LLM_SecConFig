---
# Source: yeti/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-yeti-redis
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-yeti
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: yeti/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-yeti-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: yeti/charts/redis/templates/replicas/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: my-yeti-redis-replica
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
---
# Source: yeti/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: yeti
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
---
# Source: yeti/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-yeti-secret
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  yeti-user: "MjJHejlNbVpxQzl2YktNNVVONWR4SENoUGhvTTA0Zmk="
  yeti-arangodb: "VlBMT2hNdzhpTmlpRElPQw=="
  yeti-api: "OTU4MDI4ODI0NzgzNTgyNzg3NzYwNjIzNzE3NDM0MTI1MDE2NzU3OTc3MTU1MzcyMzE3MDg4MzA1MDMwNjI2OA=="
---
# Source: yeti/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-yeti-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: yeti/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-yeti-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: yeti/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-yeti-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")
    HEADLESS_SERVICE="my-yeti-redis-headless.default.svc.cluster.local"

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: yeti/templates/nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-yeti-nginx-configmap
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
data:
  default.conf: |
    server {

        root /www;

        location /api/v2 {
            proxy_pass http://my-yeti-api:8000;
        }
        
        location ~(^/docs|^/openapi.json) {
            proxy_pass http://my-yeti-api:8000;
        }

        location / {
            try_files $uri $uri/ /index.html;
        }
    }
---
# Source: yeti/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations: 
    helm.sh/resource-policy: keep
  name: yetivolume-claim
  namespace: "default"
spec:
  
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "2Gi"
---
# Source: yeti/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/name: redis
---
# Source: yeti/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: yeti/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: replica
---
# Source: yeti/templates/api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti-api
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 8000
      protocol: TCP
      targetPort: 8000
  selector:
    app.kubernetes.io/component: api
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
---
# Source: yeti/templates/arangodb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti-arangodb
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 8529
      protocol: TCP
  selector:
    app.kubernetes.io/component: arangodb
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
---
# Source: yeti/templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-yeti
  namespace: "default"
  labels:
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  type: ClusterIP
  ports:
    - port: 9000
      protocol: TCP
      targetPort: 80
  selector:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
---
# Source: yeti/templates/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-yeti-api
  namespace: "default"
  labels:
    app.kubernetes.io/component: api
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: api
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-yeti
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "puP2v" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: api
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-yeti
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: api
          securityContext:
            {}
          image: "yetiplatform/yeti:latest"
          imagePullPolicy: Always
          command: ["sh", "-c", "/docker-entrypoint.sh webserver"]
          lifecycle:
            postStart:
              exec:
                command: ["sh", "-c", "poetry run python yetictl/cli.py create-user yeti $YETI_USER_PASSWORD --api_key $YETI_API_KEY --admin"]
          env:
            - name: YETI_REDIS_HOST
              value: my-yeti-redis-master
            - name: YETI_REDIS_PORT
              value: "6379"
            - name: YETI_REDIS_DATABASE
              value: "0"
            - name: YETI_ARANGODB_HOST
              value: my-yeti-arangodb
            - name: YETI_ARANGODB_PORT
              value: "8529"
            - name: YETI_ARANGODB_DATABASE
              value: yeti
            - name: YETI_ARANGODB_USERNAME
              value: root
            - name: YETI_AUTH_SECRET_KEY
              value: "bpdJ72gJLxN201V1fbURRw6MibVXYf0o"
            - name: YETI_AUTH_ALGORITHM
              value: HS256
            - name: YETI_AUTH_ACCESS_TOKEN_EXPIRE_MINUTES
              value: "30"
            - name: YETI_AUTH_ENABLED
              value: "True"
            - name: YETI_SYSTEM_PLUGINS_PATH
              value: "./plugins"
            - name: YETI_USER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-user
            - name: YETI_ARANGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-arangodb
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-api
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
            - containerPort: 8000
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: yetivolume-claim
            readOnly: false
---
# Source: yeti/templates/arangodb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-yeti-arangodb
  namespace: "default"
  labels:
    app.kubernetes.io/component: arangodb
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: arangodb
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-yeti
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "xA1Qe" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: arangodb
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-yeti
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: frontend
          securityContext:
            {}
          image: "arangodb:3.11.8"
          imagePullPolicy: Always
          env:
            - name: ARANGO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-arangodb
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
            - containerPort: 8529
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: yetivolume-claim
            readOnly: false
---
# Source: yeti/templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-yeti
  namespace: "default"
  labels:
    app.kubernetes.io/component: frontend
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-yeti
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "EvpP4" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-yeti
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: frontend
          securityContext:
            {}
          image: "yetiplatform/yeti-frontend:latest"
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
            - mountPath: /etc/nginx/conf.d/default.conf
              subPath: default.conf
              name: nginx-config
              readOnly: true
          ports:
            - containerPort: 9200
            - containerPort: 80
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: yetivolume-claim
            readOnly: false
        - name: nginx-config
          configMap:
            name: my-yeti-nginx-configmap
---
# Source: yeti/templates/tasks-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  my-yeti-tasks
  namespace: "default"
  labels:
    app.kubernetes.io/component: tasks
    helm.sh/chart: yeti-1.0.4
    app.kubernetes.io/name: yeti
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
    date: "2024-09-16"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: tasks
      app.kubernetes.io/name: yeti
      app.kubernetes.io/instance: my-yeti
  template:
    metadata:
      annotations:
        # Have Deployment restart after each upgrade
        roll: "gQGR9" 
        prometheus.io/port: "9200"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: tasks
        app.kubernetes.io/name: yeti
        app.kubernetes.io/instance: my-yeti
    spec:
      serviceAccountName: yeti
      securityContext:
        {}
      containers:
        - name: tasks
          securityContext:
            {}
          image: "yetiplatform/yeti:latest"
          imagePullPolicy: Always
          command: ["sh", "-c", "/docker-entrypoint.sh tasks"]
          env:
            - name: YETI_REDIS_HOST
              value: my-yeti-redis-master
            - name: YETI_REDIS_PORT
              value: "6379"
            - name: YETI_REDIS_DATABASE
              value: "0"
            - name: YETI_ARANGODB_HOST
              value: my-yeti-arangodb
            - name: YETI_ARANGODB_PORT
              value: "8529"
            - name: YETI_ARANGODB_DATABASE
              value: yeti
            - name: YETI_ARANGODB_USERNAME
              value: root
            - name: YETI_AUTH_SECRET_KEY
              value: "gMHMG3IrX5y6a96JORJp6l6zblHEXFWC"
            - name: YETI_AUTH_ALGORITHM
              value: HS256
            - name: YETI_AUTH_ACCESS_TOKEN_EXPIRE_MINUTES
              value: "30"
            - name: YETI_AUTH_ENABLED
              value: "True"
            - name: YETI_SYSTEM_PLUGINS_PATH
              value: "./plugins"
            - name: YETI_USER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-user
            - name: YETI_ARANGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-arangodb
            - name: YETI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: my-yeti-secret 
                  key: yeti-api
          volumeMounts:
            - mountPath: /mnt/yeti
              name: yetivolume
          ports:
            - containerPort: 9200
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: yetivolume
          persistentVolumeClaim:
            claimName: yetivolume-claim
            readOnly: false
---
# Source: yeti/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-yeti-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-yeti
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: my-yeti-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-yeti
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 831e6af00ef2ca570c8a855968a1fdb8643d8efbe9063d830d877ff94c776dc0
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-yeti-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-yeti
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-yeti-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-yeti-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-yeti-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-yeti
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: yeti/charts/redis/templates/replicas/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-yeti-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-yeti
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-19.3.2
    app.kubernetes.io/component: replica
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-yeti
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: replica
  serviceName: my-yeti-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-yeti
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-19.3.2
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 86bcc953bb473748a3d3dc60b7c11f34e60c93519234d4c37f42e22ada559d47
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 831e6af00ef2ca570c8a855968a1fdb8643d8efbe9063d830d877ff94c776dc0
        checksum/secret: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-yeti-redis-replica
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-yeti
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.2.4-debian-12-r16
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: my-yeti-redis-master-0.my-yeti-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: start-scripts
          configMap:
            name: my-yeti-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-yeti-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-yeti-redis-configuration
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: my-yeti
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
