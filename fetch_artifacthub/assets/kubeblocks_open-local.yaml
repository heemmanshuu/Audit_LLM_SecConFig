---
# Source: open-local/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: open-local
  namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-lvm
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com
parameters:
  volumeType: "LVM"
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-lvm-xfs
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com
parameters:
  volumeType: "LVM"
  csi.storage.k8s.io/fstype: xfs
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-device-hdd
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com
parameters:
  csi.storage.k8s.io/fstype: ext4
  volumeType: Device
  mediaType: hdd
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-device-ssd
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com
parameters:
  csi.storage.k8s.io/fstype: ext4
  volumeType: Device
  mediaType: ssd
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-lvm-io-throttling
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com
parameters:
  volumeType: "LVM"
  bps: "1048576"
  iops: "1024"
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-hostpath
  annotations:
    local.csi.aliyun.com/config: |
      - name: BasePath
        value: /var/open-local/local
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com/hostpath
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
# TODO(x.zhou): allow volume expansion
allowVolumeExpansion: false
---
# Source: open-local/templates/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open-local-hostpath-quota
  annotations:
    local.csi.aliyun.com/config: |
      - name: BasePath
        value: /var/open-local/local
      - name: EXT4Quota
        enabled: true
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
provisioner: local.csi.aliyun.com/hostpath
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
# TODO(x.zhou): allow volume expansion
allowVolumeExpansion: false
---
# Source: open-local/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: open-local
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
rules:
  - apiGroups:
      - csi.aliyun.com
    resources:
      - nodelocalstorages
      - nodelocalstorages/status
      - nodelocalstorageinitconfigs
    verbs:
      - create
      - get
      - list
      - watch
      - update
      - delete
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
      - pods
      - pods/binding
      - pods/status
      - bindings
      - persistentvolumeclaims
      - persistentvolumeclaims/status
      - persistentvolumes
      - persistentvolumes/status
      - namespaces
      - secrets
      - endpoints
    verbs:
      - create
      - get
      - list
      - watch
      - update
      - delete
      - patch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
      - csinodes
      - volumeattachments
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - snapshot.storage.k8s.io
    resources:
      - volumesnapshotclasses
      - volumesnapshots
      - volumesnapshots/status
      - volumesnapshotcontents
      - volumesnapshotcontents/status
    verbs:
      - create
      - get
      - list
      - watch
      - update
      - delete
      - patch
  - apiGroups:
      - "coordination.k8s.io"
    resources:
      - leases
    verbs:
      - list
      - watch
      - create
      - update
      - patch
      - delete
      - get
---
# Source: open-local/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: open-local
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: open-local
subjects:
- kind: ServiceAccount
  name: open-local
  namespace: kube-system
---
# Source: open-local/templates/extender.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
  name: open-local-scheduler-extender
  namespace: kube-system
spec:
  selector:
    app.kubernetes.io/name: open-local
    app.kubernetes.io/component: open-local-scheduler-extender
  ports:
  - protocol: TCP
    port: 23000
    targetPort: 23000
    name: http-port
---
# Source: open-local/templates/agent.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: open-local-agent
  namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
    app.kubernetes.io/component: open-local-agent
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: open-local-agent
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "open-local"
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: "open-local-0.8.1"
        app.kubernetes.io/component: open-local-agent
    spec:
      tolerations:
      - operator: Exists
      serviceAccount: open-local
      priorityClassName: system-node-critical
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: agent
        args :
        - agent
        - "--nodename=$(KUBE_NODE_NAME)"
        - "--path.sysfs=/host_sys"
        - "--path.mount=/mnt/open-local/"
        - "--lvname=local"
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: HOST_SYS
          value: "/host_sys"
        - name: TZ
          value: Asia/Shanghai
        securityContext:
          privileged: true
          capabilities:
            add: ["SYS_ADMIN"]
          allowPrivilegeEscalation: true
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 50m
            memory: 64Mi
        volumeMounts:
        - mountPath: /dev
          mountPropagation: "HostToContainer"
          name: host-dev
        - name: sys
          readOnly: true
          mountPropagation: "HostToContainer"
          mountPath: "/host_sys"
        - mountPath: /mnt/open-local/
          name: localvolume
          mountPropagation: "Bidirectional"
      - name: driver-registrar
        image: ack-agility-registry.cn-shanghai.cr.aliyuncs.com/ecp_builder/csi-node-driver-registrar:v2.3.0
        imagePullPolicy: Always
        args:
        - "--v=5"
        - "--csi-address=/csi/csi.sock"
        - "--kubelet-registration-path=/var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock"
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: TZ
          value: Asia/Shanghai
        resources:
          limits:
            cpu: 200m
            memory: 100Mi
          requests:
            cpu: 20m
            memory: 50Mi
        volumeMounts:
          - name: plugin-dir
            mountPath: /csi
          - name: registration-dir
            mountPath: /registration
      - name: csi-plugin
        securityContext:
          privileged: true
          capabilities:
            add: ["SYS_ADMIN"]
          allowPrivilegeEscalation: true
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        args:
        - csi
        - "--endpoint=$(CSI_ENDPOINT)"
        - "--nodeID=$(KUBE_NODE_NAME)"
        - "--driver=local.csi.aliyun.com"
        - "--driver-mode=node"
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CSI_ENDPOINT
          value: unix://var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock
        - name: TZ
          value: Asia/Shanghai
        - name: ISSUE_ORPHANED_POD
          value: "true"
        - name: ISSUE_BLOCK_REFERENCE
          value: "true"
        - name: ISSUE_MESSAGE_FILE
          value: "true"
        resources:
          limits:
            cpu: 2
            memory: 4Gi
          requests:
            cpu: 50m
            memory: 128Mi
        volumeMounts:
        - name: pods-mount-dir
          mountPath: /var/lib/kubelet
          mountPropagation: "Bidirectional"
        - mountPath: /dev
          mountPropagation: "HostToContainer"
          name: host-dev
        - mountPath: /mnt/open-local/
          mountPropagation: "Bidirectional"
          name: localvolume
        - mountPath: /var/log
          name: host-log
        - mountPath: /host_sys
          mountPropagation: Bidirectional
          name: sys
      volumes:
      - name: host-dev
        hostPath:
          path: /dev
      - name: sys
        hostPath:
          path: "/sys"
      - name: localvolume
        hostPath:
          path: /mnt/open-local/
          type: DirectoryOrCreate
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins/local.csi.aliyun.com
          type: DirectoryOrCreate
      - name: registration-dir
        hostPath:
          path: /var/lib/kubelet/plugins_registry
          type: DirectoryOrCreate
      - name: pods-mount-dir
        hostPath:
          path: /var/lib/kubelet
          type: Directory
      - name: host-log
        hostPath:
          path: /var/log
          type: DirectoryOrCreate
  updateStrategy:
    type: RollingUpdate
---
# Source: open-local/templates/controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
    app.kubernetes.io/component: open-local-controller
  name: open-local-controller
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: open-local-controller
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "open-local"
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: "open-local-0.8.1"
        app.kubernetes.io/component: open-local-controller
    spec:
      containers:
      - name: csi-provisioner
        args:
        - --csi-address=$(ADDRESS)
        - --volume-name-prefix=local
        - --feature-gates=Topology=True
        - --strict-topology=True
        - --extra-create-metadata=true
        - --timeout=10m
        env:
        - name: ADDRESS
          value: /var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock
        - name: TZ
          value: Asia/Shanghai
        image: ack-agility-registry.cn-shanghai.cr.aliyuncs.com/ecp_builder/csi-provisioner:v2.2.2
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        volumeMounts:
        - mountPath: /var/lib/kubelet/plugins/local.csi.aliyun.com
          name: socket-dir
      - name: csi-resizer
        args:
        - --csi-address=$(ADDRESS)
        env:
        - name: ADDRESS
          value: /var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock
        - name: TZ
          value: Asia/Shanghai
        image: ack-agility-registry.cn-shanghai.cr.aliyuncs.com/ecp_builder/csi-resizer:v1.3.0
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        volumeMounts:
        - mountPath: /var/lib/kubelet/plugins/local.csi.aliyun.com
          name: socket-dir
      - name: csi-snapshotter
        args:
        - --csi-address=$(ADDRESS)
        - --snapshot-name-prefix=snap
        env:
        - name: ADDRESS
          value: /var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock
        - name: TZ
          value: Asia/Shanghai
        image: ack-agility-registry.cn-shanghai.cr.aliyuncs.com/ecp_builder/csi-snapshotter:v4.2.1
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        volumeMounts:
        - mountPath: /var/lib/kubelet/plugins/local.csi.aliyun.com
          name: socket-dir
      - name: csi-plugin
        args:
        - csi
        - --endpoint=$(CSI_ENDPOINT)
        - --nodeID=$(KUBE_NODE_NAME)
        - --driver=local.csi.aliyun.com
        - --driver-mode=controller
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CSI_ENDPOINT
          value: unix://var/lib/kubelet/plugins/local.csi.aliyun.com/csi.sock
        - name: TZ
          value: Asia/Shanghai
        - name: ISSUE_ORPHANED_POD
          value: "true"
        - name: ISSUE_BLOCK_REFERENCE
          value: "true"
        - name: ISSUE_MESSAGE_FILE
          value: "true"
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
        volumeMounts:
        - mountPath: /var/lib/kubelet/plugins/local.csi.aliyun.com
          name: socket-dir
      - name: controller
        args:
        - controller
        - --initconfig=open-local
        - --feature-gates=UpdateNLS=true
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 50m
            memory: 64Mi
        env:
        - name: TZ
          value: Asia/Shanghai
      - name: snapshot-controller
        image: ack-agility-registry.cn-shanghai.cr.aliyuncs.com/ecp_builder/snapshot-controller:v4.2.1
        env:
          - name: TZ
            value: Asia/Shanghai
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 50m
            memory: 128Mi
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      serviceAccount: open-local
      serviceAccountName: open-local
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
        operator: Exists
      volumes:
      - emptyDir: {}
        name: socket-dir
---
# Source: open-local/templates/extender.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-local-scheduler-extender
  namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
    app.kubernetes.io/component: open-local-scheduler-extender
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: open-local-scheduler-extender
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "open-local"
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: "open-local-0.8.1"
        app.kubernetes.io/component: open-local-scheduler-extender
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      priorityClassName: system-cluster-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - open-local-scheduler-extender
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - scheduler
        - --port=23000
        - --scheduler-strategy=spread
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        name: open-local-scheduler-extender
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 128Mi
        env:
        - name: TZ
          value: Asia/Shanghai
      serviceAccount: open-local
---
# Source: open-local/templates/provisioner-hostpath.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
    app.kubernetes.io/component: open-local-provisioner-hostpath
  name: open-local-provisioner-hostpath
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: open-local-provisioner-hostpath
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "open-local"
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: "open-local-0.8.1"
        app.kubernetes.io/component: open-local-provisioner-hostpath
    spec:
      serviceAccountName: open-local
      containers:
      - name: provisioner-hostpath
        args:
        - provisioner
        - --name=local.csi.aliyun.com/hostpath
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 50m
            memory: 64Mi
        env:
        - name: TZ
          value: Asia/Shanghai
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
---
# Source: open-local/templates/init-job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: open-local-init-job
  namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
    app.kubernetes.io/component: open-local-init-job
spec:
  completions: 3
  parallelism: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/name: "open-local"
        app.kubernetes.io/version: "0.8.1"
        helm.sh/chart: "open-local-0.8.1"
        app.kubernetes.io/component: open-local-init-job
    spec:
      hostNetwork: true
      tolerations:
      - effect: NoSchedule
        operator: Exists
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: In
                values:
                - ""
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - open-local-init-job
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: init
        image: docker.io/apecloud/open-local:v0.7.3
        imagePullPolicy: Always
        command:
        - sh
        - "-c"
        - |
 
            set -ex
            sleep 4

            echo "generating kubeScheduler-config.yml..."

            cat >/etc/kubernetes/kubeScheduler-config.yml  <<EOF
            apiVersion: kubescheduler.config.k8s.io/v1
            kind: KubeSchedulerConfiguration
            clientConnection:
              kubeconfig: /etc/kubernetes/scheduler.conf
            extenders:
              - urlPrefix: 'http://open-local-scheduler-extender.kube-system:23000/scheduler'
                filterVerb: predicates
                prioritizeVerb: priorities
                preemptVerb: ''
                bindVerb: ''
                weight: 10
                enableHTTPS: false
                nodeCacheCapable: true
                ignorable: true
            EOF

            echo "modifying kube-scheduler.yaml..."

            if ! grep "^\  dnsPolicy: ClusterFirstWithHostNet" /etc/kubernetes/manifests/kube-scheduler.yaml; then
                sed -i "/  hostNetwork: true/a \  dnsPolicy: ClusterFirstWithHostNet" /etc/kubernetes/manifests/kube-scheduler.yaml
            fi

            if ! grep "^\    - --config=*" /etc/kubernetes/manifests/kube-scheduler.yaml; then
                sed -i "/    - --kubeconfig=/a \    - --config=/etc/kubernetes/kubeScheduler-config.yml" /etc/kubernetes/manifests/kube-scheduler.yaml
            fi

            if ! grep "^\      name: scheduler-config" /etc/kubernetes/manifests/kube-scheduler.yaml; then
                sed -i "/    volumeMounts:/a \    - mountPath: /etc/kubernetes/kubeScheduler-config.yml\n      name: scheduler-config\n      readOnly: true" /etc/kubernetes/manifests/kube-scheduler.yaml
                sed -i "/  volumes:/a \  - hostPath:\n      path: /etc/kubernetes/kubeScheduler-config.yml\n      type: File\n    name: scheduler-config" /etc/kubernetes/manifests/kube-scheduler.yaml
            fi
        volumeMounts:
        - name: kube-dir
          mountPath: /etc/kubernetes/
      restartPolicy: OnFailure
      volumes:
        - name: kube-dir
          hostPath:
            path: /etc/kubernetes/
            type: DirectoryOrCreate
---
# Source: open-local/templates/agent.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: local.csi.aliyun.com
spec:
  attachRequired: false
  podInfoOnMount: true
  volumeLifecycleModes:
  - Persistent
  - Ephemeral
---
# Source: open-local/templates/nlsc.yaml
apiVersion: csi.aliyun.com/v1alpha1
kind: NodeLocalStorageInitConfig
metadata:
  name: open-local
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
spec:
# globalConfig is the default global node configuration
# when the agent creates the NodeLocalStorage resource, the value will be filled in spec of the NodeLocalStorage
  globalConfig:
    # listConfig is the white and black list of storage devices(vgs and mountPoints) and supports regular expressions
    listConfig:
      vgs:
        include:
        - open-local-pool-[0-9]+
        - yoda-pool[0-9]+
        - ackdistro-pool
    resourceToBeInited:
      vgs:
      - devices:
        - /dev/vdb
        name: open-local-pool-0
---
# Source: open-local/templates/storage-class.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: open-local-lvm
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/name: "open-local"
    app.kubernetes.io/version: "0.8.1"
    helm.sh/chart: "open-local-0.8.1"
driver: local.csi.aliyun.com
deletionPolicy: Delete
parameters:
  csi.aliyun.com/readonly: "true"
  csi.aliyun.com/snapshot-initial-size: 4Gi
  csi.aliyun.com/snapshot-expansion-size: 1Gi
  csi.aliyun.com/snapshot-expansion-threshold: 50%
