---
# Source: sentry/charts/kafka/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-sentry-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: kafka
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow client connections
    - ports:
        - port: 9092
        - port: 9094
        - port: 9093
---
# Source: sentry/charts/nginx/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: my-sentry-nginx
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: nginx
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 8080
        - port: 8443
---
# Source: sentry/charts/kafka/templates/broker/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-sentry-kafka-broker
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: broker
    app.kubernetes.io/part-of: kafka
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: broker
      app.kubernetes.io/part-of: kafka
---
# Source: sentry/charts/kafka/templates/controller-eligible/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-sentry-kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
---
# Source: sentry/charts/nginx/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-sentry-nginx
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: nginx
---
# Source: sentry/charts/rabbitmq/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-sentry-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
spec:
  minAvailable: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: my-sentry
---
# Source: sentry/charts/kafka/templates/provisioning/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sentry-kafka-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
automountServiceAccountToken: false
---
# Source: sentry/charts/kafka/templates/rbac/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sentry-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: kafka
automountServiceAccountToken: false
---
# Source: sentry/charts/nginx/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sentry-nginx
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
automountServiceAccountToken: false
---
# Source: sentry/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-sentry-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
automountServiceAccountToken: true
secrets:
  - name: my-sentry-rabbitmq
---
# Source: sentry/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: my-sentry-sentry-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
---
# Source: sentry/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-kafka-kraft-cluster-id
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
type: Opaque
data:
  kraft-cluster-id: "RFhYZ1k0TUUwQXVwbFFxWkVzNUJlQQ=="
---
# Source: sentry/charts/nginx/templates/tls-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-nginx-tls
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
type: kubernetes.io/tls
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURsakNDQW42Z0F3SUJBZ0lRU2tBS3FUcVRKTllxeWYyT01ESWJRVEFOQmdrcWhraUc5dzBCQVFzRkFEQVQKTVJFd0R3WURWUVFERXdodVoybHVlQzFqWVRBZUZ3MHlOREE1TVRZd09ERXhNRE5hRncweU5UQTVNVFl3T0RFeApNRE5hTUJveEdEQVdCZ05WQkFNVEQyMTVMWE5sYm5SeWVTMXVaMmx1ZURDQ0FTSXdEUVlKS29aSWh2Y05BUUVCCkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUxyS1F1R1F0UlNjOU94L1lERGpDb0g5VVQzYmFPcmYxQ0JNcmFqdlZqUzkKSTZOdzl0OGNDYy95TzVTWHVZMXZsNSswcHNMQzRSTWl2a01maTI4QkdnaUdiTXZ6MytsOGJBSjJQd0ZUalFSRAp4WDgyY0d1L2RhUEovd3VMNm15V05ObXVaRXJCMHlFekpYWGtmOUVTYTJTSFpyejgzWk1nRHc1RkFUL2ZUam5lCitVc1VwZHE4UXFoNmVoaGxabnlEdEZ4VHBxY2JBYTZ2MTg4Y28ySW12RGJKRDJEQUxQUFMwcHNzK0s4dW1GWkwKZFhEL1VERERDTEJ0VFJxd3h4eVdkOWZ2Nyt6RVFob0FxT2VwbU1ZdWtobC8yTnR3aEVpeXVjSk9wcFo3b2pldgpYcG1ldGw1Q2hQUzhzcU9HQ1ZCSGNzcStGcG9oVzVwc3ExcWlGWWNDU0E4Q0F3RUFBYU9CM2pDQjJ6QU9CZ05WCkhROEJBZjhFQkFNQ0JhQXdIUVlEVlIwbEJCWXdGQVlJS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUF3R0ExVWQKRXdFQi93UUNNQUF3SHdZRFZSMGpCQmd3Rm9BVU1RblRDTTIxSDhlekl1VzBCaTc0TUkzV0tqWXdld1lEVlIwUgpCSFF3Y29JUGJYa3RjMlZ1ZEhKNUxXNW5hVzU0Z2hkdGVTMXpaVzUwY25rdGJtZHBibmd1WkdWbVlYVnNkSUliCmJYa3RjMlZ1ZEhKNUxXNW5hVzU0TG1SbFptRjFiSFF1YzNaamdpbHRlUzF6Wlc1MGNua3RibWRwYm5ndVpHVm0KWVhWc2RDNXpkbU11WTJ4MWMzUmxjaTVzYjJOaGJEQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFQSG5jaGZEaApuUDNmQ1grWm1JbStXVHdpYzU2L2RUa3NFMW84MUREdmpYdWtRM1F1YnhMYzhlakFzbGJWdDNoQVBQZXJVaktoCitiNzB3L2NhVWYyOGhZUXZ4eUhBYmVWTWNFWW1lUldwL0ZhdkI0TUV2Z3FhVHkyVXUwcDdFZ0FpMVh6VmV2dEoKd3ZQMlpvdlB1YWJVdzdONjNwRUhIN0xHcjVCbCtiWXZKaEFOTFFaSDZ4TDlLNEErY3VNYldKOFdmNUZtbFpYZwpHK1JFbThoRXNuWFhCa2ZsdnpiNW5BSzFMaUZWYVplRklOWnkyam1pVVBYMktjY0ppRmgxRGVBOWs2RzYwY3NUClRzUDBJRzhYdkdZaFBnbUZVeUpBYjY5VnRvbkNSN1NPWjdaNjJkTDZuck1zZHQvYkNvM3pOK1BGV0dNL0hHTU0KNXhDdjJnLzRXekpUalE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBdXNwQzRaQzFGSnowN0g5Z01PTUtnZjFSUGR0bzZ0L1VJRXl0cU85V05MMGpvM0QyCjN4d0p6L0k3bEplNWpXK1huN1Ntd3NMaEV5SytReCtMYndFYUNJWnN5L1BmNlh4c0FuWS9BVk9OQkVQRmZ6WncKYTc5MW84bi9DNHZxYkpZMDJhNWtTc0hUSVRNbGRlUi8wUkpyWklkbXZQemRreUFQRGtVQlA5OU9PZDc1U3hTbAoycnhDcUhwNkdHVm1mSU8wWEZPbXB4c0JycS9Yenh5allpYThOc2tQWU1Bczg5TFNteXo0cnk2WVZrdDFjUDlRCk1NTUlzRzFOR3JESEhKWjMxKy92N01SQ0dnQ281Nm1ZeGk2U0dYL1kyM0NFU0xLNXdrNm1sbnVpTjY5ZW1aNjIKWGtLRTlMeXlvNFlKVUVkeXlyNFdtaUZibW15cldxSVZod0pJRHdJREFRQUJBb0lCQVFDQWpaTmFDbTJWc2pBZQpVOHV2MlpIeHhKV1ZMTndZU2RPRUowR2RjcHF4MFRvbDBpa1ZoQVo4eHdtYVNwRzVkWU8wdG0vT3VyQ01xdXltCjVaN2cxckZFREwvejNmUE1qWVVJYkxrUUFLQ2N2c2U5dFluSm5nc29ubXlaVmNJUTJZK21BUStsSTZCYXlRaVAKQ3FPdEVEQTVEN2IxZWxKRVgxdjlSaUNHQlBtaGVWWVVBRWFWSjdOQ05XN1lzQksvZWtNQmRFMCtxdnRkUlQzOQozYXh5blhMS0RscGF2c0daR3U4Ry9Zd0xldjBSdHlHYVJtcFlLNlRtRUh1Y25FMkxwblNlb2w4bTBxaXlYRnhLCmIvUDdqNjdsSWU3S091VFVoYnRCakJLS3kyTVRkamZIMm0rcjN2VnkyMDU2aWpJR3ZNaWhWOW8yc0c1WmJuN0oKSXZQaEVFQ0JBb0dCQU16S2RSRTE1SjlsMkFydElnT1VzT2NMdmlRRWU2cE9rYS9JUTJYdlAvMlhUWkgya1ZrUwpIcmVCWWFVUVJ3ZkFybVlhYkducVMwMGFmamVTTEtqb0RhaG04anlmSDh6b1hPenlMdHczVGJZNHFWTUtIMWZlCmh0cGNzRHlvRDYrRU1HYTdiWVlKaHpVOVppWldQTkp2U0V5TU95aVZsejZVWFgrdGpMcUlKb2JiQW9HQkFPbC8KZjE5ODVRZTd1cFdYNFlROGVqN3lBY1FZRmorZ2FOUzc4L2ljaHkzcWsrUzFKR2V0OWM3T3RqMnZCM3R4b01PdQorc002ejByZ3U1V0JIWWlBVndzcXdONWpFRmgzL1k0bW9WYzhyQ0RlU21DRCt4YTJ3YzRaN2pieVZwNkdUenBhCmdHemQ5UGhBNmh4TURtTXdwVmJ1SThpY2Y3dHZhMk9wL0dZa2hhZmRBb0dBTUhUYlBHWkhJVnhFd1ZrWXlrWWQKa1ozYXNNVUt5ZUpyWUc5L1d4aEFTTzNMSWZWNHNOMnp2UEgxTzh4ZE1qMXFScWlMN0lmT3A0RFVTcXZndTFxbApHS3E3T0pMaVFHYmV6Z1lFQm9GamR2RnpSejNHNnRDeGtldUttOStjVG9oMXQya3o0aHBSRHN6blpielVDV3JoCmJaTmswQmFOUWZaekpTblllYTZMaHk4Q2dZRUEwMUtwdzVEMGMyQkt5TEt4em8weGtpQlN5dGVGNDBYWk9wZDAKZ1krZ3BFVEdYdG8yUFlOcWtTWHlON2RlRFRIVWJ1WEJXOG13N0RLaWN2ZHM5Q283SFFjdmtOV1ZhSXdJTll2dQp2MVpRN2xwZ2RDTmVycnJ1Z2dwajF1VFMwaHY2UG5URUFFT05QdFQ1VEd6bEpJNVZSV3kzaEF0bmF3cTI1WGRsCjBFZkRreDBDZ1lFQW9nVSt5QmQxR3hzMXZwZ3h6VmZRdVF6SG9JRDJXQmIzb2wxOWlIRlNuWkZaOUMvdlJuSEgKUjBkelhJekE1U21UL1BWRExQck1ManBoRlhHRHYrdys3YmdNQ0xlMkJEWHEwNFFhRDEveUxoSCtrTnBibjA5MAp4U1FubDkxRjlocDU1SkF4a29tM0FzTnJ0bnZLb3FLdEp3N1VHaCtjMjZyb3Z4dDJJU2Frb0QwPQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFVENDQWZtZ0F3SUJBZ0lRTUNQMmduNnJNMTZYYUYrcHVUSUZHREFOQmdrcWhraUc5dzBCQVFzRkFEQVQKTVJFd0R3WURWUVFERXdodVoybHVlQzFqWVRBZUZ3MHlOREE1TVRZd09ERXhNRE5hRncweU5UQTVNVFl3T0RFeApNRE5hTUJNeEVUQVBCZ05WQkFNVENHNW5hVzU0TFdOaE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBCk1JSUJDZ0tDQVFFQTBWeGt5VTZ3d20vcDdsNldxbkZEcVBpaVU0Qk5kTnhvck1uT2RrVnJYTFNGbkdqYzhCV2oKWFo2TzkrWm9NQzgyL2Z1NlF5NGxMY0ZydmYwTmV1UG41eVBLOVhrbWFUSnIrb2lXZTRvTGJUa1lFem0vM0VrMworalc1TGlSL281eXdqdGVTU0E3NUppSGFTckJQaDQ0emhqU3hpWWczWHJkc2t0MlZ0SURCSjd1eVdMM3lJNTRwCi9xV2xnZ0xtRldNdEUrK2E0QzQwVGZJQlBBemRUR2d0bWIvL3NkVnVrVTVJRmRycXNJQTBqVEJIZDN3VnRNM0cKdkFMRG1PMmVxWHA4YXJsektsTXZXWVlaTDdYdXpuY3JBbkR5NmlqdXpJWWdvSVlFczJpQlZQemN0UTRLRU1qLwpyVGdtWFZkajgvVVYwK3JaN1pURjJuamZHZDZycGo4OFN3SURBUUFCbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DCkFxUXdIUVlEVlIwbEJCWXdGQVlJS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUIKQWY4d0hRWURWUjBPQkJZRUZERUowd2pOdFIvSHN5TGx0QVl1K0RDTjFpbzJNQTBHQ1NxR1NJYjNEUUVCQ3dVQQpBNElCQVFCNUVyTTdnMmdVek5YTlBJak92SnVWOU5zQTB5RXJNck1xbGlveDEwRXhRcDIxS3YyMXdwaVVDTm9QCmRUL3JtWW00R05ZSStoQ1lxSm5aSHgvZ1psd3BuYWtIbjYyc0FJRU9FUm1jU1Y4c3F1dmplaEZ6T29IOUY5a1UKZ1J2R05CYkVKb043SnAvN3BzSmpjL2p4bG5RdmZNZktQQmVXdysyOW5sckRWdjczelEwMHhsdjBvaEg5TEp0UwovTGRiaGdpaW56dlBLbU9oVENMUi9yODd2UDRraEpVNmdIeHNzSnpNY2lRQjdsWkdyMnN1N3ZXamRTbDJ4OTE1Cncrdmh1Zmo3YVhZTm5pc3RxejFSeWxzYzFzMXVtR2NFYWRIRFpRNGFUenFqMWdnZ3UrTE5hVkhJaFRQd1Y4TzMKbkMvU3FKckpzWGJ3VTRQS2kzcTJ2NlhGYWtDRAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
---
# Source: sentry/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-sentry-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-postgresql
    helm.sh/chart: postgresql-12.5.1
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
type: Opaque
data:
  postgres-password: "T1dXaHJhMWZPcg=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: sentry/charts/rabbitmq/templates/config-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-rabbitmq-config
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
type: Opaque
data:
  rabbitmq.conf: |-
    IyMgVXNlcm5hbWUgYW5kIHBhc3N3b3JkCiMjCmRlZmF1bHRfdXNlciA9IGd1ZXN0CiMjIENsdXN0ZXJpbmcKIyMKY2x1c3Rlcl9mb3JtYXRpb24ucGVlcl9kaXNjb3ZlcnlfYmFja2VuZCAgPSByYWJiaXRfcGVlcl9kaXNjb3ZlcnlfazhzCmNsdXN0ZXJfZm9ybWF0aW9uLms4cy5ob3N0ID0ga3ViZXJuZXRlcy5kZWZhdWx0CmNsdXN0ZXJfZm9ybWF0aW9uLm5vZGVfY2xlYW51cC5pbnRlcnZhbCA9IDEwCmNsdXN0ZXJfZm9ybWF0aW9uLm5vZGVfY2xlYW51cC5vbmx5X2xvZ193YXJuaW5nID0gdHJ1ZQpjbHVzdGVyX3BhcnRpdGlvbl9oYW5kbGluZyA9IGF1dG9oZWFsCgpjbHVzdGVyX2Zvcm1hdGlvbi50YXJnZXRfY2x1c3Rlcl9zaXplX2hpbnQgPSAxCgpsb2FkX2RlZmluaXRpb25zID0gL2FwcC9sb2FkX2RlZmluaXRpb24uanNvbgojIHF1ZXVlIG1hc3RlciBsb2NhdG9yCnF1ZXVlX21hc3Rlcl9sb2NhdG9yID0gbWluLW1hc3RlcnMKIyBlbmFibGUgbG9vcGJhY2sgdXNlcgpsb29wYmFja191c2Vycy5ndWVzdCA9IGZhbHNlCmxvYWRfZGVmaW5pdGlvbnMgPSAvYXBwL2xvYWRfZGVmaW5pdGlvbi5qc29uCg==
---
# Source: sentry/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
type: Opaque
data:
  rabbitmq-password: "Z3Vlc3Q="
  
  rabbitmq-erlang-cookie: "cEhncHkzUTZhZFRza3pBVDZiTEhDRnFGVEY3bE14aEE="
---
# Source: sentry/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: load-definition
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
type: Opaque
stringData:
  load_definition.json: |
    {
      "users": [
        {
          "name": "guest",
          "password": "guest",
          "tags": "administrator"
        }
      ],
      "permissions": [{
        "user": "guest",
        "vhost": "/",
        "configure": ".*",
        "write": ".*",
        "read": ".*"
      }],
      "policies": [
        {
          "name": "ha-all",
          "pattern": ".*",
          "vhost": "/",
          "definition": {
            "ha-mode": "all",
            "ha-sync-mode": "automatic",
            "ha-sync-batch-size": 1
          }
        }
      ],
      "vhosts": [
        {
          "name": "/"
        }
      ]
    }
---
# Source: sentry/templates/snuba/secret-snuba-env.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-snuba-env
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
type: Opaque
data:
  CLICKHOUSE_PORT: "OTAwMA=="
  CLICKHOUSE_DATABASE: "ZGVmYXVsdA=="
  CLICKHOUSE_USER: "ZGVmYXVsdA=="
  CLICKHOUSE_PASSWORD: ""
---
# Source: sentry/charts/clickhouse/templates/configmap-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-clickhouse-config
  labels:
    app.kubernetes.io/name: clickhouse-config
    app.kubernetes.io/instance: my-sentry-config
    app.kubernetes.io/managed-by: Helm
data:
  config.xml: |-
    <?xml version="1.0"?>
    <yandex>
        <path>/var/lib/clickhouse/</path>
        <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
        <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>
        <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>

        <include_from>/etc/clickhouse-server/metrica.d/metrica.xml</include_from>

        <users_config>users.xml</users_config>

        <display_name>my-sentry-clickhouse</display_name>
        <listen_host>0.0.0.0</listen_host>
        <http_port>8123</http_port>
        <tcp_port>9000</tcp_port>
        <interserver_http_port>9009</interserver_http_port>
        <max_connections>4096</max_connections>
        <keep_alive_timeout>3</keep_alive_timeout>
        <max_concurrent_queries>100</max_concurrent_queries>
        <uncompressed_cache_size>8589934592</uncompressed_cache_size>
        <mark_cache_size>5368709120</mark_cache_size>
        <timezone>UTC</timezone>
        <umask>022</umask>
        <mlock_executable>false</mlock_executable>
        <remote_servers>
            <my-sentry-clickhouse>
                <shard>
                    <replica>
                        <internal_replication>true</internal_replication>
                        <host>my-sentry-clickhouse-0.my-sentry-clickhouse-headless.default.svc.cluster.local</host>
                        <port>9000</port>
                        <user>default</user>
                        <compression>true</compression>
                    </replica>
                </shard>
            </my-sentry-clickhouse>
        </remote_servers>
        <zookeeper incl="zookeeper-servers" optional="true" />
        <macros incl="macros" optional="true" />
        <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
        <max_session_timeout>3600</max_session_timeout>
        <default_session_timeout>60</default_session_timeout>
        <disable_internal_dns_cache>1</disable_internal_dns_cache>

        <query_log>
            <database>system</database>
            <table>query_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>

        <query_thread_log>
            <database>system</database>
            <table>query_thread_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_thread_log>

        <distributed_ddl>
            <path>/clickhouse/task_queue/ddl</path>
        </distributed_ddl>
        <logger>
            <level>trace</level>
            <log>/var/log/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>1000M</size>
            <count>10</count>
        </logger>
    </yandex>
---
# Source: sentry/charts/clickhouse/templates/configmap-metrika.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-clickhouse-metrica
  labels:
    app.kubernetes.io/name: clickhouse-metrica
    app.kubernetes.io/instance: my-sentry-metrica
    app.kubernetes.io/managed-by: Helm
data:
  metrica.xml: |-
    <?xml version="1.0"?>
    <yandex>
        <zookeeper-servers>
            <node index="clickhouse">
                <host>my-sentry-zookeeper-clickhouse</host>
                <port>2181</port>
            </node>
            <session_timeout_ms>30000</session_timeout_ms>
            <operation_timeout_ms>10000</operation_timeout_ms>
            <root></root>
            <identity></identity>
        </zookeeper-servers>
        <clickhouse_remote_servers>
            <my-sentry-clickhouse>
                <shard>
                    <replica>
                        <internal_replication>true</internal_replication>
                        <host>my-sentry-clickhouse-0.my-sentry-clickhouse-headless.default.svc.cluster.local</host>
                        <port>9000</port>
                        <user>default</user>
                        <compression>true</compression>
                    </replica>
                </shard>
            </my-sentry-clickhouse>
        </clickhouse_remote_servers>

        <macros>
            <replica from_env="HOSTNAME"></replica>
            <shard from_env="SHARD"></shard>
        </macros>
    </yandex>
---
# Source: sentry/charts/clickhouse/templates/configmap-users.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-clickhouse-users
  labels:
    app.kubernetes.io/name: clickhouse-users
    app.kubernetes.io/instance: my-sentry-users
    app.kubernetes.io/managed-by: Helm
data:
  users.xml: |-
    <?xml version="1.0"?>
    <yandex>
    </yandex>
---
# Source: sentry/charts/kafka/templates/controller-eligible/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-kafka-controller-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
data:
  server.properties: |-
    # Listeners configuration
    listeners=CLIENT://:9092,INTERNAL://:9094,CONTROLLER://:9093
    advertised.listeners=CLIENT://advertised-address-placeholder:9092,INTERNAL://advertised-address-placeholder:9094
    listener.security.protocol.map=CLIENT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
    # KRaft process roles
    process.roles=controller,broker
    #node.id=
    controller.listener.names=CONTROLLER
    controller.quorum.voters=0@my-sentry-kafka-controller-0.my-sentry-kafka-controller-headless.default.svc.cluster.local:9093,1@my-sentry-kafka-controller-1.my-sentry-kafka-controller-headless.default.svc.cluster.local:9093,2@my-sentry-kafka-controller-2.my-sentry-kafka-controller-headless.default.svc.cluster.local:9093
    # Kafka data logs directory
    log.dir=/bitnami/kafka/data
    # Kafka application logs directory
    logs.dir=/opt/bitnami/kafka/logs

    # Common Kafka Configuration
    
    # Interbroker configuration
    inter.broker.listener.name=INTERNAL

    # Custom Kafka Configuration
---
# Source: sentry/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
data:
  kafka-init.sh: |-
    #!/bin/bash

    set -o errexit
    set -o nounset
    set -o pipefail

    error(){
      local message="${1:?missing message}"
      echo "ERROR: ${message}"
      exit 1
    }

    retry_while() {
        local -r cmd="${1:?cmd is missing}"
        local -r retries="${2:-12}"
        local -r sleep_time="${3:-5}"
        local return_value=1

        read -r -a command <<< "$cmd"
        for ((i = 1 ; i <= retries ; i+=1 )); do
            "${command[@]}" && return_value=0 && break
            sleep "$sleep_time"
        done
        return $return_value
    }

    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }

    kafka_conf_set() {
        local file="${1:?missing file}"
        local key="${2:?missing key}"
        local value="${3:?missing value}"

        # Check if the value was set before
        if grep -q "^[#\\s]*$key\s*=.*" "$file"; then
            # Update the existing key
            replace_in_file "$file" "^[#\\s]*${key}\s*=.*" "${key}=${value}" false
        else
            # Add a new key
            printf '\n%s=%s' "$key" "$value" >>"$file"
        fi
    }

    replace_placeholder() {
      local placeholder="${1:?missing placeholder value}"
      local password="${2:?missing password value}"
      local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues with delimiter symbols in sed string
      sed -i "s${del}$placeholder${del}$password${del}g" "$KAFKA_CONFIG_FILE"
    }

    append_file_to_kafka_conf() {
        local file="${1:?missing source file}"
        local conf="${2:?missing kafka conf file}"

        cat "$1" >> "$2"
    }

    configure_external_access() {
      # Configure external hostname
      if [[ -f "/shared/external-host.txt" ]]; then
        host=$(cat "/shared/external-host.txt")
      elif [[ -n "${EXTERNAL_ACCESS_HOST:-}" ]]; then
        host="$EXTERNAL_ACCESS_HOST"
      elif [[ -n "${EXTERNAL_ACCESS_HOSTS_LIST:-}" ]]; then
        read -r -a hosts <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_HOSTS_LIST}")"
        host="${hosts[$POD_ID]}"
      elif [[ "$EXTERNAL_ACCESS_HOST_USE_PUBLIC_IP" =~ ^(yes|true)$ ]]; then
        host=$(curl -s https://ipinfo.io/ip)
      else
        error "External access hostname not provided"
      fi

      # Configure external port
      if [[ -f "/shared/external-port.txt" ]]; then
        port=$(cat "/shared/external-port.txt")
      elif [[ -n "${EXTERNAL_ACCESS_PORT:-}" ]]; then
        if [[ "${EXTERNAL_ACCESS_PORT_AUTOINCREMENT:-}" =~ ^(yes|true)$ ]]; then
          port="$((EXTERNAL_ACCESS_PORT + POD_ID))"
        else
          port="$EXTERNAL_ACCESS_PORT"
        fi
      elif [[ -n "${EXTERNAL_ACCESS_PORTS_LIST:-}" ]]; then
        read -r -a ports <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_PORTS_LIST}")"
        port="${ports[$POD_ID]}"
      else
        error "External access port not provided"
      fi
      # Configure Kafka advertised listeners
      sed -i -E "s|^(advertised\.listeners=\S+)$|\1,EXTERNAL://${host}:${port}|" "$KAFKA_CONFIG_FILE"
    }

    export KAFKA_CONFIG_FILE=/config/server.properties
    cp /configmaps/server.properties $KAFKA_CONFIG_FILE

    # Get pod ID and role, last and second last fields in the pod name respectively
    POD_ID=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 1 | rev)
    POD_ROLE=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 2 | rev)

    # Configure node.id and/or broker.id
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        if grep -q "broker.id" /bitnami/kafka/data/meta.properties; then
          ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        else
          ID="$(grep "node.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        fi
    else
        ID=$((POD_ID + KAFKA_MIN_ID))
        kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
    fi
    replace_placeholder "advertised-address-placeholder" "${MY_POD_NAME}.my-sentry-kafka-${POD_ROLE}-headless.default.svc.cluster.local"
    if [[ "${EXTERNAL_ACCESS_ENABLED:-false}" =~ ^(yes|true)$ ]]; then
      configure_external_access
    fi
    if [ -f /secret-config/server-secret.properties ]; then
      append_file_to_kafka_conf /secret-config/server-secret.properties $KAFKA_CONFIG_FILE
    fi
---
# Source: sentry/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-sentry-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: sentry/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-sentry-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: sentry/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-sentry-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: sentry/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-zookeeper-clickhouse-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper-clickhouse
    helm.sh/chart: zookeeper-11.4.11
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.2"
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOSTNAME"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: sentry/templates/configmap-nginx.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-nginx
data:
  server-block.conf: |
    
    upstream relay {
      server my-sentry-relay:3000;
    }
    upstream sentry {
      server my-sentry-web:9000;
    }

    server {
      listen 8080;

      proxy_redirect off;
      proxy_buffer_size          128k;
      proxy_buffers              4 256k;
      proxy_busy_buffers_size    256k;
      proxy_set_header Host $host;

      
      location /api/store/ {
        proxy_pass http://relay;
      }

      location ~ ^/api/[1-9]\d*/ {
        proxy_pass http://relay;
      }
      location / {
        proxy_pass http://sentry;
      }

      location /_static/ {
        proxy_pass http://sentry;
        proxy_hide_header Content-Disposition;
      }

    }
---
# Source: sentry/templates/relay/configmap-relay.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-relay
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
data:
  
  config.yml: |-
    relay:
      mode: managed
      upstream: "http://my-sentry-web:9000/"
      host: 0.0.0.0
      port: 3000
  
    processing:
      enabled: true
  
      kafka_config:
        - name: "bootstrap.servers"
          value: "my-sentry-kafka:9092"
        - name: "message.max.bytes"
          value: "50000000"
      redis: "redis://:@my-sentry-sentry-redis-master:6379"
      topics:
        metrics_sessions: ingest-metrics
  
    
    # No YAML relay config given
---
# Source: sentry/templates/sentry/configmap-sentry.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-sentry
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
data:
  
  config.yml: |-
  
    # This URL will be used to tell Symbolicator where to obtain the Sentry source.
    # See https://getsentry.github.io/symbolicator/api/
    system.internal-url-prefix: 'http://my-sentry-web:9000'
    symbolicator.enabled: false
  
    ##########
    # Github #
    ##########
  
    ##########
    # Google #
    ##########
  
    #########
    # Slack #
    #########
  
    ###########
    # Discord #
    ###########
  
    #########
    # Redis #
    #########
    redis.clusters:
      default:
        hosts:
          0:
            host: "my-sentry-sentry-redis-master"
            port: 6379
            password: ""
  
    ################
    # File storage #
    ################
    # Uploaded media uses these `filestore` settings. The available
    # backends are either `filesystem` or `s3`.
    filestore.backend: "filesystem"
    filestore.options:
      location: "/var/lib/sentry/files"
    
  sentry.conf.py: |-
    from sentry.conf.server import *  # NOQA
    from distutils.util import strtobool
  
    BYTE_MULTIPLIER = 1024
    UNITS = ("K", "M", "G")
    def unit_text_to_bytes(text):
        unit = text[-1].upper()
        power = UNITS.index(unit) + 1
        return float(text[:-1])*(BYTE_MULTIPLIER**power)
  
    DATABASES = {
        "default": {
            "ENGINE": "sentry.db.postgres",
            "NAME": os.environ.get("POSTGRES_NAME", ""),
            "USER": os.environ.get("POSTGRES_USER", ""),
            "PASSWORD": os.environ.get("POSTGRES_PASSWORD", ""),
            "HOST": os.environ.get("POSTGRES_HOST", ""),
            "PORT": os.environ.get("POSTGRES_PORT", ""),
            "CONN_MAX_AGE": 0,
        }
    }
  
    # You should not change this setting after your database has been created
    # unless you have altered all schemas first
    SENTRY_USE_BIG_INTS = True
  
    ###########
    # General #
    ###########
  
  
    secret_key = env('SENTRY_SECRET_KEY')
    if not secret_key:
      raise Exception('Error: SENTRY_SECRET_KEY is undefined')
  
    SENTRY_OPTIONS['system.secret-key'] = secret_key
  
    # Instruct Sentry that this install intends to be run by a single organization
    # and thus various UI optimizations should be enabled.
    SENTRY_SINGLE_ORGANIZATION = True
  
    SENTRY_OPTIONS["system.event-retention-days"] = int(env('SENTRY_EVENT_RETENTION_DAYS') or "90")
  
    #########
    # Queue #
    #########
  
    # See https://docs.getsentry.com/on-premise/server/queue/ for more
    # information on configuring your queue broker and workers. Sentry relies
    # on a Python framework called Celery to manage queues.
    BROKER_URL = os.environ.get("BROKER_URL", "amqp://guest:guest@my-sentry-rabbitmq:5672//")
  
    #########
    # Cache #
    #########
  
    # Sentry currently utilizes two separate mechanisms. While CACHES is not a
    # requirement, it will optimize several high throughput patterns.
  
    # CACHES = {
    #     "default": {
    #         "BACKEND": "django.core.cache.backends.memcached.MemcachedCache",
    #         "LOCATION": ["memcached:11211"],
    #         "TIMEOUT": 3600,
    #     }
    # }
  
    # A primary cache is required for things such as processing events
    SENTRY_CACHE = "sentry.cache.redis.RedisCache"
  
    DEFAULT_KAFKA_OPTIONS = {
        "bootstrap.servers": "my-sentry-kafka:9092",
        "message.max.bytes": 50000000,
        "socket.timeout.ms": 1000,
    }
  
    SENTRY_EVENTSTREAM = "sentry.eventstream.kafka.KafkaEventStream"
    SENTRY_EVENTSTREAM_OPTIONS = {"producer_configuration": DEFAULT_KAFKA_OPTIONS}
  
    KAFKA_CLUSTERS["default"] = DEFAULT_KAFKA_OPTIONS
  
    ###############
    # Rate Limits #
    ###############
  
    # Rate limits apply to notification handlers and are enforced per-project
    # automatically.
  
    SENTRY_RATELIMITER = "sentry.ratelimits.redis.RedisRateLimiter"
  
    ##################
    # Update Buffers #
    ##################
  
    # Buffers (combined with queueing) act as an intermediate layer between the
    # database and the storage API. They will greatly improve efficiency on large
    # numbers of the same events being sent to the API in a short amount of time.
    # (read: if you send any kind of real data to Sentry, you should enable buffers)
  
    SENTRY_BUFFER = "sentry.buffer.redis.RedisBuffer"
  
    ##########
    # Quotas #
    ##########
  
    # Quotas allow you to rate limit individual projects or the Sentry install as
    # a whole.
  
    SENTRY_QUOTAS = "sentry.quotas.redis.RedisQuota"
  
    ########
    # TSDB #
    ########
  
    # The TSDB is used for building charts as well as making things like per-rate
    # alerts possible.
  
    SENTRY_TSDB = "sentry.tsdb.redissnuba.RedisSnubaTSDB"
  
    #########
    # SNUBA #
    #########
  
    SENTRY_SEARCH = "sentry.search.snuba.EventsDatasetSnubaSearchBackend"
    SENTRY_SEARCH_OPTIONS = {}
    SENTRY_TAGSTORE_OPTIONS = {}
  
    ###########
    # Digests #
    ###########
  
    # The digest backend powers notification summaries.
  
    SENTRY_DIGESTS = "sentry.digests.backends.redis.RedisBackend"
  
    ###################
    # Metrics Backend #
    ###################
  
    SENTRY_RELEASE_HEALTH = "sentry.release_health.metrics.MetricsReleaseHealthBackend"
    SENTRY_RELEASE_MONITOR = "sentry.release_health.release_monitor.metrics.MetricReleaseMonitorBackend"
  
    ##############
    # Web Server #
    ##############
    SENTRY_WEB_HOST = "0.0.0.0"
  
  
    SENTRY_WEB_PORT = 9000
    SENTRY_PUBLIC = False
    SENTRY_WEB_OPTIONS = {
        "http": "%s:%s" % (SENTRY_WEB_HOST, SENTRY_WEB_PORT),
        "protocol": "uwsgi",
        # This is needed to prevent https://git.io/fj7Lw
        "uwsgi-socket": None,
        # Keep this between 15s-75s as that's what Relay supports
        "http-keepalive": 15,
        "http-chunked-input": True,
        # the number of web workers
        'workers': 3,
        # Turn off memory reporting
        "memory-report": False,
        # Some stuff so uwsgi will cycle workers sensibly
        'max-requests': 100000,
        'max-requests-delta': 500,
        'max-worker-lifetime': 86400,
        # Duplicate options from sentry default just so we don't get
        # bit by sentry changing a default value that we depend on.
        'thunder-lock': True,
        'log-x-forwarded-for': False,
        'buffer-size': 32768,
        'limit-post': 209715200,
        'disable-logging': True,
        'reload-on-rss': 600,
        'ignore-sigpipe': True,
        'ignore-write-errors': True,
        'disable-write-exception': True,
    }
  
    ###########
    # SSL/TLS #
    ###########
  
    # If you're using a reverse SSL proxy, you should enable the X-Forwarded-Proto
    # header and enable the settings below
  
    # SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
    # SESSION_COOKIE_SECURE = True
    # CSRF_COOKIE_SECURE = True
    # SOCIAL_AUTH_REDIRECT_IS_HTTPS = True
  
    # End of SSL/TLS settings
  
    ############
    # Features #
    ############
  
  
    SENTRY_FEATURES = {
      "auth:register": True
    }
    SENTRY_FEATURES["projects:sample-events"] = False
    SENTRY_FEATURES.update(
        {
            feature: True
            for feature in ("organizations:advanced-search",
                "organizations:android-mappings",
                "organizations:api-keys",
                "organizations:boolean-search",
                "organizations:related-events",
                "organizations:alert-filters",
                "organizations:custom-symbol-sources",
                "organizations:dashboards-basic",
                "organizations:dashboards-edit",
                "organizations:data-forwarding",
                "organizations:discover",
                "organizations:discover-basic",
                "organizations:discover-query",
                "organizations:discover-frontend-use-events-endpoint",
                "organizations:enterprise-perf",
                "organizations:event-attachments",
                "organizations:events",
                "organizations:global-views",
                "organizations:incidents",
                "organizations:metric-alert-builder-aggregate",
                "organizations:metric-alert-gui-filters",
                "organizations:integrations-event-hooks",
                "organizations:integrations-issue-basic",
                "organizations:integrations-issue-sync",
                "organizations:integrations-alert-rule",
                "organizations:integrations-chat-unfurl",
                "organizations:integrations-incident-management",
                "organizations:integrations-ticket-rules",
                "organizations:integrations-vsts-limited-scopes",
                "organizations:integrations-stacktrace-link",
                "organizations:internal-catchall",
                "organizations:invite-members",
                "organizations:large-debug-files",
                "organizations:monitors",
                "organizations:onboarding",
                "organizations:org-saved-searches",
                "organizations:performance-view",
                "organizations:performance-frontend-use-events-endpoint",
                "organizations:project-detail",
                "organizations:relay",
                "organizations:release-performance-views",
                "organizations:rule-page",
                "organizations:set-grouping-config",
                "organizations:custom-event-title",
                "organizations:slack-migration",
                "organizations:sso-basic",
                "organizations:sso-rippling",
                "organizations:sso-saml2",
                "organizations:sso-migration",
                "organizations:stacktrace-hover-preview",
                "organizations:symbol-sources",
                "organizations:transaction-comparison",
                "organizations:usage-stats-graph",
                "organizations:inbox",
                "organizations:unhandled-issue-flag",
                "organizations:invite-members-rate-limits",
                "organizations:dashboards-v2",
                "organizations:reprocessing-v2",
                "organizations:metrics",
                "organizations:metrics-extraction",
                "organizations:transaction-metrics-extraction",
                "organizations:session-replay",
                "organizations:session-replay-ui",
                "organizations:session-replay-sdk",
                "organizations:session-replay-count-query-optimize",
                "organizations:session-replay-sdk-errors-only",
                "organizations:session-replay-recording-scrubbing",
                "organizations:session-replay-a11y-tab",
                "organizations:session-replay-slack-new-issue",
                "organizations:session-replay-issue-emails",
                "organizations:session-replay-event-linking",
                "organizations:session-replay-weekly-email",
                "organizations:session-replay-trace-table",
                "organizations:session-replay-rage-dead-selectors",
                "organizations:session-replay-new-event-counts",
                "organizations:session-replay-new-timeline",
                "organizations:issue-details-replay-event",
                "organizations:issue-platform","organizations:dashboards-mep",
                "organizations:mep-rollout-flag",
                "organizations:dashboards-rh-widget",
                "organizations:metrics-extraction",
                "organizations:transaction-metrics-extraction",
  
                "projects:alert-filters",
                "projects:custom-inbound-filters",
                "projects:data-forwarding",
                "projects:discard-groups",
                "projects:issue-alerts-targeting",
                "projects:minidump",
                "projects:rate-limits",
                "projects:sample-events",
                "projects:servicehooks",
                "projects:similarity-view",
                "projects:similarity-indexing",
                "projects:similarity-view-v2",
                "projects:similarity-indexing-v2",
  
                "projects:plugins",
            )
        }
    )
  
    #######################
    # Email Configuration #
    #######################
    SENTRY_OPTIONS['mail.backend'] = os.getenv("SENTRY_EMAIL_BACKEND", "dummy")
    SENTRY_OPTIONS['mail.use-tls'] = bool(strtobool(os.getenv("SENTRY_EMAIL_USE_TLS", "false")))
    SENTRY_OPTIONS['mail.use-ssl'] = bool(strtobool(os.getenv("SENTRY_EMAIL_USE_SSL", "false")))
    SENTRY_OPTIONS['mail.username'] = os.getenv("SENTRY_EMAIL_USERNAME", "")
    SENTRY_OPTIONS['mail.password'] = os.getenv("SENTRY_EMAIL_PASSWORD", "")
    SENTRY_OPTIONS['mail.port'] = int(os.getenv("SENTRY_EMAIL_PORT", "25"))
    SENTRY_OPTIONS['mail.host'] = os.getenv("SENTRY_EMAIL_HOST", "")
    SENTRY_OPTIONS['mail.from'] = os.getenv("SENTRY_EMAIL_FROM", "")
  
    #######################
    # Filestore S3 Configuration #
    #######################
  
    #########################
    # Bitbucket Integration #
    #########################
  
    # BITBUCKET_CONSUMER_KEY = 'YOUR_BITBUCKET_CONSUMER_KEY'
    # BITBUCKET_CONSUMER_SECRET = 'YOUR_BITBUCKET_CONSUMER_SECRET'
  
    #########
    # Relay #
    #########
    SENTRY_RELAY_WHITELIST_PK = []
    SENTRY_RELAY_OPEN_REGISTRATION = True
  
    #######################
    # OpenAi Suggestions #
    #######################
  
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    if OPENAI_API_KEY:
      SENTRY_FEATURES["organizations:open-ai-suggestion"] = True
    
    # No Python Extension Config Given
---
# Source: sentry/templates/snuba/configmap-snuba.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-sentry-snuba
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
data:
  
  settings.py: |
    import os
  
    from snuba.settings import *
  
    env = os.environ.get
  
    DEBUG = env("DEBUG", "0").lower() in ("1", "true")
  
    # Clickhouse Options
    CLUSTERS = [
      {
        "host": env("CLICKHOUSE_HOST", "my-sentry-clickhouse"),
        "port": int(9000),
        "user":  env("CLICKHOUSE_USER", "default"),
        "password": env("CLICKHOUSE_PASSWORD", ""),
        "max_connections": int(os.environ.get("CLICKHOUSE_MAX_CONNECTIONS", 100)),
        "database": env("CLICKHOUSE_DATABASE", "default"),
        "http_port": 8123,
        "storage_sets": {
            "cdc",
            "discover",
            "events",
            "events_ro",
            "metrics",
            "metrics_summaries",
            "migrations",
            "outcomes",
            "querylog",
            "sessions",
            "transactions",
            "profiles",
            "functions",
            "replays",
            "generic_metrics_sets",
            "generic_metrics_distributions",
            "search_issues",
            "generic_metrics_counters",
            "spans",
            "group_attributes",
            "generic_metrics_gauges",
            "profile_chunks",
        },
        "single_node": False,
        "cluster_name": "my-sentry-clickhouse",
        "distributed_cluster_name": "my-sentry-clickhouse",
      },
    ]
  
    # Redis Options
    REDIS_HOST = "my-sentry-sentry-redis-master"
    REDIS_PORT = 6379
    REDIS_PASSWORD = ""
    REDIS_DB = int(env("REDIS_DB", 1))
  
    
    # No Python Extension Config Given
---
# Source: sentry/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-sentry-data
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "10Gi"
---
# Source: sentry/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-sentry-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]
---
# Source: sentry/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-sentry-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
subjects:
  - kind: ServiceAccount
    name: my-sentry-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-sentry-rabbitmq-endpoint-reader
---
# Source: sentry/charts/clickhouse/templates/svc-clickhouse-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-clickhouse-headless
  labels:
    app.kubernetes.io/name: clickhouse-headless
    app.kubernetes.io/instance: my-sentry-headless
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: "None"
  ports:
  - port: 9000
    targetPort: tcp-port
    protocol: TCP
    name: tcp-port
  - port: 8123
    targetPort: http-port
    protocol: TCP
    name: http-port
  - port: 9009
    targetPort: inter-http-port
    protocol: TCP
    name: inter-http-port
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-sentry
---
# Source: sentry/charts/clickhouse/templates/svc-clickhouse.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-clickhouse
  labels:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 9000
    targetPort: tcp-port
    protocol: TCP
    name: tcp-port
  - port: 8123
    targetPort: http-port
    protocol: TCP
    name: http-port
  - port: 9009
    targetPort: inter-http-port
    protocol: TCP
    name: inter-http-port
  selector:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-sentry
---
# Source: sentry/charts/kafka/templates/controller-eligible/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-kafka-controller-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      protocol: TCP
      port: 9093
      targetPort: controller
  selector:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/name: kafka
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
---
# Source: sentry/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
      nodePort: null
  selector:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: kafka
---
# Source: sentry/charts/nginx/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-nginx
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      port: 80
      targetPort: http
    - name: https
      port: 443
      targetPort: https
  selector:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/name: nginx
---
# Source: sentry/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-sentry-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-postgresql
    helm.sh/chart: postgresql-12.5.1
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: sentry-postgresql
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: primary
---
# Source: sentry/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-sentry-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-postgresql
    helm.sh/chart: postgresql-12.5.1
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: sentry-postgresql
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: primary
---
# Source: sentry/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-rabbitmq-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: my-sentry
  publishNotReadyAddresses: true
---
# Source: sentry/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
      nodePort: null
    - name: epmd
      port: 4369
      targetPort: epmd
      nodePort: null
    - name: dist
      port: 25672
      targetPort: dist
      nodePort: null
    - name: http-stats
      port: 15672
      targetPort: stats
      nodePort: null
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: my-sentry
---
# Source: sentry/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-sentry-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: sentry-redis
    app.kubernetes.io/instance: my-sentry
---
# Source: sentry/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-sentry-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: sentry-redis
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: master
---
# Source: sentry/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-sentry-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: sentry-redis
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: replica
---
# Source: sentry/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-zookeeper-clickhouse-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper-clickhouse
    helm.sh/chart: zookeeper-11.4.11
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.2"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper-clickhouse
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: zookeeper
---
# Source: sentry/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-zookeeper-clickhouse
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper-clickhouse
    helm.sh/chart: zookeeper-11.4.11
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.2"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper-clickhouse
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/component: zookeeper
---
# Source: sentry/templates/relay/service-relay.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-relay
  annotations:
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
  - port: 3000
    targetPort: 3000
    protocol: TCP
    name: sentry-relay
  selector:
    app: my-sentry
    role: relay
---
# Source: sentry/templates/sentry/web/service-sentry-web.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-web
  annotations:
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
  - port: 9000
    targetPort: 9000
    protocol: TCP
    name: sentry
  selector:
    app: my-sentry
    role: web
---
# Source: sentry/templates/snuba/service-snuba.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-sentry-snuba
  annotations:
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
  - port: 1218
    targetPort: 1218
    protocol: TCP
    name: sentry
  selector:
    app: my-sentry
    role: snuba-api
---
# Source: sentry/charts/nginx/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-nginx
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: 1.27.1
    helm.sh/chart: nginx-18.1.12
spec:
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nginx
        app.kubernetes.io/version: 1.27.1
        helm.sh/chart: nginx-18.1.12
      annotations:
    spec:
      
      shareProcessNamespace: false
      serviceAccountName: my-sentry-nginx
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: nginx
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      hostNetwork: false
      hostIPC: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: preserve-logs-symlinks
          image: docker.io/bitnami/nginx:1.27.1-debian-12-r2
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          command:
            - /bin/bash
          args:
            - -ec
            - |
              #!/bin/bash
              . /opt/bitnami/scripts/libfs.sh
              # We copy the logs folder because it has symlinks to stdout and stderr
              if ! is_dir_empty /opt/bitnami/nginx/logs; then
                cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir
              fi
          volumeMounts:
            - name: empty-dir
              mountPath: /emptydir
      containers:
        - name: nginx
          image: docker.io/bitnami/nginx:1.27.1-debian-12-r2
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: NGINX_HTTP_PORT_NUMBER
              value: "8080"
            - name: NGINX_HTTPS_PORT_NUMBER
              value: "8443"
          envFrom:
          ports:
            - name: http
              containerPort: 8080
            - name: https
              containerPort: 8443
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/nginx/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/nginx/logs
              subPath: app-logs-dir
            - name: empty-dir
              mountPath: /opt/bitnami/nginx/tmp
              subPath: app-tmp-dir
            - name: nginx-server-block
              mountPath: /opt/bitnami/nginx/conf/server_blocks
            - name: certificate
              mountPath: /certs
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: certificate
          secret:
            secretName: my-sentry-nginx-tls
            items:
              - key: tls.crt
                path: server.crt
              - key: tls.key
                path: server.key
        - name: nginx-server-block
          configMap:
            name: my-sentry-nginx
---
# Source: sentry/templates/sentry/cron/deployment-sentry-cron.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-cron
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: cron
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: cron
    spec:
      affinity:
      containers:
      - name: sentry-cron
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "cron"
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/web/deployment-sentry-web.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-web
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  revisionHistoryLimit: 10
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: web
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: web
    spec:
      affinity:
      containers:
      - name: sentry-web
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry", "run", "web"]
        ports:
        - containerPort: 9000
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /_health/
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /_health/
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        persistentVolumeClaim:
          claimName: my-sentry-data
---
# Source: sentry/templates/sentry/worker/deployment-sentry-worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-worker
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: worker
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: worker
    spec:
      affinity:
      containers:
      - name: sentry-worker
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "worker"
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        livenessProbe:
          periodSeconds: 60
          initialDelaySeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
          exec:
            command:
              - sentry
              - exec
              - -c
              - 'from sentry.celery import app; import os; dest="celery@{}".format(os.environ["HOSTNAME"]); print(app.control.ping(destination=[dest], timeout=5)[0][dest]["ok"])'
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/snuba/deployment-snuba-api.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-api
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: snuba-api
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-api
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: 1218
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /health
            port: 1218
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/charts/clickhouse/templates/statefulset-clickhouse.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-clickhouse
  labels:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  serviceName: my-sentry-clickhouse-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: my-sentry
  template:
    metadata:
      annotations:
        checksum/config: 4180250e5dfd09b29fbee09b718da7b26ea3c042cd758ee8c3fd568a70502d94
      labels:
        app.kubernetes.io/name: clickhouse
        app.kubernetes.io/instance: my-sentry
    spec:
      containers:
      - name: my-sentry-clickhouse
        image: yandex/clickhouse-server:21.8.13.6
        imagePullPolicy: IfNotPresent
        command:
          - /bin/bash
          - -c
          - export SHARD=${HOSTNAME##*-} && /entrypoint.sh
        ports:
        - name: http-port
          containerPort: 8123
        - name: tcp-port
          containerPort: 9000
        - name: inter-http-port
          containerPort: 9009
        livenessProbe:
          tcpSocket:
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        readinessProbe:
          tcpSocket:
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        volumeMounts:
        - name: my-sentry-clickhouse-data
          mountPath: /var/lib/clickhouse
        - name: my-sentry-clickhouse-logs
          mountPath: /var/log/clickhouse-server
        - name: my-sentry-clickhouse-config
          mountPath: /etc/clickhouse-server/config.d
        - name: my-sentry-clickhouse-metrica
          mountPath: /etc/clickhouse-server/metrica.d
        - name: my-sentry-clickhouse-users
          mountPath: /etc/clickhouse-server/users.d
      volumes:
      - name: my-sentry-clickhouse-data
        persistentVolumeClaim:
          claimName: my-sentry-clickhouse-data
      - name: my-sentry-clickhouse-logs
        emptyDir: {}
      - name: my-sentry-clickhouse-config
        configMap:
          name: my-sentry-clickhouse-config
          items:
          - key: config.xml
            path: config.xml
      - name: my-sentry-clickhouse-metrica
        configMap:
          name: my-sentry-clickhouse-metrica
          items:
          - key: metrica.xml
            path: metrica.xml
      - name: my-sentry-clickhouse-users
        configMap:
          name: my-sentry-clickhouse-users
          items:
          - key: users.xml
            path: users.xml
  volumeClaimTemplates:
  - metadata:
      name: my-sentry-clickhouse-data
      labels:
        app.kubernetes.io/name: clickhouse-data
        app.kubernetes.io/instance: my-sentry-data
        app.kubernetes.io/managed-by: Helm
    spec:
      accessModes:
      - "ReadWriteOnce"
      resources:
        requests:
          storage: "30Gi"
---
# Source: sentry/charts/kafka/templates/controller-eligible/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
  serviceName: my-sentry-kafka-controller-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kafka
        app.kubernetes.io/version: 3.7.0
        helm.sh/chart: kafka-29.3.2
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/part-of: kafka
      annotations:
        checksum/configuration: 08e2cac95c823d422ea4e9b0c2fa45dd8df2fe04e24be6c911cb20ea0fbd9beb
    spec:
      
      automountServiceAccountToken: false
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/component: controller-eligible
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        seccompProfile:
          type: RuntimeDefault
        supplementalGroups: []
        sysctls: []
      serviceAccountName: my-sentry-kafka
      enableServiceLinks: true
      initContainers:
        - name: kafka-init
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          resources:
            limits: {}
            requests: {} 
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /scripts/kafka-init.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_MIN_ID
              value: "0"
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: kafka-config
              mountPath: /config
            - name: kafka-configmaps
              mountPath: /configmaps
            - name: kafka-secret-config
              mountPath: /secret-config
            - name: scripts
              mountPath: /scripts
            - name: tmp
              mountPath: /tmp
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_KRAFT_CLUSTER_ID
              valueFrom:
                secretKeyRef:
                  name: my-sentry-kafka-kraft-cluster-id
                  key: kraft-cluster-id
          ports:
            - name: controller
              containerPort: 9093
            - name: client
              containerPort: 9092
            - name: interbroker
              containerPort: 9094
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - pgrep
                - -f
                - kafka
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 1024Mi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: kafka-config
              mountPath: /opt/bitnami/kafka/config/server.properties
              subPath: server.properties
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: kafka-configmaps
          configMap:
            name: my-sentry-kafka-controller-configuration
        - name: kafka-secret-config
          emptyDir: {}
        - name: kafka-config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: scripts
          configMap:
            name: my-sentry-kafka-scripts
            defaultMode: 493
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-sentry-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-postgresql
    helm.sh/chart: postgresql-12.5.1
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "15.3.0"
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: my-sentry-sentry-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: sentry-postgresql
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: my-sentry-sentry-postgresql
      labels:
        app.kubernetes.io/name: sentry-postgresql
        helm.sh/chart: postgresql-12.5.1
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "15.3.0"
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: sentry-postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.3.0-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-sentry-sentry-postgresql
                  key: postgres-password
            - name: POSTGRES_DB
              value: "sentry"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "dbname=sentry" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "postgres" -d "dbname=sentry" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.16.2
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.11.18"
spec:
  serviceName: my-sentry-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: my-sentry
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-11.16.2
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.11.18"
      annotations:
        checksum/config: 17eb7b89e96fe120772741e8465241955efec0f22cc862ce8d4495d86af52bbf
        checksum/secret: 8fba2f244ceffac7623bb59d34431ce679a048639b88ed00ef407ed6c1f64b3c
    spec:
      
      serviceAccountName: my-sentry-rabbitmq
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: rabbitmq
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      terminationGracePeriodSeconds: 120
      initContainers:
      containers:
        - name: rabbitmq
          image: docker.io/bitnami/rabbitmq:3.11.18-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -ec
                  - |
                    if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then
                        /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t "120" -d "false"
                    else
                        rabbitmqctl stop_app
                    fi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: my-sentry-rabbitmq-headless
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: "yes"
            - name: RABBITMQ_NODE_NAME
              value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: K8S_HOSTNAME_SUFFIX
              value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: RABBITMQ_MNESIA_DIR
              value: "/bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)"
            - name: RABBITMQ_LDAP_ENABLE
              value: "no"
            - name: RABBITMQ_LOGS
              value: "-"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: "65536"
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  name: my-sentry-rabbitmq
                  key: rabbitmq-erlang-cookie
            - name: RABBITMQ_LOAD_DEFINITIONS
              value: "yes"
            - name: RABBITMQ_DEFINITIONS_FILE
              value: "/app/load_definition.json"
            - name: RABBITMQ_SECURE_PASSWORD
              value: "yes"
            - name: RABBITMQ_USERNAME
              value: "guest"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-sentry-rabbitmq
                  key: rabbitmq-password
            - name: RABBITMQ_PLUGINS
              value: "rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap"
          envFrom:
          ports:
            - name: amqp
              containerPort: 5672
            - name: dist
              containerPort: 25672
            - name: stats
              containerPort: 15672
            - name: epmd
              containerPort: 4369
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - sh
                - -ec
                - rabbitmq-diagnostics -q ping
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - sh
                - -ec
                - rabbitmq-diagnostics -q check_running && rabbitmq-diagnostics -q check_local_alarms
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: configuration
              mountPath: /bitnami/rabbitmq/conf
            - name: data
              mountPath: /bitnami/rabbitmq/mnesia
            - name: load-definition-volume
              mountPath: /app
              readOnly: true
      volumes:
        - name: configuration
          projected:
            sources:
              - secret:
                  name: my-sentry-rabbitmq-config
        - name: load-definition-volume
          secret:
            secretName: "load-definition"
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
        labels:
          app.kubernetes.io/name: rabbitmq
          app.kubernetes.io/instance: my-sentry
      spec:
        accessModes:
            - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-sentry-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: sentry-redis
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/component: master
  serviceName: my-sentry-sentry-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sentry-redis
        helm.sh/chart: redis-17.11.3
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.11"
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: c7dd85aa4ba35d4c4ee487a0c66eeb78a4b00a5ed42881356248c74ce6e60ae6
        checksum/health: 2eb8c335e5359f9e954b057857bd504cfc071558a8fe64755315302a6544c655
        checksum/scripts: bf08e4b595aa96126aa8de8e38cafe2ac54ee22e11041dc05624a225b9dc0a09
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: my-sentry-sentry-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: sentry-redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.11-debian-11-r12
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: my-sentry-sentry-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-sentry-sentry-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-sentry-sentry-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: sentry-redis
          app.kubernetes.io/instance: my-sentry
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-sentry-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: sentry-redis
    helm.sh/chart: redis-17.11.3
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.11"
    app.kubernetes.io/component: replica
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: sentry-redis
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/component: replica
  serviceName: my-sentry-sentry-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sentry-redis
        helm.sh/chart: redis-17.11.3
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.11"
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: c7dd85aa4ba35d4c4ee487a0c66eeb78a4b00a5ed42881356248c74ce6e60ae6
        checksum/health: 2eb8c335e5359f9e954b057857bd504cfc071558a8fe64755315302a6544c655
        checksum/scripts: bf08e4b595aa96126aa8de8e38cafe2ac54ee22e11041dc05624a225b9dc0a09
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: my-sentry-sentry-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: sentry-redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.11-debian-11-r12
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: my-sentry-sentry-redis-master-0.my-sentry-sentry-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: my-sentry-sentry-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: my-sentry-sentry-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: my-sentry-sentry-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: sentry-redis
          app.kubernetes.io/instance: my-sentry
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-sentry-zookeeper-clickhouse
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper-clickhouse
    helm.sh/chart: zookeeper-11.4.11
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.2"
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper-clickhouse
      app.kubernetes.io/instance: my-sentry
      app.kubernetes.io/component: zookeeper
  serviceName: my-sentry-zookeeper-clickhouse-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper-clickhouse
        helm.sh/chart: zookeeper-11.4.11
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.8.2"
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: my-sentry
                    app.kubernetes.io/name: zookeeper-clickhouse
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.2-debian-11-r27
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: my-sentry-zookeeper-clickhouse-0.my-sentry-zookeeper-clickhouse-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: my-sentry-zookeeper-clickhouse-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: sentry/templates/sentry/cleanup/cronjob-sentry-cleanup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-sentry-sentry-cleanup
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
spec:
  schedule: "0 0 * * *"
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 5
  concurrencyPolicy: "Allow"
  jobTemplate:
    spec:
      activeDeadlineSeconds: 100
      template:
        metadata:
          annotations:
            checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
            checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
            checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
          labels:
            app: my-sentry
            release: "my-sentry"
        spec:
          affinity:
          containers:
          - name: sentry-sentry-cleanup
            image: "getsentry/sentry:24.7.1"
            imagePullPolicy: IfNotPresent
            command: ["sentry"]
            args:
              - "cleanup"
              - "--concurrency"
              - "1"
              - "--days"
              - "90"
            env:
            - name: C_FORCE_ROOT
              value: "true"
            - name: SNUBA
              value: http://my-sentry-snuba:1218
            - name: VROOM
              value: http://my-sentry-vroom:8085
            - name: SENTRY_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: my-sentry-sentry-secret
                  key: "key"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: my-sentry-sentry-postgresql
                  key: postgres-password
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_NAME
              value: "sentry"
            - name: POSTGRES_HOST
              value: "my-sentry-sentry-postgresql"
            - name: POSTGRES_PORT
              value: "5432"
            volumeMounts:
            - mountPath: /etc/sentry
              name: config
              readOnly: true
            - mountPath: /var/lib/sentry/files
              name: sentry-data
            resources:
              null
          restartPolicy: Never
          volumes:
          - name: config
            configMap:
              name: my-sentry-sentry
          - name: sentry-data
            emptyDir: {}
---
# Source: sentry/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
 name: my-sentry
 labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
 annotations:
     nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  rules:
    - host: 
      http:
        paths:
          - path: "/"
            pathType: ImplementationSpecific
            backend:
              service:
                name: my-sentry-nginx
                port:
                  number: 80
---
# Source: sentry/templates/hooks/sentry-secret-create.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-sentry-sentry-secret
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-weight": "3"
type: Opaque
data:
  key: "UkFNMUFHeGZFcjV5aGRsVlFpTDdOZ1ltQ3AybHcxRWNOMkZjN2xEd3VWWXdyVmxnVkU="
---
# Source: sentry/templates/relay/deployment-relay.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-relay
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "25"
spec:
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: relay
  replicas: 1
  revisionHistoryLimit: 10
  template:
    metadata:
      annotations:
        checksum/relay: 8074a66c015a8309dc9bdef7524b89bb223749847663f454012dba4e7ed06cc3
        checksum/config.yaml: 3ea47406d7e6264babb4664d6016221d4409a6e110d5c923c37befec27d9d8ae
      labels:
        app: my-sentry
        release: "my-sentry"
        role: relay
    spec:
      affinity:
      initContainers:
        - name: sentry-relay-init
          image: "getsentry/relay:24.7.1"
          imagePullPolicy: IfNotPresent
          args:
            - "credentials"
            - "generate"
          resources:
            {}
          env:
            - name: RELAY_PORT
              value: '3000'
          volumeMounts:
            - name: credentials
              mountPath: /work/.relay
            - name: config
              mountPath: /work/.relay/config.yml
              subPath: config.yml
              readOnly: true
      containers:
      - name: sentry-relay
        image: "getsentry/relay:24.7.1"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 3000
        env:
        - name: RELAY_PORT
          value: '3000'
        volumeMounts:
          - name: credentials
            mountPath: /work/.relay
          - name: config
            mountPath: /work/.relay/config.yml
            subPath: config.yml
            readOnly: true
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /api/relay/healthcheck/live/
            port: 3000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /api/relay/healthcheck/ready/
            port: 3000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-relay
          defaultMode: 0644
      - name: credentials
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/attachments/deployment-sentry-ingest-consumer-attachments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-consumer-attachments
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-consumer-attachments
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-consumer-attachments
    spec:
      affinity:
      containers:
      - name: sentry-ingest-consumer-attachments
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-attachments"
          - "--consumer-group"
          - "ingest-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/events/deployment-sentry-ingest-consumer-events.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-consumer-events
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-consumer-events
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-consumer-events
    spec:
      affinity:
      containers:
      - name: sentry-ingest-consumer-events
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-events"
          - "--consumer-group"
          - "ingest-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/monitors/deployment-sentry-ingest-monitors.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-monitors
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-monitors
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-monitors
    spec:
      affinity:
      containers:
      - name: sentry-ingest-monitors
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-monitors"
          - "--consumer-group"
          - "ingest-monitors"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/occurrences/deployment-sentry-ingest-occurrences.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-occurrences
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-occurrences
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-occurrences
    spec:
      affinity:
      containers:
      - name: sentry-ingest-occurrences
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-occurrences"
          - "--consumer-group"
          - "ingest-occurrences"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/replay-recordings/deployment-sentry-ingest-replay-recordings.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-replay-recordings
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-replay-recordings
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-replay-recordings
    spec:
      affinity:
      containers:
      - name: sentry-ingest-replay-recordings
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-replay-recordings"
          - "--consumer-group"
          - "ingest-replay-recordings"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/ingest/transactions/deployment-sentry-ingest-consumer-transactions.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-ingest-consumer-transactions
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: ingest-consumer-transactions
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: ingest-consumer-transactions
    spec:
      affinity:
      containers:
      - name: sentry-ingest-consumer-transactions
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-transactions"
          - "--consumer-group"
          - "ingest-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/metrics/billing/deployment-sentry-billing-metrics-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-billing-metrics-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: billing-metrics-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: billing-metrics-consumer
    spec:
      affinity:
      containers:
      - name: sentry-billing-metrics-consumer
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "billing-metrics-consumer"
          - "--consumer-group"
          - "billing-metrics-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/metrics/deployment-sentry-metrics-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-metrics-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: metrics-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: metrics-consumer
    spec:
      affinity:
      containers:
      - name: sentry-metrics-consumer
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-metrics"
          - "--consumer-group"
          - "metrics-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/metrics/generic/deployment-sentry-generic-metrics-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-generic-metrics-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: generic-metrics-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: generic-metrics-consumer
    spec:
      affinity:
      containers:
      - name: sentry-generic-metrics-consumer
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "ingest-generic-metrics"
          - "--consumer-group"
          - "generic-metrics-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/post-process-forwarder/errors/deployment-sentry-post-process-forwarder-errors.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-post-process-forward-errors
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-errors
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-errors
    spec:
      affinity:
      containers:
      - name: sentry-post-process-forward-errors
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "post-process-forwarder-errors"
          - "--consumer-group"
          - "post-process-forwarder"
          - "--synchronize-commit-log-topic=snuba-commit-log"
          - "--synchronize-commit-group=snuba-consumers"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/post-process-forwarder/issue-platform/deployment-sentry-post-process-forwarder-issue-platform.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-post-process-forward-issue-platform
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-issue-platform
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-issue-platform
    spec:
      affinity:
      containers:
      - name: sentry-post-process-forward-issue-platform
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "post-process-forwarder-issue-platform"
          - "--consumer-group"
          - "post-process-forwarder"
          - "--synchronize-commit-log-topic=snuba-generic-events-commit-log"
          - "--synchronize-commit-group"
          - "generic_events_group"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/post-process-forwarder/transactions/deployment-sentry-post-process-forwarder-transactions.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-post-process-forward-transactions
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-transactions
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
        role: sentry-post-process-forward-transactions
    spec:
      affinity:
      containers:
      - name: sentry-post-process-forward-transactions
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "post-process-forwarder-transactions"
          - "--consumer-group"
          - "post-process-forwarder"
          - "--synchronize-commit-log-topic=snuba-transactions-commit-log"
          - "--synchronize-commit-group"
          - "transactions_group"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/subscription-consumer/events/deployment-sentry-subscription-consumer-events.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-subscription-consumer-events
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: sentry
        release: "my-sentry"
        role: sentry-subscription-consumer-events
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
        role: sentry-subscription-consumer-events
    spec:
      affinity:
      containers:
      - name: sentry-subscription-consumer-events
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "events-subscription-results"
          - "--consumer-group"
          - "query-subscription-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/subscription-consumer/generic-metrics/deployment-sentry-subscription-consumer-generic-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-subscription-consumer-generic-metrics
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: subscription-consumer-generic-metrics
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: subscription-consumer-generic-metrics
    spec:
      affinity:
      containers:
      - name: sentry-subscription-consumer-generic-metrics
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "generic-metrics-subscription-results"
          - "--consumer-group"
          - "query-subscription-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/subscription-consumer/metrics/deployment-sentry-subscription-consumer-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-subscription-consumer-metrics
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: my-sentry
      release: "my-sentry"
      role: subscription-consumer-metrics
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: my-sentry
        release: "my-sentry"
        role: subscription-consumer-metrics
    spec:
      affinity:
      containers:
      - name: sentry-subscription-consumer-metrics
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "metrics-subscription-results"
          - "--consumer-group"
          - "query-subscription-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
          - "--"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: C_FORCE_ROOT
          value: "true"
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/sentry/subscription-consumer/transactions/deployment-sentry-subscription-consumer-transactions.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-subscription-consumer-transactions
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: sentry
        release: "my-sentry"
        role: sentry-subscription-consumer-transactions
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/configYml: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/sentryConfPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
        role: sentry-subscription-consumer-transactions
    spec:
      affinity:
      containers:
      - name: sentry-subscription-consumer-transactions
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry"]
        args:
          - "run"
          - "consumer"
          - "transactions-subscription-results"
          - "--consumer-group"
          - "query-subscription-consumer"
          - "--healthcheck-file-path"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        - mountPath: /var/lib/sentry/files
          name: sentry-data
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
      - name: sentry-data
        emptyDir: {}
---
# Source: sentry/templates/snuba/deployment-snuba-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "10"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "errors"
          - "--consumer-group"
          - "snuba-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-generic-metrics-counters-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-generic-metrics-counters-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-counters-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-counters-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "generic_metrics_counters_raw"
          - "--consumer-group"
          - "snuba-gen-metrics-counters-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-generic-metrics-distributions-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-generic-metrics-distributions-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-distributions-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-distributions-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "generic_metrics_distributions_raw"
          - "--consumer-group"
          - "snuba-gen-metrics-distributions-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-generic-metrics-sets-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-generic-metrics-sets-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-sets-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-generic-metrics-sets-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "generic_metrics_sets_raw"
          - "--consumer-group"
          - "snuba-gen-metrics-sets-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-group-attributes-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-group-attributes-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-group-attributes-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-group-attributes-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "group_attributes"
          - "--consumer-group"
          - "snuba-group-attributes-group"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-issue-occurrence-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-issue-occurrence-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "16"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: issue-occurrence-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: issue-occurrence-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "search_issues"
          - "--consumer-group"
          - "generic_events_group"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-metrics-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-metrics-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-metrics-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-metrics-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "metrics_raw"
          - "--consumer-group"
          - "snuba-metrics-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-outcomes-billing-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-outcomes-billing-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-outcomes-billing-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-outcomes-billing-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "outcomes_raw"
          - "--consumer-group"
          - "snuba-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--raw-events-topic"
          - "outcomes-billing"
          - "--max-batch-size"
          - "3"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-outcomes-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-outcomes-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "17"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-outcomes-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-outcomes-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "outcomes_raw"
          - "--consumer-group"
          - "snuba-consumers"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-size"
          - "3"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-replacer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-replacer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "18"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-replacer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-replacer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "replacer"
          - "--storage"
          - "errors"
          - "--auto-offset-reset"
          - "earliest"
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-replays-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-replays-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-replays-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-replays-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "replays"
          - "--consumer-group"
          - "replays_group"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-spans-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-spans-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-spans-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-spans-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "spans"
          - "--consumer-group"
          - "snuba-spans-group"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-subscription-consumer-events.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-subscription-consumer-events
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "18"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-events
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-events
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "subscriptions-scheduler-executor"
          - "--auto-offset-reset=earliest"
          - "--dataset=events"
          - "--entity=events"
          - "--consumer-group=snuba-events-subscriptions-consumers"
          - "--followed-consumer-group=snuba-consumers"
          - "--schedule-ttl=60"
          - "--stale-threshold-seconds=900"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-subscription-consumer-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-subscription-consumer-metrics
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "18"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-metrics
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-metrics
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "subscriptions-scheduler-executor"
          - "--auto-offset-reset=earliest"
          - "--dataset=metrics"
          - "--entity=metrics_sets"
          - "--entity=metrics_counters"
          - "--consumer-group=snuba-metrics-subscriptions-consumers"
          - "--followed-consumer-group=snuba-metrics-consumers"
          - "--schedule-ttl=60"
          - "--stale-threshold-seconds=900"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-subscription-consumer-transactions.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-subscription-consumer-transactions
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "18"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-transactions
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-subscription-consumer-transactions
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "subscriptions-scheduler-executor"
          - "--auto-offset-reset=earliest"
          - "--dataset=transactions"
          - "--entity=transactions"
          - "--consumer-group=snuba-transactions-subscriptions-consumers"
          - "--followed-consumer-group=transactions_group"
          - "--schedule-ttl=60"
          - "--stale-threshold-seconds=900"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/snuba/deployment-snuba-transactions-consumer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-sentry-snuba-transactions-consumer
  labels:
    app: my-sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
    app.kubernetes.io/managed-by: "Helm"
  annotations:
    meta.helm.sh/release-name: "my-sentry"
    meta.helm.sh/release-namespace: "default"
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-weight": "12"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-transactions-consumer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: my-sentry
        release: "my-sentry"
        role: snuba-transactions-consumer
    spec:
      affinity:
      containers:
      - name: sentry-snuba
        image: "getsentry/snuba:24.7.1"
        imagePullPolicy: IfNotPresent
        command:
          - "snuba"
          - "consumer"
          - "--storage"
          - "transactions"
          - "--consumer-group"
          - "transactions_group"
          - "--auto-offset-reset"
          - "earliest"
          - "--max-batch-time-ms"
          - "750"
          - "--health-check-file"
          - "/tmp/health.txt"
        livenessProbe:
          exec:
            command:
              - rm
              - /tmp/health.txt
          initialDelaySeconds: 5
          periodSeconds: 320
        ports:
        - containerPort: 1218
        env:
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
            {}
      volumes:
        - name: config
          configMap:
            name: my-sentry-snuba
---
# Source: sentry/charts/kafka/templates/provisioning/job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: my-sentry-kafka-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-sentry
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.7.0
    helm.sh/chart: kafka-29.3.2
    app.kubernetes.io/component: kafka-provisioning
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: my-sentry
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kafka
        app.kubernetes.io/version: 3.7.0
        helm.sh/chart: kafka-29.3.2
        app.kubernetes.io/component: kafka-provisioning
    spec:
      serviceAccountName: my-sentry-kafka-provisioning
      automountServiceAccountToken: false
      enableServiceLinks: true
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        seccompProfile:
          type: RuntimeDefault
        supplementalGroups: []
        sysctls: []
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      initContainers:
        - name: wait-for-available-kafka
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          command:
            - /bin/bash
          args:
            - -ec
            - |
              wait-for-port \
                --host=my-sentry-kafka \
                --state=inuse \
                --timeout=120 \
                9092;
              echo "Kafka is available";
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
      containers:
        - name: kafka-provisioning
          image: docker.io/bitnami/kafka:3.7.0-debian-12-r7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
          command:
            - /bin/bash
          args:
            - -ec
            - |
              echo "Configuring environment"
              . /opt/bitnami/scripts/libkafka.sh
              export CLIENT_CONF="${CLIENT_CONF:-/tmp/client.properties}"
              if [ ! -f "$CLIENT_CONF" ]; then
                touch $CLIENT_CONF

                kafka_common_conf_set "$CLIENT_CONF" security.protocol "PLAINTEXT"
              fi

              echo "Running pre-provisioning script if any given"
              
              

              kafka_provisioning_commands=(
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic event-replacements"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic cdc"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic transactions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-transactions-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic outcomes"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic outcomes-billing"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-sessions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-sessions-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-metrics-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-transactions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-sessions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-generic-metrics-sets"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-generic-metrics-distributions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic scheduled-subscriptions-generic-metrics-counters"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic events-subscription-results"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic transactions-subscription-results"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic sessions-subscription-results"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic metrics-subscription-results"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic generic-metrics-subscription-results"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-queries"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic processed-profiles"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic profiles-call-tree"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config max.message.bytes=15000000 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-replay-events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-generic-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-generic-metrics-sets-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-generic-metrics-distributions-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-generic-metrics-counters-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic generic-events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config cleanup.policy=compact,delete \
                    --config min.compaction.lag.ms=3600000 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-generic-events-commit-log"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config message.timestamp.type=LogAppendTime \
                    --command-config ${CLIENT_CONF} \
                    --topic group-attributes"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-attribution"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-sessions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-generic-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-replays"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-generic-events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-querylog"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-dead-letter-group-attributes"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-attachments"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-transactions"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-replay-recordings"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-performance-metrics"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-monitors"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic profiles"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic ingest-occurrences"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-spans"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic shared-resources-usage"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic snuba-metrics-summaries"
              )

              echo "Starting provisioning"
              for ((index=0; index < ${#kafka_provisioning_commands[@]}; index+=1))
              do
                for j in $(seq ${index} $((${index}+1-1)))
                do
                    ${kafka_provisioning_commands[j]} & # Async command
                done
                wait  # Wait the end of the jobs
              done

              echo "Running post-provisioning script if any given"
              
              

              echo "Provisioning succeeded"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_SERVICE
              value: my-sentry-kafka:9092
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 1024Mi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          volumeMounts:
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: tmp
          emptyDir: {}
---
# Source: sentry/templates/hooks/sentry-db-check.job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-sentry-db-check
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "hook-succeeded,before-hook-creation"
    "helm.sh/hook-weight": "-1"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: my-sentry-db-check
      annotations:
      labels:
        app: sentry
        release: "my-sentry"
    spec:
      restartPolicy: Never
      containers:
      - name: db-check
        image: subfuzion/netcat:latest
        imagePullPolicy: IfNotPresent
        command:
          - /bin/sh
          - -c
          - |
            echo "Checking if clickhouse is up"
            CLICKHOUSE_STATUS=0
            while [ $CLICKHOUSE_STATUS -eq 0 ]; do
              CLICKHOUSE_STATUS=1
              CLICKHOUSE_REPLICAS=1
              i=0; while [ $i -lt $CLICKHOUSE_REPLICAS ]; do
                CLICKHOUSE_HOST=my-sentry-clickhouse-$i.my-sentry-clickhouse-headless
                if ! nc -z "$CLICKHOUSE_HOST" 9000; then
                  CLICKHOUSE_STATUS=0
                  echo "$CLICKHOUSE_HOST is not available yet"
                fi
                i=$((i+1))
              done
              if [ "$CLICKHOUSE_STATUS" -eq 0 ]; then
                echo "Clickhouse not ready. Sleeping for 10s before trying again"
                sleep 10;
              fi
            done
            echo "Clickhouse is up"

            echo "Checking if kafka is up"
            KAFKA_STATUS=0
            while [ $KAFKA_STATUS -eq 0 ]; do
              KAFKA_STATUS=1
              KAFKA_REPLICAS=3
              echo "Kafka Kraft is enabled, checking if Kraft controllers are up"
              KRAFT_STATUS=0
              while [ $KRAFT_STATUS -eq 0 ]; do
                KRAFT_STATUS=1
                i=0; while [ $i -lt $KAFKA_REPLICAS ]; do
                  KRAFT_HOST=my-sentry-kafka-controller-$i.my-sentry-kafka-controller-headless
                  if ! nc -z "$KRAFT_HOST" 9093; then
                    KRAFT_STATUS=0
                    echo "$KRAFT_HOST is not available yet"
                  fi
                  i=$((i+1))
                done
                if [ "$KRAFT_STATUS" -eq 0 ]; then
                  echo "Kraft controllers not ready. Sleeping for 10s before trying again"
                  sleep 10;
                fi
              done
              echo "Kraft controllers are up"
              if [ "$KAFKA_STATUS" -eq 0 ]; then
                echo "Kafka not ready. Sleeping for 10s before trying again"
                sleep 10;
              fi
            done
            echo "Kafka is up"
        env:
        resources:
          limits:
            memory: 64Mi
          requests:
            cpu: 100m
            memory: 64Mi
---
# Source: sentry/templates/hooks/sentry-db-init.job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-sentry-db-init
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "hook-succeeded,before-hook-creation"
    "helm.sh/hook-weight": "6"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: my-sentry-db-init
      annotations:
        checksum/configmap.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
    spec:
      restartPolicy: Never
      containers:
      - name: db-init-job
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["sentry","upgrade","--noinput"]
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        resources:
          limits:
            memory: 2048Mi
          requests:
            cpu: 300m
            memory: 2048Mi
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
---
# Source: sentry/templates/hooks/snuba-db-init.job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-sentry-snuba-db-init
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "hook-succeeded,before-hook-creation"
    "helm.sh/hook-weight": "3"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: my-sentry-snuba-db-init
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: sentry
        release: "my-sentry"
    spec:
      restartPolicy: Never
      containers:
      - name: snuba-init
        image: "getsentry/snuba:24.7.1"
        command:
        - snuba
        - bootstrap
        - --no-migrate
        - --kafka
        - --force
        env:
        - name: LOG_LEVEL
          value: debug
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
          limits:
            cpu: 2000m
            memory: 1Gi
          requests:
            cpu: 700m
            memory: 1Gi
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/hooks/snuba-migrate.job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-sentry-snuba-migrate
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "hook-succeeded,before-hook-creation"
    "helm.sh/hook-weight": "5"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: my-sentry-snuba-migrate
      annotations:
        checksum/snubaSettingsPy: d5f85a6a8afbc55eebe23801e1a51a0fb4c0428c9a73ef6708d8dc83e079cd49
        checksum/config.yaml: 0a1a428135a3b30d2eacbb2d6e489ffa3b0ae13774d1a6725058e62ddd35f5f4
      labels:
        app: sentry
        release: "my-sentry"
    spec:
      restartPolicy: Never
      containers:
      - name: snuba-migrate
        image: "getsentry/snuba:24.7.1"
        command: [snuba, migrations, migrate, --force]
        env:
        - name: LOG_LEVEL
          value: debug
        - name: SNUBA_SETTINGS
          value: /etc/snuba/settings.py
        - name: DEFAULT_BROKERS
          value: "my-sentry-kafka:9092"
        - name: CLICKHOUSE_MAX_CONNECTIONS
          value: "100"
        envFrom:
        - secretRef:
            name: my-sentry-snuba-env
        volumeMounts:
        - mountPath: /etc/snuba
          name: config
          readOnly: true
        resources:
          limits:
            cpu: 2000m
            memory: 1Gi
          requests:
            cpu: 700m
            memory: 1Gi
      volumes:
      - name: config
        configMap:
          name: my-sentry-snuba
---
# Source: sentry/templates/hooks/user-create.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: my-sentry-user-create
  labels:
    app: sentry
    chart: "sentry-25.7.0"
    release: "my-sentry"
    heritage: "Helm"
  annotations:
    "helm.sh/hook": "post-install,post-upgrade"
    "helm.sh/hook-delete-policy": "hook-succeeded,before-hook-creation"
    "helm.sh/hook-weight": "9"
spec:
  activeDeadlineSeconds: 600
  template:
    metadata:
      name: my-sentry-user-create
      annotations:
        checksum/configmap.yaml: 541cc84e4e68a437fb0bf675bbbe6075ec2124fe7089b62fabca4fb6920270bc
      labels:
        app: sentry
        release: "my-sentry"
    spec:
      restartPolicy: Never
      containers:
      - name: user-create-job
        image: "getsentry/sentry:24.7.1"
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c"]
        # Create user but do not exit 1 when user already exists (exit code 3 from createuser command)
        # https://docs.sentry.io/server/cli/createuser/
        args:
          - >
            sentry createuser \
              --no-input \
              --superuser \
              --email "admin@sentry.local" \
              --password "$ADMIN_PASSWORD" || true; \
            if [ $? -eq 0 ] || [ $? -eq 3 ]; then \
              exit 0; \
            else \
              exit 1; \
            fi
        env:
        - name: SNUBA
          value: http://my-sentry-snuba:1218
        - name: VROOM
          value: http://my-sentry-vroom:8085
        - name: SENTRY_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-secret
              key: "key"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: my-sentry-sentry-postgresql
              key: postgres-password
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_NAME
          value: "sentry"
        - name: POSTGRES_HOST
          value: "my-sentry-sentry-postgresql"
        - name: POSTGRES_PORT
          value: "5432"
        - name: ADMIN_PASSWORD
          value: "aaaa"
        volumeMounts:
        - mountPath: /etc/sentry
          name: config
          readOnly: true
        resources:
          limits:
            memory: 2048Mi
          requests:
            cpu: 300m
            memory: 2048Mi
      volumes:
      - name: config
        configMap:
          name: my-sentry-sentry
