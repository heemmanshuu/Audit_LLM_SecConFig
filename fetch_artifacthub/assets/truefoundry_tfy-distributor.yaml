---
# Source: tfy-distributor/charts/nats/templates/pod-disruption-budget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.7
    helm.sh/chart: nats-1.1.6
  name: my-tfy-distributor-nats
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: my-tfy-distributor
      app.kubernetes.io/name: nats
---
# Source: tfy-distributor/charts/nats/templates/config-map.yaml
apiVersion: v1
data:
  nats.conf: |
    {
      "authorization": {
        "timeout": 5,
        "users": [
          {
            "password": "<placeholder>",
            "permissions": {
              "publish": [
                ">"
              ],
              "subscribe": [
                ">"
              ]
            },
            "user": "<placeholder>"
          }
        ]
      },
      "cluster": {
        "name": "my-tfy-distributor-nats",
        "no_advertise": true,
        "port": 6222,
        "routes": [
          "nats://my-tfy-distributor-nats-0.my-tfy-distributor-nats-headless:6222",
          "nats://my-tfy-distributor-nats-1.my-tfy-distributor-nats-headless:6222",
          "nats://my-tfy-distributor-nats-2.my-tfy-distributor-nats-headless:6222"
        ]
      },
      "http_port": 8222,
      "jetstream": {
        "max_file_store": 2Gi,
        "max_memory_store": 250Mi,
        "store_dir": "/data"
      },
      "lame_duck_duration": "30s",
      "lame_duck_grace_period": "10s",
      "pid_file": "/var/run/nats/nats.pid",
      "port": 4222,
      "server_name": $SERVER_NAME,
      "websocket": {
        "compression": true,
        "no_tls": true,
        "port": 8080
      }
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.7
    helm.sh/chart: nats-1.1.6
  name: my-tfy-distributor-nats-config
---
# Source: tfy-distributor/charts/nats/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.7
    helm.sh/chart: nats-1.1.6
  name: my-tfy-distributor-nats-headless
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: websocket
    port: 8080
    targetPort: websocket
  - appProtocol: tcp
    name: cluster
    port: 6222
    targetPort: cluster
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/name: nats
---
# Source: tfy-distributor/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.7
    helm.sh/chart: nats-1.1.6
  name: my-tfy-distributor-nats
spec:
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: websocket
    port: 8080
    targetPort: websocket
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/name: nats
---
# Source: tfy-distributor/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-tfy-distributor-distributor
spec:
  ports:
    - name: port-3000
      port: 3000
      protocol: TCP
      targetPort: 3000
  selector:
    app: my-tfy-distributor-distributor
  type: ClusterIP
---
# Source: tfy-distributor/templates/deployment.yaml
# deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-tfy-distributor-distributor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-tfy-distributor-distributor
  template:
    metadata:
      labels:
        app: my-tfy-distributor-distributor
    spec:
      tolerations:
        - effect: NoSchedule
          key: cloud.google.com/gke-spot
          operator: Equal
          value: "true"
        - effect: NoSchedule
          key: kubernetes.azure.com/scalesetpriority
          operator: Equal
          value: spot
      containers:
        - name: my-tfy-distributor-distributor
          image: public.ecr.aws/truefoundrycloud/async-service-distributor:5d48113bc678d694a0c8f8dabb2207c5aa2cfc53
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 3000
          env:
            - name: NATS_AUTH_CONFIG
              value: '{"user": "<placeholder>", "password": "<placeholder>"}'
            - name: NATS_CONNECTION_CONFIG
              value: '{"servers": "ws://my-tfy-distributor-nats:8080", "pingInterval": 5000, "maxPingOut": 3}'
            - name: SERVICE_STREAM_REPLICAS
              value: "3"
          resources:
            limits:
              cpu: 0.1
              memory: 150M
            requests:
              cpu: 0.1
              memory: 150M
---
# Source: tfy-distributor/charts/nats/templates/stateful-set.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: my-tfy-distributor
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.7
    helm.sh/chart: nats-1.1.6
  name: my-tfy-distributor-nats
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: my-tfy-distributor
      app.kubernetes.io/name: nats
  serviceName: my-tfy-distributor-nats-headless
  template:
    metadata:
      annotations:
        checksum/config: 6af6911b6b972b5827e221a047e3ec6eabae87b45d45d9252d93e2fa2e0b4cbe
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/component: nats
        app.kubernetes.io/instance: my-tfy-distributor
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nats
        app.kubernetes.io/version: 2.10.7
        helm.sh/chart: nats-1.1.6
    spec:
      containers:
      - args:
        - --config
        - /etc/nats-config/nats.conf
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        image: nats:2.10.7-alpine
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - nats-server
              - -sl=ldm=/var/run/nats/nats.pid
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-enabled-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: nats
        ports:
        - containerPort: 4222
          name: nats
        - containerPort: 8080
          name: websocket
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-server-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 100m
            memory: 250Mi
          requests:
            cpu: 100m
            memory: 250Mi
        startupProbe:
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        volumeMounts:
        - mountPath: /etc/nats-config
          name: config
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /data
          name: my-tfy-distributor-nats-js
      - args:
        - -pid
        - /var/run/nats/nats.pid
        - -config
        - /etc/nats-config/nats.conf
        image: natsio/nats-server-config-reloader:0.14.0
        name: reloader
        volumeMounts:
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /etc/nats-config
          name: config
      - args:
        - -port=7777
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - -jsz=all
        - http://localhost:8222/
        image: natsio/prometheus-nats-exporter:0.13.0
        imagePullPolicy: IfNotPresent
        name: prom-exporter
        ports:
        - containerPort: 7777
          name: prom-metrics
      enableServiceLinks: false
      shareProcessNamespace: true
      tolerations:
      - effect: NoSchedule
        key: cloud.google.com/gke-spot
        operator: Equal
        value: "true"
      - effect: NoSchedule
        key: kubernetes.azure.com/scalesetpriority
        operator: Equal
        value: spot
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: nats
            app.kubernetes.io/instance: my-tfy-distributor
            app.kubernetes.io/name: nats
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
      volumes:
      - configMap:
          name: my-tfy-distributor-nats-config
        name: config
      - emptyDir: {}
        name: pid
  volumeClaimTemplates:
  - metadata:
      name: my-tfy-distributor-nats-js
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
