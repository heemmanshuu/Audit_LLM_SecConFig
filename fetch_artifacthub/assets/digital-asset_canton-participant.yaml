---
# Source: canton-participant/templates/networkpolicy/from-all-to-ledger-api-metrics.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: my-canton-participant-allow-ledger-api-metrics
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/component: participant
      canton.io/participant: participant1
  policyTypes:
  - Ingress
  ingress:
    - ports:
        - protocol: TCP
          port: 4001
---
# Source: canton-participant/templates/networkpolicy/from-same-namespace-to-all.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: my-canton-participant-allow-same-namespace
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: default
---
# Source: canton-participant/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-canton-participant
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
automountServiceAccountToken: false
secrets:
---
# Source: canton-participant/templates/configmap-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-canton-participant-envmap
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
data:
  JDK_JAVA_OPTIONS: "-XX:+ExitOnOutOfMemoryError -XX:InitialRAMPercentage=70 -XX:MaxRAMPercentage=70"
---
# Source: canton-participant/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-canton-participant
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
data:
  participant.conf: |
    canton {
      participants {
        participant1 {          
          storage {
            type = postgres
            config {
              dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
              properties = {
                serverName = "postgres"
                portNumber = 5432
                user = "canton"
                password = ${?PGPASSWORD}
                databaseName = "participant1"
                ssl = true
                sslmode = "require"
              }
            }
            max-connections = 10
          }

          ledger-api {
            address = "0.0.0.0"
            port = 4001
            postgres-data-source.synchronous-commit = off

            command-service.max-commands-in-flight = 10000
            max-contract-state-cache-size = 1000000
            max-contract-key-state-cache-size = 1000000
            max-transactions-in-memory-fan-out-buffer-size = 100000
          }

          admin-api {
            address = "0.0.0.0"
            port = 4002
          }

          caching {
            contract-store {
              maximum-size = 1000000
              expire-after-access = "10m"
            }
          }

          init.parameters.unique-contract-keys = false

          replication.enabled = true

          monitoring.grpc-health-server {
            address = "0.0.0.0"
            port = 4003
          }
        }
      }

      
    }
  participant.canton: |
    participants.local.head.resources.set_resource_limits(
      ResourceLimits(
        maxRate = Some(200),
        maxDirtyRequests = Some(500),
        maxBurstFactor = 10
      )
    )
---
# Source: canton-participant/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-canton-participant
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
---
# Source: canton-participant/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-canton-participant
  namespace: "default"
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
roleRef:
  kind: Role
  name: my-canton-participant
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: my-canton-participant
    namespace: "default"
---
# Source: canton-participant/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-canton-participant
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
spec:
  type: ClusterIP
  ports:
    - name: public
      protocol: TCP
      port: 4001
      targetPort: public
    - name: admin
      protocol: TCP
      port: 4002
      targetPort: admin
  selector:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/component: participant
---
# Source: canton-participant/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-canton-participant
  labels:
    app.kubernetes.io/instance: my-canton-participant
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: canton-participant
    app.kubernetes.io/part-of: canton
    app.kubernetes.io/version: 2.8.1
    canton.io/participant: participant1
    helm.sh/chart: canton-participant-0.6.1
    app.kubernetes.io/component: participant
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: my-canton-participant
      app.kubernetes.io/name: canton-participant
      app.kubernetes.io/component: participant
  strategy:
    type: "Recreate"
  template:
    metadata:
      annotations:
        "checksum/config": "eadf0288403e99b676de1990f9e956883dc6b9dad84609c0290962370efb9da4"
        "checksum/env": "f89bebb7a5711b76de18940f29b1e56966e16ae2dc1d37d82070496f1d942f41"
        "checksum/secret": "01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b"
      labels:
        app.kubernetes.io/instance: my-canton-participant
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: canton-participant
        app.kubernetes.io/part-of: canton
        app.kubernetes.io/version: 2.8.1
        canton.io/participant: participant1
        helm.sh/chart: canton-participant-0.6.1
        app.kubernetes.io/component: participant
    spec:
      serviceAccountName: my-canton-participant
      securityContext:
        fsGroup: 65532
        fsGroupChangePolicy: Always
        sysctls: []
      containers:
        - name: canton
          image: "digitalasset-docker.jfrog.io/canton-enterprise:2.8.1"
          imagePullPolicy: "IfNotPresent"
          securityContext:
            readOnlyRootFilesystem: false
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          args:
            - "daemon"
            - "--log-encoder=plain"
            - "--log-profile=container"
            - "--log-level-root=INFO"
            - "--log-level-canton=INFO"
            - "--log-level-stdout=INFO"
            - "--config=/etc/canton/participant.conf"
            - "--bootstrap=/etc/canton/participant.canton"
          envFrom:
            - configMapRef:
                name: my-canton-participant-envmap
          ports:
            - name: public
              containerPort: 4001
            - name: admin
              containerPort: 4002
          resources:
            limits:
              cpu: 2
              memory: 3Gi
            requests:
              cpu: 500m
          livenessProbe:
            grpc:
              port: 4003
              service: "liveness"
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 3
          readinessProbe:
            grpc:
              port: 4003
              service: ""
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 1
            timeoutSeconds: 3
          volumeMounts:
            - name: config
              mountPath: /etc/canton
              readOnly: true
      volumes:
        - name: config
          configMap:
            name: my-canton-participant
