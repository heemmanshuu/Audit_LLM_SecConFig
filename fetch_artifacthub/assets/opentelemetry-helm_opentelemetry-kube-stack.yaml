---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-operator
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
---
# Source: opentelemetry-kube-stack/templates/hooks.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: delete-resources-sa
  annotations:
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-manager
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - persistentvolumeclaims
      - persistentvolumes
      - pods
      - serviceaccounts
      - services
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - statefulsets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
      - extensions
    resources:
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - config.openshift.io
    resources:
      - infrastructures
      - infrastructures/status
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
      - get
      - list
      - update
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
      - servicemonitors
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - instrumentations
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - opampbridges
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - opampbridges/finalizers
    verbs:
      - update
  - apiGroups:
      - opentelemetry.io
    resources:
      - opampbridges/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors/finalizers
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - opentelemetry.io
    resources:
      - opentelemetrycollectors/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - route.openshift.io
    resources:
      - routes
      - routes/custom-host
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-metrics
rules:
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-proxy
rules:
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
---
# Source: opentelemetry-kube-stack/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-opentelemetry-kube-stack-collector
rules:
- apiGroups: [""]
  resources:
  - namespaces
  - nodes
  - nodes/proxy
  - nodes/metrics
  - nodes/stats
  - services
  - endpoints
  - pods
  - events
  - secrets
  verbs: ["get", "list", "watch"]
- apiGroups: ["monitoring.coreos.com"]
  resources:
  - servicemonitors
  - podmonitors
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs: ["get", "list", "watch"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- apiGroups: ["discovery.k8s.io"]
  resources:
  - endpointslices
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]

- apiGroups:
  - ""
  resources:
  - events
  - namespaces
  - namespaces/status
  - nodes
  - nodes/spec
  - pods
  - pods/status
  - replicationcontrollers
  - replicationcontrollers/status
  - resourcequotas
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - get
  - list
  - watch
- apiGroups:
    - autoscaling
  resources:
    - horizontalpodautoscalers
  verbs:
    - get
    - list
    - watch
- apiGroups: ["events.k8s.io"]
  resources: ["events"]
  verbs: ["watch", "list"]
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-opentelemetry-kube-stack-opentelemetry-operator-manager
subjects:
  - kind: ServiceAccount
    name: opentelemetry-operator
    namespace: default
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-proxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-opentelemetry-kube-stack-opentelemetry-operator-proxy
subjects:
  - kind: ServiceAccount
    name: opentelemetry-operator
    namespace: default
---
# Source: opentelemetry-kube-stack/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-opentelemetry-kube-stack-cluster-stats
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-opentelemetry-kube-stack-collector
subjects:
- kind: ServiceAccount
  # quirk of the Operator
  name: "my-opentelemetry-kube-stack-cluster-stats-collector"
  namespace: default
---
# Source: opentelemetry-kube-stack/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-opentelemetry-kube-stack-daemon
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-opentelemetry-kube-stack-collector
subjects:
- kind: ServiceAccount
  # quirk of the Operator
  name: "my-opentelemetry-kube-stack-daemon-collector"
  namespace: default
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-leader-election
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - configmaps/status
    verbs:
      - get
      - update
      - patch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: opentelemetry-kube-stack/templates/hooks.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: delete-resources-role
  annotations:
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
rules:
  - apiGroups:
      - opentelemetry.io
    resources:
      - instrumentations
      - opampbridges
      - opentelemetrycollectors
    verbs:
      - get
      - list
      - delete
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-leader-election
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-opentelemetry-kube-stack-opentelemetry-operator-leader-election
subjects:
  - kind: ServiceAccount
    name: opentelemetry-operator
    namespace: default
---
# Source: opentelemetry-kube-stack/templates/hooks.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: delete-resources-rolebinding
  annotations:
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: delete-resources-role
subjects:
  - kind: ServiceAccount
    name: delete-resources-sa
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator
  namespace: default
spec:
  ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: metrics
  selector:
      app.kubernetes.io/name: opentelemetry-operator
      app.kubernetes.io/component: controller-manager
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
  namespace: default
spec:
  ports:
    - port: 443
      protocol: TCP
      targetPort: webhook-server
  selector:
      app.kubernetes.io/name: opentelemetry-operator
      app.kubernetes.io/component: controller-manager
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  name: my-opentelemetry-kube-stack-opentelemetry-operator
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-operator
      app.kubernetes.io/component: controller-manager
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        app.kubernetes.io/name: opentelemetry-operator
        app.kubernetes.io/component: controller-manager
    spec:
      hostNetwork: false
      containers:
        - args:
            - --metrics-addr=0.0.0.0:8080
            - --enable-leader-election
            - --health-probe-addr=:8081
            - --webhook-port=9443
            - --collector-image=otel/opentelemetry-collector-k8s:0.107.0
          command:
            - /manager
          env:
            - name: ENABLE_WEBHOOKS
              value: "true"
          image: "ghcr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator:0.107.0"
          name: manager
          ports:
            - containerPort: 8080
              name: metrics
              protocol: TCP
            - containerPort: 9443
              name: webhook-server
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources: 
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          volumeMounts:
            - mountPath: /tmp/k8s-webhook-server/serving-certs
              name: cert
              readOnly: true
        
        - args:
            - --secure-listen-address=0.0.0.0:8443
            - --upstream=http://127.0.0.1:8080/
            - --logtostderr=true
            - --v=0
          image: "quay.io/brancz/kube-rbac-proxy:v0.15.0"
          name: kube-rbac-proxy
          ports:
            - containerPort: 8443
              name: https
              protocol: TCP
          resources: 
            limits:
              cpu: 500m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
      serviceAccountName: opentelemetry-operator
      terminationGracePeriodSeconds: 10
      volumes:
        - name: cert
          secret:
            defaultMode: 420
            secretName: my-opentelemetry-kube-stack-opentelemetry-operator-controller-manager-service-cert
      securityContext:
        fsGroup: 65532
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/admission-webhooks/operator-webhook.yaml
---
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/certmanager.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: webhook
  name: my-opentelemetry-kube-stack-opentelemetry-operator-serving-cert
  namespace: default
spec:
  dnsNames:
    - my-opentelemetry-kube-stack-opentelemetry-operator-webhook.default.svc
    - my-opentelemetry-kube-stack-opentelemetry-operator-webhook.default.svc.cluster.local
  issuerRef:
    kind: Issuer
    name: my-opentelemetry-kube-stack-opentelemetry-operator-selfsigned-issuer
  secretName: my-opentelemetry-kube-stack-opentelemetry-operator-controller-manager-service-cert
  subject:
    organizationalUnits:
      - my-opentelemetry-kube-stack-opentelemetry-operator
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/certmanager.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: webhook
  name: my-opentelemetry-kube-stack-opentelemetry-operator-selfsigned-issuer
  namespace: default
spec:
  selfSigned: {}
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/admission-webhooks/operator-webhook-with-cert-manager.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: default/my-opentelemetry-kube-stack-opentelemetry-operator-serving-cert
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: webhook
  name: my-opentelemetry-kube-stack-opentelemetry-operator-mutation
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /mutate-opentelemetry-io-v1alpha1-instrumentation
        port: 443
    failurePolicy: Ignore
    name: minstrumentation.kb.io
    rules:
    - apiGroups:
        - opentelemetry.io
      apiVersions:
        - v1alpha1
      operations:
        - CREATE
        - UPDATE
      resources:
        - instrumentations
      scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /mutate-opentelemetry-io-v1beta1-opentelemetrycollector
        port: 443
    failurePolicy: Ignore
    name: mopentelemetrycollectorbeta.kb.io
    rules:
      - apiGroups:
          - opentelemetry.io
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - opentelemetrycollectors
        scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /mutate-v1-pod
        port: 443
    failurePolicy: Ignore
    name: mpod.kb.io
    rules:
      - apiGroups:
          - ""
        apiVersions:
          - v1
        operations:
          - CREATE
        resources:
          - pods
        scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
---
# Source: opentelemetry-kube-stack/templates/collector.yaml
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: my-opentelemetry-kube-stack-cluster-stats
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-kube-stack-0.2.1
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm        
spec:
  managementState: managed
  mode: deployment
  config:
    exporters:
      debug: {}
    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.tag
          - container.image.name
          - k8s.cluster.uid
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      resourcedetection/env:
        detectors:
        - env
        override: false
        timeout: 2s
    receivers:
      k8s_cluster:
        allocatable_types_to_report:
        - cpu
        - memory
        - storage
        auth_type: serviceAccount
        collection_interval: 10s
        node_conditions_to_report:
        - Ready
        - MemoryPressure
        - DiskPressure
        - NetworkUnavailable
      k8sobjects:
        objects:
        - exclude_watch_type:
          - DELETED
          group: events.k8s.io
          mode: watch
          name: events
    service:
      pipelines:
        logs:
          exporters:
          - debug
          processors:
          - k8sattributes
          - resourcedetection/env
          - batch
          receivers:
          - k8sobjects
        metrics:
          exporters:
          - debug
          processors:
          - k8sattributes
          - resourcedetection/env
          - batch
          receivers:
          - k8s_cluster
  replicas: 1
  imagePullPolicy: IfNotPresent
  upgradeStrategy: automatic
  hostNetwork: false
  shareProcessNamespace: false
  terminationGracePeriodSeconds: 30
  resources:
    limits:
      cpu: 100m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 500Mi
  securityContext:
    {}
  volumeMounts:
  env:
  - name: OTEL_K8S_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
  - name: OTEL_K8S_NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_K8S_NAMESPACE
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.namespace
  - name: OTEL_K8S_POD_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.name
  - name: OTEL_K8S_POD_IP
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: status.podIP
  
  volumes:
---
# Source: opentelemetry-kube-stack/templates/collector.yaml
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: my-opentelemetry-kube-stack-daemon
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-kube-stack-0.2.1
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm        
spec:
  managementState: managed
  mode: daemonset
  config:
    exporters:
      debug: {}
    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s
      k8sattributes:
        extract:
          labels:
          - from: pod
            key: app.kubernetes.io/name
            tag_name: service.name
          - from: pod
            key: k8s-app
            tag_name: service.name
          - from: pod
            key: app.kubernetes.io/instance
            tag_name: k8s.app.instance
          - from: pod
            key: app.kubernetes.io/version
            tag_name: service.version
          - from: pod
            key: app.kubernetes.io/component
            tag_name: k8s.app.component
          metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.tag
          - container.image.name
          - k8s.cluster.uid
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
          - from: resource_attribute
            name: k8s.node.name
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.name
          - from: resource_attribute
            name: k8s.namespace.name
        - sources:
          - from: connection
      resourcedetection/env:
        detectors:
        - env
        override: false
        timeout: 2s
    receivers:
      filelog:
        exclude: []
        include:
        - /var/log/pods/*/*/*.log
        include_file_name: false
        include_file_path: true
        operators:
        - id: container-parser
          max_log_size: 102400
          type: container
        retry_on_failure:
          enabled: true
        start_at: end
      hostmetrics:
        collection_interval: 10s
        root_path: /hostfs
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
          disk: {}
          filesystem:
            exclude_fs_types:
              fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
              match_type: strict
            exclude_mount_points:
              match_type: regexp
              mount_points:
              - /dev/*
              - /proc/*
              - /sys/*
              - /run/k3s/containerd/*
              - /var/lib/docker/*
              - /var/lib/kubelet/*
              - /snap/*
            metrics:
              system.filesystem.utilization:
                enabled: true
          load: {}
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
          network: {}
      kubeletstats:
        auth_type: serviceAccount
        collection_interval: 15s
        endpoint: https://${env:OTEL_K8S_NODE_IP}:10250
        extra_metadata_labels:
        - container.id
        - k8s.volume.type
        insecure_skip_verify: true
        k8s_api_config:
          auth_type: serviceAccount
        metric_groups:
        - node
        - pod
        - volume
        - container
        metrics:
          container.cpu.usage:
            enabled: true
          k8s.node.cpu.usage:
            enabled: true
          k8s.node.uptime:
            enabled: true
          k8s.pod.cpu.usage:
            enabled: true
          k8s.pod.uptime:
            enabled: true
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
          - job_name: kubernetes-pods
            kubernetes_sd_configs:
            - role: pod
              selectors:
              - field: spec.nodeName=${env:OTEL_K8S_NODE_NAME}
                role: pod
            relabel_configs:
            - action: keep
              regex: true
              source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scrape
            - action: drop
              regex: true
              source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
            - action: replace
              regex: (https?)
              source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $$1:$$2
              source_labels:
              - __address__
              - __meta_kubernetes_pod_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod
            - action: drop
              regex: Pending|Succeeded|Failed|Completed
              source_labels:
              - __meta_kubernetes_pod_phase
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              target_label: job
            scrape_interval: 30s
          - job_name: node-exporter
            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - action: replace
              regex: (.*)
              replacement: $$1
              separator: ;
              source_labels:
              - job
              target_label: __tmp_prometheus_job_name
            scrape_interval: 30s
            static_configs:
            - targets:
              - ${env:OTEL_K8S_NODE_IP}:9100
          - authorization:
              credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              type: Bearer
            follow_redirects: true
            honor_labels: true
            honor_timestamps: true
            job_name: kubelet
            kubernetes_sd_configs:
            - follow_redirects: true
              role: node
              selectors:
              - field: metadata.name=${env:OTEL_K8S_NODE_NAME}
                role: node
            metric_relabel_configs:
            - action: drop
              regex: container_cpu_(load_average_10s|system_seconds_total|user_seconds_total)
              replacement: $$1
              separator: ;
              source_labels:
              - __name__
            - action: drop
              regex: container_fs_(io_current|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
              replacement: $$1
              separator: ;
              source_labels:
              - __name__
            - action: drop
              regex: container_memory_(mapped_file|swap)
              replacement: $$1
              separator: ;
              source_labels:
              - __name__
            - action: drop
              regex: container_(file_descriptors|tasks_state|threads_max)
              replacement: $$1
              separator: ;
              source_labels:
              - __name__
            - action: drop
              regex: container_spec.*
              replacement: $$1
              separator: ;
              source_labels:
              - __name__
            - action: drop
              regex: .+;
              replacement: $$1
              separator: ;
              source_labels:
              - id
              - pod
            metrics_path: /metrics/cadvisor
            relabel_configs:
            - action: replace
              regex: (.*)
              replacement: $$1
              separator: ;
              source_labels:
              - job
              target_label: __tmp_prometheus_job_name
            - action: replace
              replacement: kubelet
              target_label: job
            - action: replace
              regex: (.*)
              replacement: $$1
              separator: ;
              source_labels:
              - __meta_kubernetes_node_name
              target_label: node
            - action: replace
              regex: (.*)
              replacement: https-metrics
              separator: ;
              target_label: endpoint
            - action: replace
              regex: (.*)
              replacement: $$1
              separator: ;
              source_labels:
              - __metrics_path__
              target_label: metrics_path
            - action: hashmod
              modulus: 1
              regex: (.*)
              replacement: $$1
              separator: ;
              source_labels:
              - __address__
              target_label: __tmp_hash
            - action: keep
              regex: $(SHARD)
              replacement: $$1
              separator: ;
              source_labels:
              - __tmp_hash
            scheme: https
            scrape_interval: 15s
            scrape_timeout: 10s
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
    service:
      pipelines:
        logs:
          exporters:
          - debug
          processors:
          - k8sattributes
          - resourcedetection/env
          - batch
          receivers:
          - otlp
          - filelog
        metrics:
          exporters:
          - debug
          processors:
          - k8sattributes
          - resourcedetection/env
          - batch
          receivers:
          - prometheus
          - otlp
          - hostmetrics
          - kubeletstats
        traces:
          exporters:
          - debug
          processors:
          - k8sattributes
          - resourcedetection/env
          - batch
          receivers:
          - otlp
  imagePullPolicy: IfNotPresent
  upgradeStrategy: automatic
  hostNetwork: false
  shareProcessNamespace: false
  terminationGracePeriodSeconds: 30
  resources:
    limits:
      cpu: 100m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 128Mi
  securityContext:
    {}
  volumeMounts:
  - name: varlogpods
    mountPath: /var/log/pods
    readOnly: true
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true
  - name: hostfs
    mountPath: /hostfs
    readOnly: true
    mountPropagation: HostToContainer
  env:
  - name: OTEL_K8S_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
  - name: OTEL_K8S_NODE_IP
    valueFrom:
      fieldRef:
        fieldPath: status.hostIP
  - name: OTEL_K8S_NAMESPACE
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.namespace
  - name: OTEL_K8S_POD_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: metadata.name
  - name: OTEL_K8S_POD_IP
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: status.podIP
  
  volumes:
  - name: varlogpods
    hostPath:
      path: /var/log/pods
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
  - name: hostfs
    hostPath:
      path: /
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/admission-webhooks/operator-webhook-with-cert-manager.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
    cert-manager.io/inject-ca-from: default/my-opentelemetry-kube-stack-opentelemetry-operator-serving-cert
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: webhook
  name: my-opentelemetry-kube-stack-opentelemetry-operator-validation
webhooks:
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /validate-opentelemetry-io-v1alpha1-instrumentation
        port: 443
    failurePolicy: Ignore
    name: vinstrumentationcreateupdate.kb.io
    rules:
    - apiGroups:
        - opentelemetry.io
      apiVersions:
        - v1alpha1
      operations:
        - CREATE
        - UPDATE
      resources:
        - instrumentations
      scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /validate-opentelemetry-io-v1alpha1-instrumentation
        port: 443
    failurePolicy: Ignore
    name: vinstrumentationdelete.kb.io
    rules:
      - apiGroups:
          - opentelemetry.io
        apiVersions:
          - v1alpha1
        operations:
          - DELETE
        resources:
          - instrumentations
        scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /validate-opentelemetry-io-v1beta1-opentelemetrycollector
        port: 443
    failurePolicy: Ignore
    name: vopentelemetrycollectorcreateupdatebeta.kb.io
    rules:
      - apiGroups:
          - opentelemetry.io
        apiVersions:
          - v1beta1
        operations:
          - CREATE
          - UPDATE
        resources:
          - opentelemetrycollectors
        scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
  - admissionReviewVersions:
      - v1
    clientConfig:
      service:
        name: my-opentelemetry-kube-stack-opentelemetry-operator-webhook
        namespace: default
        path: /validate-opentelemetry-io-v1beta1-opentelemetrycollector
        port: 443
    failurePolicy: Ignore
    name: vopentelemetrycollectordeletebeta.kb.io
    rules:
      - apiGroups:
          - opentelemetry.io
        apiVersions:
          - v1beta1
        operations:
          - DELETE
        resources:
          - opentelemetrycollectors
        scope: Namespaced
    sideEffects: None
    timeoutSeconds: 10
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/tests/test-certmanager-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-opentelemetry-kube-stack-opentelemetry-operator-cert-manager"
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: webhook
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: "busybox:latest"
      env:
        - name: CERT_MANAGER_CLUSTERIP
          value: "cert-manager-webhook"
        - name: CERT_MANAGER_PORT
          value: "443"
      command:
        - sh
        - -c
        # The following shell script tests if the cert-manager service is up. If the service is up, when we try
        # to wget its exposed port, we will get an HTTP error 400.
        - |
          wget_output=$(wget -q "$CERT_MANAGER_CLUSTERIP:$CERT_MANAGER_PORT")
          if wget_output=="wget: server returned error: HTTP/1.0 400 Bad Request"
          then exit 0
          else exit 1
          fi
  restartPolicy: Never
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/tests/test-service-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-opentelemetry-kube-stack-opentelemetry-operator-metrics"
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: "busybox:latest"
      env:
        - name: MANAGER_METRICS_SERVICE_CLUSTERIP
          value: "my-opentelemetry-kube-stack-opentelemetry-operator"
        - name: MANAGER_METRICS_SERVICE_PORT
          value: "8443"
      command:
        - sh
        - -c
        # The following shell script tests if the controller-manager-metrics-service is up.
        # If the service is up, when we try to wget its exposed port, we will get an HTTP error 400.
        - |
          wget_output=$(wget -q "$MANAGER_METRICS_SERVICE_CLUSTERIP:$MANAGER_METRICS_SERVICE_PORT")
          if wget_output=="wget: server returned error: HTTP/1.0 400 Bad Request"
          then exit 0
          else exit 1
          fi
  restartPolicy: Never
---
# Source: opentelemetry-kube-stack/charts/opentelemetry-operator/templates/tests/test-service-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "my-opentelemetry-kube-stack-opentelemetry-operator-webhook"
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-operator-0.68.0
    app.kubernetes.io/name: opentelemetry-operator
    app.kubernetes.io/version: "0.107.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: my-opentelemetry-kube-stack
    
    app.kubernetes.io/component: controller-manager
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: "busybox:latest"
      env:
        - name: WEBHOOK_SERVICE_CLUSTERIP
          value: "my-opentelemetry-kube-stack-opentelemetry-operator-webhook"
        - name: WEBHOOK_SERVICE_PORT
          value: "443"
      command:
        - sh
        - -c
        # The following shell script tests if the webhook service is up. If the service is up, when we try
        # to wget its exposed port, we will get an HTTP error 400.
        - |
          wget_output=$(wget -q "$WEBHOOK_SERVICE_CLUSTERIP:$WEBHOOK_SERVICE_PORT")
          if wget_output=="wget: server returned error: HTTP/1.0 400 Bad Request"
          then exit 0
          else exit 1
          fi
  restartPolicy: Never
---
# Source: opentelemetry-kube-stack/templates/hooks.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: opentelemetry-kube-stack-pre-delete-job
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
spec:
  template:
    spec:
      restartPolicy: Never
      serviceAccountName: delete-resources-sa
      containers:
      - name: delete-resources
        image: bitnami/kubectl:latest
        command:
          - /bin/sh
          - -c
          - |
            kubectl delete instrumentations,opampbridges,opentelemetrycollectors \
              -l helm.sh/chart=opentelemetry-kube-stack-0.2.1
